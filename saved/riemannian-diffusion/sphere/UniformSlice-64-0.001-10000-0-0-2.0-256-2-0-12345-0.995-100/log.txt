2022-11-17 20:41:02.031149	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-17 20:41:02.031292	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-17 20:41:10.846287	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-17 20:41:10.846477	cuda unavailable
2022-11-17 20:41:14.030695	Iteration 1 	loss 2.426604986190796
2022-11-17 20:41:14.757190	Iteration 100 	loss 1.6974592208862305
2022-11-17 20:41:15.479946	Iteration 200 	loss 1.1923394203186035
2022-11-17 20:41:16.205212	Iteration 300 	loss 1.7829198837280273
2022-11-17 20:41:16.966778	Iteration 400 	loss 1.6929311752319336
2022-11-17 20:41:17.690798	Iteration 500 	loss 1.003495454788208
2022-11-17 20:43:49.995083	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-17 20:43:49.995270	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-17 20:44:14.889997	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-17 20:44:14.890239	cuda unavailable
2022-11-17 20:44:18.189997	Iteration 1 	loss 2.459419012069702
2022-11-17 20:44:18.924881	Iteration 100 	loss 1.3944973945617676
2022-11-17 20:44:19.652018	Iteration 200 	loss 0.9261648654937744
2022-11-17 20:44:20.380457	Iteration 300 	loss 1.3888881206512451
2022-11-17 20:44:21.140022	Iteration 400 	loss 1.0675991773605347
2022-11-17 20:44:21.867535	Iteration 500 	loss 1.509911298751831
2022-11-17 20:52:56.971645	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-17 20:52:56.971784	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-17 20:52:57.764524	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-17 20:52:57.764757	cuda unavailable
2022-11-17 20:53:01.066698	Iteration 1 	loss 2.848324775695801
2022-11-17 20:53:01.778898	Iteration 100 	loss 1.7391211986541748
2022-11-17 20:53:02.488251	Iteration 200 	loss 1.4904590845108032
2022-11-17 20:53:03.200403	Iteration 300 	loss 1.4177501201629639
2022-11-17 20:53:03.931844	Iteration 400 	loss 1.12455153465271
2022-11-17 20:53:04.639346	Iteration 500 	loss 1.3137227296829224
2022-11-17 20:56:24.376084	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-17 20:56:24.376381	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-17 20:56:25.160370	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-17 20:56:25.160560	cuda unavailable
2022-11-17 20:56:28.483186	Iteration 1 	loss 2.6996326446533203
2022-11-17 20:56:29.194317	Iteration 100 	loss 2.0397984981536865
2022-11-17 20:56:29.899013	Iteration 200 	loss 1.3956286907196045
2022-11-17 20:56:30.608244	Iteration 300 	loss 1.5933963060379028
2022-11-17 20:56:31.336292	Iteration 400 	loss 1.0926008224487305
2022-11-17 20:56:32.043256	Iteration 500 	loss 1.6150648593902588
2022-11-19 16:10:03.253107	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-19 16:10:03.253380	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-19 16:10:04.031453	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-19 16:10:04.031615	cuda unavailable
2022-11-19 16:10:07.363874	Iteration 1 	loss 2.5339105129241943
2022-11-19 16:10:08.121950	Iteration 100 	loss 1.0902340412139893
2022-11-19 16:10:08.885646	Iteration 200 	loss 1.4700608253479004
2022-11-19 16:10:09.654568	Iteration 300 	loss 1.414972186088562
2022-11-19 16:10:10.415550	Iteration 400 	loss 1.6242934465408325
2022-11-19 16:10:11.149624	Iteration 500 	loss 1.2591218948364258
2022-11-24 17:49:26.174158	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 17:49:26.174425	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='report')
2022-11-24 17:49:27.715072	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-24 17:49:27.715255	cuda unavailable
2022-11-24 20:42:28.835418	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 20:42:28.835720	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 20:42:29.902642	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-24 20:42:29.902881	cuda unavailable
2022-11-24 20:42:34.231089	Iteration 1 	loss 2.7934601306915283
2022-11-24 20:44:16.884516	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 20:44:16.884641	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 20:44:17.726889	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-24 20:44:17.727115	cuda unavailable
2022-11-24 20:44:21.141890	Iteration 1 	loss 2.6525208950042725
2022-11-24 20:44:21.921939	Iteration 100 	loss 1.820278525352478
2022-11-24 20:44:22.693869	Iteration 200 	loss 2.393012523651123
2022-11-24 20:44:23.496299	Iteration 300 	loss 1.7525923252105713
2022-11-24 20:44:24.283846	Iteration 400 	loss 1.7922441959381104
2022-11-24 20:44:25.076163	Iteration 500 	loss 1.5075905323028564
2022-11-24 20:45:37.535571	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 20:45:37.535844	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 20:45:38.325831	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-24 20:45:38.326100	cuda unavailable
2022-11-24 20:45:41.668365	Iteration 1 	loss 2.655740976333618
2022-11-24 20:45:42.504591	Iteration 100 	loss 1.2471866607666016
2022-11-24 20:45:43.339506	Iteration 200 	loss 0.9131572246551514
2022-11-24 20:45:44.195112	Iteration 300 	loss 1.8798470497131348
2022-11-24 20:45:45.053832	Iteration 400 	loss 1.1240171194076538
2022-11-24 20:45:45.886669	Iteration 500 	loss 1.2810213565826416
2022-11-24 20:47:24.681632	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 20:47:24.681893	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 20:47:25.484022	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-24 20:47:25.484274	cuda unavailable
2022-11-24 20:47:28.855042	Iteration 1 	loss 2.260521411895752
2022-11-24 20:52:05.228042	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 20:52:05.228355	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 20:52:06.106964	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-24 20:52:06.107216	cuda unavailable
2022-11-24 20:52:09.697691	Iteration 1 	loss 2.2743353843688965
2022-11-24 21:31:19.069158	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 21:31:19.069563	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 21:32:20.382189	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-24 21:32:20.382678	cuda unavailable
2022-11-24 21:37:38.890609	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 21:37:38.890768	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 21:38:40.436592	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-24 21:38:40.437033	cuda unavailable
2022-11-24 21:39:35.599458	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 21:39:35.599582	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 21:39:48.293807	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-24 21:39:48.294025	cuda unavailable
2022-11-24 21:39:50.482564	Iteration 1 	loss 2.5469460487365723
2022-11-24 21:40:19.030841	Iteration 100 	loss 2.187865734100342
2022-11-24 21:40:48.483292	Iteration 200 	loss 2.0771002769470215
2022-11-24 21:41:17.859893	Iteration 300 	loss 2.199282646179199
2022-11-24 21:51:03.981503	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 21:51:03.981625	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 21:51:17.409293	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-24 21:51:17.409504	cuda unavailable
2022-11-24 21:51:19.390662	Iteration 1 	loss 2.5261123180389404
2022-11-24 21:52:19.420159	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 21:52:19.420274	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 21:52:32.345364	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-24 21:52:32.345561	cuda unavailable
2022-11-24 21:52:34.248092	Iteration 1 	loss 2.514641761779785
2022-11-24 21:52:54.753054	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 21:52:54.753172	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 21:53:07.779559	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-24 21:53:07.779763	cuda unavailable
2022-11-24 21:53:09.688875	Iteration 1 	loss 2.527434825897217
2022-11-24 21:53:39.252436	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 21:53:39.252556	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 21:53:51.753174	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-24 21:53:51.753440	cuda unavailable
2022-11-24 21:53:53.639525	Iteration 1 	loss 2.5597541332244873
2022-11-24 21:56:22.262288	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 21:56:22.262666	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 21:56:35.019126	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-24 21:56:35.019342	cuda unavailable
2022-11-24 21:56:36.927654	Iteration 1 	loss 2.510392665863037
2022-11-24 21:57:59.332041	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 21:57:59.332173	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 21:58:11.929341	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-24 21:58:11.929534	cuda unavailable
2022-11-24 21:58:13.841135	Iteration 1 	loss 2.575115919113159
2022-11-24 22:00:09.049498	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 22:00:09.049623	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 22:00:21.857716	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-24 22:00:21.858192	cuda unavailable
2022-11-24 22:00:23.795349	Iteration 1 	loss 2.5224533081054688
2022-11-24 22:01:03.848142	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 22:01:03.848257	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 22:01:16.537514	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-24 22:01:16.537717	cuda unavailable
2022-11-24 22:01:18.415823	Iteration 1 	loss 2.545114040374756
2022-11-24 22:02:03.447842	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 22:02:03.448038	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 22:02:16.127768	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-24 22:02:16.127984	cuda unavailable
2022-11-24 22:02:18.059257	Iteration 1 	loss 2.5457770824432373
2022-11-24 22:02:20.485386	Iteration 100 	loss 2.1624064445495605
2022-11-24 22:02:22.951078	Iteration 200 	loss 2.1247644424438477
2022-11-24 22:02:25.491344	Iteration 300 	loss 2.087594509124756
2022-11-24 22:02:27.973286	Iteration 400 	loss 2.146014451980591
2022-11-24 22:02:30.444218	Iteration 500 	loss 2.3684158325195312
2022-11-24 22:04:23.917640	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 22:04:23.917948	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 22:04:36.675430	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-24 22:04:36.675888	cuda unavailable
2022-11-24 22:04:38.574933	Iteration 1 	loss 2.525400400161743
2022-11-24 22:04:41.060553	Iteration 100 	loss 2.2947182655334473
2022-11-24 22:04:43.640001	Iteration 200 	loss 2.2218363285064697
2022-11-24 22:04:46.276659	Iteration 300 	loss 2.2230224609375
2022-11-24 22:04:48.845154	Iteration 400 	loss 2.2013559341430664
2022-11-24 22:04:51.406223	Iteration 500 	loss 2.0592665672302246
2022-11-24 22:35:54.492838	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 22:35:54.493193	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 22:36:08.660016	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-24 22:36:08.660198	cuda unavailable
2022-11-24 22:36:10.543738	Iteration 1 	loss 2.533285140991211
2022-11-24 22:36:16.870892	Iteration 100 	loss 2.1257925033569336
2022-11-24 22:42:36.746212	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 22:42:36.746537	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 22:42:49.840324	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-24 22:42:49.840544	cuda unavailable
2022-11-24 22:42:51.759366	Iteration 1 	loss 2.492088794708252
2022-11-24 22:42:54.254077	Iteration 100 	loss 0.9996795654296875
2022-11-24 22:42:56.817248	Iteration 200 	loss 0.8538877964019775
2022-11-24 22:42:59.456985	Iteration 300 	loss 0.7833638787269592
2022-11-24 22:43:02.036510	Iteration 400 	loss 0.5184190273284912
2022-11-24 22:43:04.623069	Iteration 500 	loss 0.821596086025238
2022-11-24 22:47:59.151984	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 22:47:59.152295	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 22:48:12.079232	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-24 22:48:12.079448	cuda unavailable
2022-11-24 22:48:36.732188	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 22:48:36.732363	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 22:48:49.692997	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-24 22:48:49.693497	cuda unavailable
2022-11-24 22:48:51.577302	Iteration 1 	loss 2.5066723823547363
2022-11-24 22:48:54.058694	Iteration 100 	loss 2.268798589706421
2022-11-24 22:48:56.622778	Iteration 200 	loss 2.286569118499756
2022-11-24 22:48:59.251773	Iteration 300 	loss 2.1997177600860596
2022-11-24 22:49:01.804640	Iteration 400 	loss 2.1657533645629883
2022-11-24 22:49:04.349508	Iteration 500 	loss 2.2151548862457275
2022-11-24 22:54:18.302729	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 22:54:18.302912	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 22:54:32.080365	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-24 22:54:32.096711	cuda unavailable
2022-11-24 22:55:09.020596	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 22:55:09.020774	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 22:55:23.135752	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-24 22:55:23.135979	cuda unavailable
2022-11-24 22:55:36.726725	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 22:55:36.726913	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 22:55:49.408876	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-24 22:55:49.409124	cuda unavailable
2022-11-24 22:55:51.320784	Iteration 1 	loss 2.536074638366699
2022-11-24 22:56:03.225205	Iteration 100 	loss 2.1987464427948
2022-11-24 22:56:15.625932	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 22:56:15.626063	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 22:56:28.749742	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-24 22:56:28.749944	cuda unavailable
2022-11-24 22:56:30.661277	Iteration 1 	loss 2.5436160564422607
2022-11-24 22:57:07.622741	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 22:57:07.622925	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 22:57:20.352081	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-24 22:57:20.352522	cuda unavailable
2022-11-24 22:57:22.261443	Iteration 1 	loss 2.5293078422546387
2022-11-24 22:57:24.858326	Iteration 100 	loss 2.345327854156494
2022-11-24 22:57:27.746749	Iteration 200 	loss 2.1561710834503174
2022-11-24 22:57:30.580617	Iteration 300 	loss 2.18534779548645
2022-11-24 22:57:33.234710	Iteration 400 	loss 2.003098249435425
2022-11-24 22:57:35.937834	Iteration 500 	loss 2.2428958415985107
2022-11-24 22:59:16.045542	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 22:59:16.045667	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 22:59:29.130112	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-24 22:59:29.130372	cuda unavailable
2022-11-24 22:59:31.048296	Iteration 1 	loss 2.5367069244384766
2022-11-24 22:59:33.606211	Iteration 100 	loss 2.3231472969055176
2022-11-24 22:59:36.359684	Iteration 200 	loss 2.204010009765625
2022-11-24 22:59:39.008834	Iteration 300 	loss 2.307936906814575
2022-11-24 22:59:41.579688	Iteration 400 	loss 2.292719602584839
2022-11-24 22:59:44.160124	Iteration 500 	loss 2.230743169784546
2022-11-24 23:00:06.332182	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 23:00:06.332356	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 23:00:19.806622	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-24 23:00:19.807102	cuda unavailable
2022-11-24 23:00:21.726753	Iteration 1 	loss 2.4940199851989746
2022-11-24 23:00:24.339543	Iteration 100 	loss 2.243394613265991
2022-11-24 23:00:27.049010	Iteration 200 	loss 2.113964080810547
2022-11-24 23:00:29.814025	Iteration 300 	loss 2.060410261154175
2022-11-24 23:00:32.458769	Iteration 400 	loss 2.090378999710083
2022-11-24 23:00:35.079959	Iteration 500 	loss 2.131618022918701
2022-11-24 23:23:38.716676	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 23:23:38.717134	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 23:25:44.256242	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 23:25:44.258061	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 23:26:48.700025	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 23:26:48.700223	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 23:27:37.303150	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 23:27:37.303488	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 23:28:10.967004	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 23:28:10.967189	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 23:32:21.855178	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 23:32:21.855319	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 23:33:24.960793	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 23:33:24.960952	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 23:33:43.316502	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 23:33:43.316696	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 23:35:08.361344	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 23:35:08.361507	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 23:35:41.579057	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 23:35:41.579264	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 23:35:47.488926	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 23:35:47.489157	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 23:36:51.469701	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-24 23:36:51.469915	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-24 23:37:05.386965	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-24 23:37:05.387171	cuda unavailable
2022-11-24 23:37:07.272741	Iteration 1 	loss 2.8343005180358887
2022-11-24 23:37:09.998300	Iteration 100 	loss 1.8630462884902954
2022-11-24 23:37:12.799442	Iteration 200 	loss 1.8852035999298096
2022-11-24 23:37:15.665248	Iteration 300 	loss 2.2648048400878906
2022-11-24 23:37:18.447523	Iteration 400 	loss 1.684221625328064
2022-11-24 23:37:21.240050	Iteration 500 	loss 1.8051127195358276
2022-11-25 00:44:29.186323	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-25 00:44:29.187023	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-25 00:45:24.551953	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-25 00:45:24.552201	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-25 00:47:42.893250	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-25 00:47:42.894084	cuda unavailable
2022-11-25 00:47:44.994753	Iteration 1 	loss 2.5474092960357666
2022-11-25 00:47:51.866425	Iteration 100 	loss 2.4475464820861816
2022-11-25 00:47:58.710408	Iteration 200 	loss 2.290426254272461
2022-11-25 00:48:06.147752	Iteration 300 	loss 2.3541412353515625
2022-11-25 00:48:13.937111	Iteration 400 	loss 2.3617186546325684
2022-11-25 00:48:21.192649	Iteration 500 	loss 2.5468056201934814
2022-11-25 00:58:51.417760	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-25 00:58:51.418191	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-25 01:01:06.673673	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-25 01:01:06.674384	cuda unavailable
2022-11-25 01:01:08.759567	Iteration 1 	loss 2.545583724975586
2022-11-25 01:01:11.515694	Iteration 100 	loss 2.4804091453552246
2022-11-25 01:01:14.442735	Iteration 200 	loss 2.626192331314087
2022-11-25 01:01:17.434592	Iteration 300 	loss 2.617486000061035
2022-11-25 01:01:20.337009	Iteration 400 	loss 2.54590106010437
2022-11-25 01:01:23.351711	Iteration 500 	loss 2.3163082599639893
2022-11-25 01:37:01.546664	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-25 01:37:01.547036	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-25 01:39:03.363470	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-25 01:39:03.363952	cuda unavailable
2022-11-25 01:39:05.347348	Iteration 1 	loss 2.5398881435394287
2022-11-25 01:39:07.823909	Iteration 100 	loss 2.480830669403076
2022-11-25 01:39:10.424867	Iteration 200 	loss 2.3996541500091553
2022-11-25 01:39:13.103182	Iteration 300 	loss 2.5492329597473145
2022-11-25 01:39:15.716598	Iteration 400 	loss 2.1826601028442383
2022-11-25 01:39:18.403874	Iteration 500 	loss 2.2946865558624268
2022-11-25 12:34:48.070043	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-11-25 12:34:48.071741	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-11-25 12:36:49.612188	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-11-25 12:36:49.612573	cuda unavailable
2022-11-25 12:36:51.736305	Iteration 1 	loss 2.477694272994995
2022-11-25 12:36:54.081656	Iteration 100 	loss 2.6332616806030273
2022-11-25 12:36:56.455394	Iteration 200 	loss 2.4961531162261963
2022-11-25 12:36:59.005218	Iteration 300 	loss 2.3460004329681396
2022-11-25 12:37:01.466993	Iteration 400 	loss 2.4001190662384033
2022-11-25 12:37:03.907254	Iteration 500 	loss 2.2988970279693604
2022-12-12 17:41:21.441854	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-12-12 17:41:21.442267	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-12-12 17:43:22.063848	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-12-12 17:43:22.064366	cuda unavailable
2022-12-12 17:43:24.158660	Iteration 1 	loss 2.4838621616363525
2022-12-12 17:43:26.576507	Iteration 100 	loss 2.270975112915039
2022-12-12 17:43:28.959579	Iteration 200 	loss 2.3253371715545654
2022-12-12 17:43:31.391667	Iteration 300 	loss 2.1356232166290283
2022-12-12 17:43:33.806715	Iteration 400 	loss 2.2904491424560547
2022-12-12 17:43:36.214870	Iteration 500 	loss 2.05112886428833
2022-12-12 17:44:43.025624	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-12-12 17:44:43.025797	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-12-12 17:46:43.158567	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-12-12 17:46:43.158995	cuda unavailable
2022-12-12 17:46:45.129790	Iteration 1 	loss 2.47935152053833
2022-12-12 17:46:47.357243	Iteration 100 	loss 2.347158908843994
2022-12-12 17:46:49.568055	Iteration 200 	loss 2.4147183895111084
2022-12-12 17:46:51.793699	Iteration 300 	loss 2.386061668395996
2022-12-12 17:46:54.043904	Iteration 400 	loss 2.4806482791900635
2022-12-12 17:46:56.288026	Iteration 500 	loss 2.327803373336792
2022-12-12 17:58:25.922105	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-12-12 17:58:25.922279	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-12-12 17:59:22.065992	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-12-12 17:59:22.066200	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-12-12 18:01:25.146281	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-12-12 18:01:25.146704	cuda unavailable
2022-12-12 18:01:27.110879	Iteration 1 	loss 2.4781713485717773
2022-12-12 18:01:29.386149	Iteration 100 	loss 2.391993761062622
2022-12-12 18:01:31.664204	Iteration 200 	loss 2.3995869159698486
2022-12-12 18:01:33.935614	Iteration 300 	loss 2.522160053253174
2022-12-12 18:01:36.194212	Iteration 400 	loss 2.3516952991485596
2022-12-12 18:01:38.514478	Iteration 500 	loss 2.3011035919189453
2022-12-12 18:04:38.969259	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-12-12 18:04:38.969423	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-12-12 18:06:55.716042	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-12-12 18:06:55.716482	cuda unavailable
2022-12-12 18:06:57.760072	Iteration 1 	loss 2.5297064781188965
2022-12-12 18:07:00.272010	Iteration 100 	loss 2.4711577892303467
2022-12-12 18:07:02.738618	Iteration 200 	loss 2.3756608963012695
2022-12-12 18:07:05.201305	Iteration 300 	loss 2.446915626525879
2022-12-12 18:07:07.660445	Iteration 400 	loss 2.29223895072937
2022-12-12 18:07:10.105489	Iteration 500 	loss 2.340059280395508
2022-12-12 18:10:19.548443	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-12-12 18:10:19.548803	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-12-12 18:12:18.707670	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-12-12 18:12:18.708088	cuda unavailable
2022-12-12 18:12:20.649604	Iteration 1 	loss 2.5466442108154297
2022-12-12 18:12:22.902609	Iteration 100 	loss 2.4537978172302246
2022-12-12 18:12:25.204109	Iteration 200 	loss 2.2723939418792725
2022-12-12 18:12:27.495204	Iteration 300 	loss 2.2525970935821533
2022-12-12 18:12:29.785136	Iteration 400 	loss 2.198791027069092
2022-12-12 18:12:32.072193	Iteration 500 	loss 2.205815076828003
2022-12-12 18:14:10.542991	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-12-12 18:14:10.543346	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-12-12 18:16:10.769037	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-12-12 18:16:10.769236	cuda unavailable
2022-12-12 18:16:12.770601	Iteration 1 	loss 2.5931360721588135
2022-12-12 18:16:15.046463	Iteration 100 	loss 2.441132068634033
2022-12-12 18:16:17.313867	Iteration 200 	loss 2.5453968048095703
2022-12-12 18:16:19.572757	Iteration 300 	loss 2.4585325717926025
2022-12-12 18:16:21.843124	Iteration 400 	loss 2.448160171508789
2022-12-12 18:16:24.115394	Iteration 500 	loss 2.4735522270202637
2022-12-12 18:16:26.518916	Iteration 600 	loss 2.2964587211608887
2022-12-12 18:16:28.807690	Iteration 700 	loss 2.3619117736816406
2022-12-12 18:16:31.092538	Iteration 800 	loss 2.3218326568603516
2022-12-12 18:16:33.388108	Iteration 900 	loss 2.4955272674560547
2022-12-12 18:16:35.736174	Iteration 1000 	loss 2.429543972015381
2022-12-12 18:16:38.189347	Iteration 1100 	loss 2.5018162727355957
2022-12-12 18:16:40.450348	Iteration 1200 	loss 2.322730779647827
2022-12-12 18:16:42.736056	Iteration 1300 	loss 2.343864917755127
2022-12-12 18:16:45.002585	Iteration 1400 	loss 2.233888626098633
2022-12-12 18:16:47.273132	Iteration 1500 	loss 2.313447952270508
2022-12-12 18:16:49.673430	Iteration 1600 	loss 2.335226535797119
2022-12-12 18:16:51.947030	Iteration 1700 	loss 2.2941694259643555
2022-12-12 18:16:54.222643	Iteration 1800 	loss 2.4586470127105713
2022-12-12 18:16:56.510269	Iteration 1900 	loss 2.4552810192108154
2022-12-12 18:16:58.789214	Iteration 2000 	loss 2.3577988147735596
2022-12-12 18:17:01.191172	Iteration 2100 	loss 2.0970497131347656
2022-12-12 18:17:03.466865	Iteration 2200 	loss 2.255286931991577
2022-12-12 18:17:05.733430	Iteration 2300 	loss 2.169522285461426
2022-12-12 18:17:08.006806	Iteration 2400 	loss 2.4284331798553467
2022-12-12 18:17:10.269700	Iteration 2500 	loss 2.515850067138672
2022-12-12 18:17:12.767692	Iteration 2600 	loss 2.0961050987243652
2022-12-12 18:17:15.084213	Iteration 2700 	loss 2.3690385818481445
2022-12-12 18:17:17.393904	Iteration 2800 	loss 2.4849648475646973
2022-12-12 18:17:19.782981	Iteration 2900 	loss 2.210184097290039
2022-12-12 18:17:22.116897	Iteration 3000 	loss 2.2520313262939453
2022-12-12 18:17:24.533391	Iteration 3100 	loss 2.283907651901245
2022-12-12 18:17:26.813568	Iteration 3200 	loss 2.317138671875
2022-12-12 18:17:29.108749	Iteration 3300 	loss 2.474670171737671
2022-12-12 18:17:31.392521	Iteration 3400 	loss 2.3942320346832275
2022-12-12 18:17:33.651253	Iteration 3500 	loss 2.478238821029663
2022-12-12 18:17:36.048939	Iteration 3600 	loss 2.32255482673645
2022-12-12 18:17:38.340722	Iteration 3700 	loss 2.103722333908081
2022-12-12 18:17:40.599887	Iteration 3800 	loss 2.069023609161377
2022-12-12 18:17:42.855205	Iteration 3900 	loss 2.4667255878448486
2022-12-12 18:17:45.109510	Iteration 4000 	loss 1.9244474172592163
2022-12-12 18:17:47.506158	Iteration 4100 	loss 2.0921778678894043
2022-12-12 18:17:49.762591	Iteration 4200 	loss 2.404374122619629
2022-12-12 18:17:52.035166	Iteration 4300 	loss 2.227710723876953
2022-12-12 18:17:54.291113	Iteration 4400 	loss 2.3660216331481934
2022-12-12 18:17:56.538307	Iteration 4500 	loss 2.4254848957061768
2022-12-12 18:17:58.933788	Iteration 4600 	loss 2.084961414337158
2022-12-12 18:18:01.183652	Iteration 4700 	loss 1.791197657585144
2022-12-12 18:18:03.447309	Iteration 4800 	loss 2.2199716567993164
2022-12-12 18:18:05.704398	Iteration 4900 	loss 2.212463855743408
2022-12-12 18:18:07.978326	Iteration 5000 	loss 2.5436904430389404
2022-12-12 18:18:10.455383	Iteration 5100 	loss 2.380753517150879
2022-12-12 18:18:12.696123	Iteration 5200 	loss 2.179222822189331
2022-12-12 18:18:14.945550	Iteration 5300 	loss 2.2006378173828125
2022-12-12 18:18:17.204522	Iteration 5400 	loss 2.523557186126709
2022-12-12 18:18:19.479297	Iteration 5500 	loss 2.3254098892211914
2022-12-12 18:18:21.864724	Iteration 5600 	loss 2.5132155418395996
2022-12-12 18:18:24.089617	Iteration 5700 	loss 2.1972291469573975
2022-12-12 18:18:26.336194	Iteration 5800 	loss 2.5635552406311035
2022-12-12 18:18:28.578771	Iteration 5900 	loss 2.327756404876709
2022-12-12 18:18:30.830473	Iteration 6000 	loss 2.591948986053467
2022-12-12 18:18:33.207261	Iteration 6100 	loss 2.405702829360962
2022-12-12 18:18:35.448555	Iteration 6200 	loss 2.3517990112304688
2022-12-12 18:18:37.665075	Iteration 6300 	loss 2.4329121112823486
2022-12-12 18:18:39.899535	Iteration 6400 	loss 2.105076551437378
2022-12-12 18:18:42.157288	Iteration 6500 	loss 2.0544967651367188
2022-12-12 18:18:44.527961	Iteration 6600 	loss 2.3582825660705566
2022-12-12 18:18:46.775644	Iteration 6700 	loss 2.4988467693328857
2022-12-12 18:18:49.003778	Iteration 6800 	loss 2.618704319000244
2022-12-12 18:18:51.227749	Iteration 6900 	loss 2.397052764892578
2022-12-12 18:18:53.458355	Iteration 7000 	loss 2.3043906688690186
2022-12-12 18:18:55.838796	Iteration 7100 	loss 2.4618098735809326
2022-12-12 18:18:58.079599	Iteration 7200 	loss 2.327759265899658
2022-12-12 18:19:00.334304	Iteration 7300 	loss 2.2726964950561523
2022-12-12 18:19:02.567430	Iteration 7400 	loss 2.3594393730163574
2022-12-12 18:19:04.816995	Iteration 7500 	loss 2.5690712928771973
2022-12-12 18:19:07.188433	Iteration 7600 	loss 2.1539127826690674
2022-12-12 18:19:09.428468	Iteration 7700 	loss 2.2404582500457764
2022-12-12 18:19:11.667324	Iteration 7800 	loss 2.379732131958008
2022-12-12 18:19:13.892731	Iteration 7900 	loss 2.1519274711608887
2022-12-12 18:19:16.127395	Iteration 8000 	loss 2.727222442626953
2022-12-12 18:19:18.482269	Iteration 8100 	loss 2.2811977863311768
2022-12-12 18:19:20.712693	Iteration 8200 	loss 1.9087684154510498
2022-12-12 18:19:22.940007	Iteration 8300 	loss 2.086890935897827
2022-12-12 18:19:25.180075	Iteration 8400 	loss 2.2770678997039795
2022-12-12 18:19:27.392795	Iteration 8500 	loss 2.4910888671875
2022-12-12 18:19:29.897150	Iteration 8600 	loss 2.3426432609558105
2022-12-12 18:19:32.121036	Iteration 8700 	loss 2.1279804706573486
2022-12-12 18:19:34.353259	Iteration 8800 	loss 2.101694345474243
2022-12-12 18:19:36.580745	Iteration 8900 	loss 2.2273662090301514
2022-12-12 18:19:38.811729	Iteration 9000 	loss 2.208481788635254
2022-12-12 18:19:41.163348	Iteration 9100 	loss 2.3024702072143555
2022-12-12 18:19:43.413111	Iteration 9200 	loss 2.4387452602386475
2022-12-12 18:19:45.653448	Iteration 9300 	loss 1.8911563158035278
2022-12-12 18:19:48.039961	Iteration 9400 	loss 2.6292054653167725
2022-12-12 18:19:50.541702	Iteration 9500 	loss 2.4606339931488037
2022-12-12 18:19:52.963650	Iteration 9600 	loss 2.5333476066589355
2022-12-12 18:19:55.216745	Iteration 9700 	loss 2.304509162902832
2022-12-12 18:19:57.476529	Iteration 9800 	loss 2.0533554553985596
2022-12-12 18:19:59.837252	Iteration 9900 	loss 2.2887520790100098
2022-12-12 18:20:02.099893	Iteration 10000 	loss 2.032869815826416
2022-12-12 18:20:04.499054	Iteration 10100 	loss 2.4946579933166504
2022-12-12 18:20:06.766683	Iteration 10200 	loss 2.6153271198272705
2022-12-12 18:20:08.995183	Iteration 10300 	loss 2.7244720458984375
2022-12-12 18:20:11.206426	Iteration 10400 	loss 2.027838945388794
2022-12-12 18:20:13.433055	Iteration 10500 	loss 2.512665033340454
2022-12-12 18:20:15.788319	Iteration 10600 	loss 2.353470802307129
2022-12-12 18:20:18.034492	Iteration 10700 	loss 2.2369706630706787
2022-12-12 18:20:20.236694	Iteration 10800 	loss 2.1504955291748047
2022-12-12 18:20:22.439467	Iteration 10900 	loss 2.4444642066955566
2022-12-12 18:20:24.687843	Iteration 11000 	loss 2.0736379623413086
2022-12-12 18:20:27.037280	Iteration 11100 	loss 2.3104004859924316
2022-12-12 18:20:29.309282	Iteration 11200 	loss 2.2841451168060303
2022-12-12 18:20:31.631316	Iteration 11300 	loss 2.3777029514312744
2022-12-12 18:20:33.916403	Iteration 11400 	loss 2.2723896503448486
2022-12-12 18:20:36.313094	Iteration 11500 	loss 2.1607792377471924
2022-12-12 18:20:38.688216	Iteration 11600 	loss 2.234586715698242
2022-12-12 18:20:41.093245	Iteration 11700 	loss 2.0757899284362793
2022-12-12 18:20:43.318272	Iteration 11800 	loss 2.2145485877990723
2022-12-12 18:20:45.586886	Iteration 11900 	loss 2.5187082290649414
2022-12-12 18:20:47.845043	Iteration 12000 	loss 2.4909937381744385
2022-12-12 18:20:50.234427	Iteration 12100 	loss 2.4465928077697754
2022-12-12 18:20:52.464249	Iteration 12200 	loss 2.1747660636901855
2022-12-12 18:20:54.683635	Iteration 12300 	loss 2.207353115081787
2022-12-12 18:20:56.911072	Iteration 12400 	loss 2.223864793777466
2022-12-12 18:20:59.123019	Iteration 12500 	loss 2.4950900077819824
2022-12-12 18:21:01.667980	Iteration 12600 	loss 1.9377895593643188
2022-12-12 18:21:03.885669	Iteration 12700 	loss 1.8893647193908691
2022-12-12 18:21:06.127466	Iteration 12800 	loss 1.826736569404602
2022-12-12 18:21:08.352765	Iteration 12900 	loss 2.327301025390625
2022-12-12 18:21:10.553426	Iteration 13000 	loss 2.162837028503418
2022-12-12 18:21:12.899808	Iteration 13100 	loss 2.058948040008545
2022-12-12 18:21:15.114976	Iteration 13200 	loss 2.3331644535064697
2022-12-12 18:21:17.342609	Iteration 13300 	loss 2.3057854175567627
2022-12-12 18:21:19.556433	Iteration 13400 	loss 2.386826515197754
2022-12-12 18:21:21.858685	Iteration 13500 	loss 2.024186372756958
2022-12-12 18:21:24.308456	Iteration 13600 	loss 2.291410446166992
2022-12-12 18:21:26.574671	Iteration 13700 	loss 2.754213333129883
2022-12-12 18:21:28.821775	Iteration 13800 	loss 2.159848213195801
2022-12-12 18:21:31.234106	Iteration 13900 	loss 2.3915038108825684
2022-12-12 18:21:33.523581	Iteration 14000 	loss 2.049163341522217
2022-12-12 18:21:36.092165	Iteration 14100 	loss 1.9294664859771729
2022-12-12 18:21:38.398030	Iteration 14200 	loss 2.224144697189331
2022-12-12 18:21:40.619377	Iteration 14300 	loss 2.3773086071014404
2022-12-12 18:21:42.891841	Iteration 14400 	loss 2.402101516723633
2022-12-12 18:21:45.128906	Iteration 14500 	loss 1.9828203916549683
2022-12-12 18:21:47.534161	Iteration 14600 	loss 2.237612009048462
2022-12-12 18:21:49.747369	Iteration 14700 	loss 2.2388229370117188
2022-12-12 18:21:51.968581	Iteration 14800 	loss 2.403567314147949
2022-12-12 18:21:54.191594	Iteration 14900 	loss 2.040238857269287
2022-12-12 18:21:56.414794	Iteration 15000 	loss 2.5316426753997803
2022-12-12 18:21:58.781814	Iteration 15100 	loss 2.2614126205444336
2022-12-12 18:22:01.041943	Iteration 15200 	loss 2.210014581680298
2022-12-12 18:22:03.329965	Iteration 15300 	loss 2.0603981018066406
2022-12-12 18:22:05.637167	Iteration 15400 	loss 2.3139641284942627
2022-12-12 18:22:07.856408	Iteration 15500 	loss 2.263824701309204
2022-12-12 18:22:10.224549	Iteration 15600 	loss 2.1682350635528564
2022-12-12 18:22:12.443486	Iteration 15700 	loss 2.301501989364624
2022-12-12 18:22:14.663694	Iteration 15800 	loss 2.0587215423583984
2022-12-12 18:22:16.904073	Iteration 15900 	loss 2.195720672607422
2022-12-12 18:22:19.159948	Iteration 16000 	loss 2.2838919162750244
2022-12-12 18:22:21.582484	Iteration 16100 	loss 2.1812736988067627
2022-12-12 18:22:23.801110	Iteration 16200 	loss 2.4710350036621094
2022-12-12 18:22:26.059021	Iteration 16300 	loss 2.281406879425049
2022-12-12 18:22:28.307726	Iteration 16400 	loss 2.4221436977386475
2022-12-12 18:22:30.554394	Iteration 16500 	loss 2.4581642150878906
2022-12-12 18:22:32.941775	Iteration 16600 	loss 1.8261160850524902
2022-12-12 18:22:35.172540	Iteration 16700 	loss 2.2706146240234375
2022-12-12 18:22:37.387968	Iteration 16800 	loss 2.3665552139282227
2022-12-12 18:22:39.641259	Iteration 16900 	loss 2.274850606918335
2022-12-12 18:22:41.835997	Iteration 17000 	loss 2.2822089195251465
2022-12-12 18:22:44.216304	Iteration 17100 	loss 1.9512089490890503
2022-12-12 18:22:46.449047	Iteration 17200 	loss 2.3462486267089844
2022-12-12 18:22:48.675017	Iteration 17300 	loss 2.6759326457977295
2022-12-12 18:22:50.885377	Iteration 17400 	loss 2.423570394515991
2022-12-12 18:22:53.105682	Iteration 17500 	loss 2.1336536407470703
2022-12-12 18:22:55.735504	Iteration 17600 	loss 2.4175355434417725
2022-12-12 18:22:57.928060	Iteration 17700 	loss 2.5312840938568115
2022-12-12 18:23:00.143472	Iteration 17800 	loss 2.4296555519104004
2022-12-12 18:23:02.366095	Iteration 17900 	loss 2.1094937324523926
2022-12-12 18:23:04.599878	Iteration 18000 	loss 2.5136566162109375
2022-12-12 18:23:07.035502	Iteration 18100 	loss 2.1949315071105957
2022-12-12 18:23:09.274446	Iteration 18200 	loss 2.3816840648651123
2022-12-12 18:23:11.637899	Iteration 18300 	loss 2.451702117919922
2022-12-12 18:23:13.900872	Iteration 18400 	loss 2.4781267642974854
2022-12-12 18:23:16.301785	Iteration 18500 	loss 2.3490824699401855
2022-12-12 18:23:18.742768	Iteration 18600 	loss 2.7175419330596924
2022-12-12 18:23:20.967585	Iteration 18700 	loss 2.333996057510376
2022-12-12 18:23:23.320782	Iteration 18800 	loss 2.7209112644195557
2022-12-12 18:23:25.653340	Iteration 18900 	loss 2.4651939868927
2022-12-12 18:23:27.955461	Iteration 19000 	loss 2.1974830627441406
2022-12-12 18:23:30.303776	Iteration 19100 	loss 1.6694046258926392
2022-12-12 18:23:32.481629	Iteration 19200 	loss 2.246204376220703
2022-12-12 18:23:34.650127	Iteration 19300 	loss 2.2163755893707275
2022-12-12 18:23:36.909338	Iteration 19400 	loss 2.397980213165283
2022-12-12 18:23:39.136217	Iteration 19500 	loss 2.3272597789764404
2022-12-12 18:23:41.483878	Iteration 19600 	loss 2.3252639770507812
2022-12-12 18:23:43.742099	Iteration 19700 	loss 2.050382375717163
2022-12-12 18:23:45.946560	Iteration 19800 	loss 2.3663370609283447
2022-12-12 18:23:48.157915	Iteration 19900 	loss 2.1887035369873047
2022-12-12 18:23:50.352891	Iteration 20000 	loss 2.1507928371429443
2022-12-12 18:23:52.694908	Iteration 20100 	loss 2.4659006595611572
2022-12-12 18:23:54.882378	Iteration 20200 	loss 2.61136531829834
2022-12-12 18:23:57.168513	Iteration 20300 	loss 2.6102356910705566
2022-12-12 18:23:59.583397	Iteration 20400 	loss 2.0881876945495605
2022-12-12 18:24:01.860194	Iteration 20500 	loss 2.1882824897766113
2022-12-12 18:24:04.185801	Iteration 20600 	loss 2.0221118927001953
2022-12-12 18:24:06.386770	Iteration 20700 	loss 2.6676223278045654
2022-12-12 18:24:08.618434	Iteration 20800 	loss 2.3748388290405273
2022-12-12 18:24:10.780999	Iteration 20900 	loss 1.9792895317077637
2022-12-12 18:24:12.964876	Iteration 21000 	loss 2.1200954914093018
2022-12-12 18:24:15.465875	Iteration 21100 	loss 2.517197370529175
2022-12-12 18:24:17.672415	Iteration 21200 	loss 2.067917823791504
2022-12-12 18:24:19.869965	Iteration 21300 	loss 2.4564249515533447
2022-12-12 18:24:22.109932	Iteration 21400 	loss 2.621692419052124
2022-12-12 18:24:24.291996	Iteration 21500 	loss 1.833753228187561
2022-12-12 18:24:26.668924	Iteration 21600 	loss 2.4703619480133057
2022-12-12 18:24:28.905117	Iteration 21700 	loss 2.1322507858276367
2022-12-12 18:24:31.120739	Iteration 21800 	loss 2.4134020805358887
2022-12-12 18:24:33.354542	Iteration 21900 	loss 2.391948699951172
2022-12-12 18:24:35.594876	Iteration 22000 	loss 1.9667744636535645
2022-12-12 18:24:37.964939	Iteration 22100 	loss 2.3102610111236572
2022-12-12 18:24:40.177419	Iteration 22200 	loss 2.084047794342041
2022-12-12 19:49:40.815377	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-12-12 19:49:40.815831	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-12-12 19:51:41.910395	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-12-12 19:51:41.910750	cuda unavailable
2022-12-12 19:51:44.103263	Iteration 1 	loss 2.5674383640289307
2022-12-12 19:51:46.395930	Iteration 100 	loss 2.4461662769317627
2022-12-12 19:51:48.773826	Iteration 200 	loss 2.4075706005096436
2022-12-12 19:51:51.234261	Iteration 300 	loss 2.525055170059204
2022-12-12 19:51:53.602280	Iteration 400 	loss 2.458676338195801
2022-12-12 19:51:55.851621	Iteration 500 	loss 2.3597488403320312
2022-12-12 19:51:58.255350	Iteration 600 	loss 2.2232234477996826
2022-12-12 19:52:00.525211	Iteration 700 	loss 2.5712625980377197
2022-12-12 19:52:02.763723	Iteration 800 	loss 2.4642412662506104
2022-12-12 19:52:05.049143	Iteration 900 	loss 2.5422918796539307
2022-12-12 19:52:07.292418	Iteration 1000 	loss 2.461791515350342
2022-12-12 19:52:09.665132	Iteration 1100 	loss 2.394052505493164
2022-12-12 19:52:11.890446	Iteration 1200 	loss 2.4803695678710938
2022-12-12 19:52:14.163189	Iteration 1300 	loss 2.363532781600952
2022-12-12 19:52:16.433629	Iteration 1400 	loss 2.5631608963012695
2022-12-12 19:52:18.738399	Iteration 1500 	loss 2.159287452697754
2022-12-12 19:52:21.094418	Iteration 1600 	loss 2.4111289978027344
2022-12-12 19:52:23.314503	Iteration 1700 	loss 2.6829094886779785
2022-12-12 19:52:25.644590	Iteration 1800 	loss 2.5416712760925293
2022-12-12 19:52:27.893098	Iteration 1900 	loss 2.501452684402466
2022-12-12 19:52:30.126000	Iteration 2000 	loss 2.3328824043273926
2022-12-12 19:52:32.539705	Iteration 2100 	loss 2.4502978324890137
2022-12-12 19:52:34.778199	Iteration 2200 	loss 2.4077208042144775
2022-12-12 19:52:37.002399	Iteration 2300 	loss 2.3965132236480713
2022-12-12 19:52:39.250920	Iteration 2400 	loss 2.1515026092529297
2022-12-12 19:52:41.552374	Iteration 2500 	loss 2.4718470573425293
2022-12-12 19:52:44.010721	Iteration 2600 	loss 2.4638864994049072
2022-12-12 19:52:46.238854	Iteration 2700 	loss 2.461588144302368
2022-12-12 19:52:48.467675	Iteration 2800 	loss 2.404658555984497
2022-12-12 19:52:50.671014	Iteration 2900 	loss 2.313751697540283
2022-12-12 19:52:52.896129	Iteration 3000 	loss 2.3698084354400635
2022-12-12 19:52:55.244023	Iteration 3100 	loss 2.2861745357513428
2022-12-12 19:52:57.467716	Iteration 3200 	loss 2.1802122592926025
2022-12-12 19:52:59.700181	Iteration 3300 	loss 2.433907985687256
2022-12-12 19:53:01.909667	Iteration 3400 	loss 2.3754708766937256
2022-12-12 19:53:04.122827	Iteration 3500 	loss 2.1539881229400635
2022-12-12 19:53:06.475106	Iteration 3600 	loss 2.3756747245788574
2022-12-12 19:53:08.719022	Iteration 3700 	loss 2.295322895050049
2022-12-12 19:53:10.932295	Iteration 3800 	loss 2.2593812942504883
2022-12-12 19:53:13.137702	Iteration 3900 	loss 2.5811269283294678
2022-12-12 19:53:15.350013	Iteration 4000 	loss 2.4028072357177734
2022-12-12 19:53:17.708330	Iteration 4100 	loss 2.3264029026031494
2022-12-12 19:53:19.919766	Iteration 4200 	loss 2.665027141571045
2022-12-12 19:53:22.128607	Iteration 4300 	loss 2.5235342979431152
2022-12-12 19:53:24.332216	Iteration 4400 	loss 2.4767768383026123
2022-12-12 19:53:26.545723	Iteration 4500 	loss 2.7144410610198975
2022-12-12 19:53:28.895641	Iteration 4600 	loss 2.5119853019714355
2022-12-12 19:53:31.102966	Iteration 4700 	loss 2.1125481128692627
2022-12-12 19:53:33.319089	Iteration 4800 	loss 2.328057050704956
2022-12-12 19:53:35.528220	Iteration 4900 	loss 2.2558681964874268
2022-12-12 19:53:37.738473	Iteration 5000 	loss 2.4859344959259033
2022-12-12 19:53:40.174267	Iteration 5100 	loss 2.1768972873687744
2022-12-12 19:53:42.387925	Iteration 5200 	loss 2.233444929122925
2022-12-12 19:53:44.590261	Iteration 5300 	loss 2.1793007850646973
2022-12-12 19:53:46.803582	Iteration 5400 	loss 1.976108193397522
2022-12-12 19:53:49.013892	Iteration 5500 	loss 2.2513880729675293
2022-12-12 19:53:51.365024	Iteration 5600 	loss 2.166830539703369
2022-12-12 19:53:53.563278	Iteration 5700 	loss 2.2881875038146973
2022-12-12 19:53:55.778122	Iteration 5800 	loss 2.535231351852417
2022-12-12 19:53:57.983015	Iteration 5900 	loss 2.4801435470581055
2022-12-12 19:54:00.184624	Iteration 6000 	loss 2.583045721054077
2022-12-12 19:54:02.520334	Iteration 6100 	loss 2.2530722618103027
2022-12-12 19:54:04.741413	Iteration 6200 	loss 2.4227895736694336
2022-12-12 19:54:06.956160	Iteration 6300 	loss 2.2817165851593018
2022-12-12 19:54:09.171109	Iteration 6400 	loss 2.1232306957244873
2022-12-12 19:54:11.393689	Iteration 6500 	loss 2.1205859184265137
2022-12-12 19:54:13.728312	Iteration 6600 	loss 2.355367660522461
2022-12-12 19:54:15.929669	Iteration 6700 	loss 2.1827268600463867
2022-12-12 19:54:18.136569	Iteration 6800 	loss 2.2887015342712402
2022-12-12 19:54:20.350474	Iteration 6900 	loss 2.4092657566070557
2022-12-12 19:54:22.562553	Iteration 7000 	loss 2.2227282524108887
2022-12-12 19:54:24.899905	Iteration 7100 	loss 2.4576618671417236
2022-12-12 19:54:27.078524	Iteration 7200 	loss 2.319847583770752
2022-12-12 19:54:29.281645	Iteration 7300 	loss 2.248098850250244
2022-12-12 19:54:31.509091	Iteration 7400 	loss 2.6448731422424316
2022-12-12 19:54:33.721538	Iteration 7500 	loss 2.2783803939819336
2022-12-12 19:54:36.048839	Iteration 7600 	loss 2.5051653385162354
2022-12-12 19:54:38.246784	Iteration 7700 	loss 2.361783504486084
2022-12-12 19:54:40.447398	Iteration 7800 	loss 2.142850875854492
2022-12-12 19:54:42.645547	Iteration 7900 	loss 2.5804643630981445
2022-12-12 19:54:44.852861	Iteration 8000 	loss 2.2145681381225586
2022-12-12 19:54:47.183268	Iteration 8100 	loss 2.5017426013946533
2022-12-12 19:54:49.380348	Iteration 8200 	loss 2.2050745487213135
2022-12-12 19:54:51.573153	Iteration 8300 	loss 2.369072437286377
2022-12-12 19:54:53.775618	Iteration 8400 	loss 2.2452456951141357
2022-12-12 19:54:55.969597	Iteration 8500 	loss 2.1578853130340576
2022-12-12 19:54:58.411125	Iteration 8600 	loss 2.4089982509613037
2022-12-12 19:55:00.598141	Iteration 8700 	loss 2.519538164138794
2022-12-12 19:55:02.778102	Iteration 8800 	loss 2.29296875
2022-12-12 19:55:04.966851	Iteration 8900 	loss 2.5335795879364014
2022-12-12 19:55:07.163609	Iteration 9000 	loss 2.2229232788085938
2022-12-12 19:55:09.489408	Iteration 9100 	loss 2.421956777572632
2022-12-12 19:55:11.687310	Iteration 9200 	loss 2.389206886291504
2022-12-12 19:55:13.876166	Iteration 9300 	loss 2.315265655517578
2022-12-12 19:55:16.058425	Iteration 9400 	loss 2.157336711883545
2022-12-12 19:55:18.248107	Iteration 9500 	loss 2.2808241844177246
2022-12-12 19:55:20.560410	Iteration 9600 	loss 2.375195026397705
2022-12-12 19:55:22.743918	Iteration 9700 	loss 2.199814796447754
2022-12-12 19:55:24.921628	Iteration 9800 	loss 2.5241289138793945
2022-12-12 19:55:27.115547	Iteration 9900 	loss 2.405488967895508
2022-12-12 19:55:29.297500	Iteration 10000 	loss 2.4235472679138184
2022-12-12 19:55:31.619005	Iteration 10100 	loss 1.8787119388580322
2022-12-12 19:55:33.806357	Iteration 10200 	loss 2.2753384113311768
2022-12-12 19:55:35.979571	Iteration 10300 	loss 2.2067394256591797
2022-12-12 19:55:38.160853	Iteration 10400 	loss 2.1527953147888184
2022-12-12 19:55:40.345149	Iteration 10500 	loss 2.529111385345459
2022-12-12 19:55:42.657068	Iteration 10600 	loss 2.4979100227355957
2022-12-12 19:55:44.850873	Iteration 10700 	loss 2.359531879425049
2022-12-12 19:55:47.034687	Iteration 10800 	loss 2.5514283180236816
2022-12-12 19:55:49.232482	Iteration 10900 	loss 2.1058313846588135
2022-12-12 19:55:51.419539	Iteration 11000 	loss 1.7997580766677856
2022-12-12 19:55:53.800968	Iteration 11100 	loss 2.099341630935669
2022-12-12 19:55:55.999624	Iteration 11200 	loss 1.8519154787063599
2022-12-12 19:55:58.173514	Iteration 11300 	loss 2.277385711669922
2022-12-12 19:56:00.359830	Iteration 11400 	loss 2.4959659576416016
2022-12-12 19:56:02.542931	Iteration 11500 	loss 2.294217824935913
2022-12-12 19:56:04.880963	Iteration 11600 	loss 2.6582753658294678
2022-12-12 19:56:07.102695	Iteration 11700 	loss 2.2261569499969482
2022-12-12 19:56:09.303991	Iteration 11800 	loss 2.3270604610443115
2022-12-12 19:56:11.503095	Iteration 11900 	loss 2.2111809253692627
2022-12-12 19:56:13.713029	Iteration 12000 	loss 2.5545856952667236
2022-12-12 19:56:16.045617	Iteration 12100 	loss 2.6069986820220947
2022-12-12 19:56:18.254175	Iteration 12200 	loss 2.124105453491211
2022-12-12 19:56:20.443219	Iteration 12300 	loss 2.7334303855895996
2022-12-12 19:56:22.629436	Iteration 12400 	loss 2.4988322257995605
2022-12-12 19:56:24.828400	Iteration 12500 	loss 2.2966315746307373
2022-12-12 19:56:27.326485	Iteration 12600 	loss 2.1987481117248535
2022-12-12 19:56:29.549911	Iteration 12700 	loss 2.1612603664398193
2022-12-12 19:56:31.747419	Iteration 12800 	loss 2.3734683990478516
2022-12-12 19:56:33.942391	Iteration 12900 	loss 2.492501735687256
2022-12-12 19:56:36.140635	Iteration 13000 	loss 2.2247910499572754
2022-12-12 19:56:38.471786	Iteration 13100 	loss 2.4391205310821533
2022-12-12 19:56:40.657462	Iteration 13200 	loss 2.384988307952881
2022-12-12 19:56:42.852951	Iteration 13300 	loss 2.2736260890960693
2022-12-12 19:56:45.034143	Iteration 13400 	loss 2.6697418689727783
2022-12-12 19:56:47.236301	Iteration 13500 	loss 2.363203287124634
2022-12-12 19:56:49.554555	Iteration 13600 	loss 2.4366393089294434
2022-12-12 19:56:51.749164	Iteration 13700 	loss 2.1221225261688232
2022-12-12 19:56:53.937342	Iteration 13800 	loss 2.2851920127868652
2022-12-12 19:56:56.134975	Iteration 13900 	loss 2.3782200813293457
2022-12-12 19:56:58.322948	Iteration 14000 	loss 2.254051685333252
2022-12-12 19:57:00.641644	Iteration 14100 	loss 2.5218281745910645
2022-12-12 19:57:02.830897	Iteration 14200 	loss 2.3300528526306152
2022-12-12 19:57:05.077576	Iteration 14300 	loss 2.318188428878784
2022-12-12 19:57:07.266121	Iteration 14400 	loss 2.0748400688171387
2022-12-12 19:57:09.445220	Iteration 14500 	loss 2.103703260421753
2022-12-12 19:57:11.798901	Iteration 14600 	loss 2.3632524013519287
2022-12-12 19:57:14.088043	Iteration 14700 	loss 2.4651565551757812
2022-12-12 19:57:16.347278	Iteration 14800 	loss 2.179719924926758
2022-12-12 19:57:18.545344	Iteration 14900 	loss 2.1414756774902344
2022-12-12 19:57:20.731712	Iteration 15000 	loss 2.4981656074523926
2022-12-12 19:57:23.055497	Iteration 15100 	loss 2.573621988296509
2022-12-12 19:57:25.248921	Iteration 15200 	loss 1.9885902404785156
2022-12-12 19:57:27.438830	Iteration 15300 	loss 2.267392635345459
2022-12-12 19:57:29.642286	Iteration 15400 	loss 2.5386083126068115
2022-12-12 19:57:31.824463	Iteration 15500 	loss 2.444627523422241
2022-12-12 19:57:34.161898	Iteration 15600 	loss 2.2248282432556152
2022-12-12 19:57:36.353763	Iteration 15700 	loss 2.1545567512512207
2022-12-12 19:57:38.545789	Iteration 15800 	loss 2.3926374912261963
2022-12-12 19:57:40.735386	Iteration 15900 	loss 2.4140608310699463
2022-12-12 19:57:42.934443	Iteration 16000 	loss 2.4067840576171875
2022-12-12 19:57:45.271846	Iteration 16100 	loss 2.7271597385406494
2022-12-12 19:57:47.469079	Iteration 16200 	loss 2.166928768157959
2022-12-12 19:57:49.673731	Iteration 16300 	loss 2.0103871822357178
2022-12-12 19:57:51.869589	Iteration 16400 	loss 2.166195869445801
2022-12-12 19:57:54.147107	Iteration 16500 	loss 2.5086822509765625
2022-12-12 19:57:56.509809	Iteration 16600 	loss 2.31941294670105
2022-12-12 19:57:58.844621	Iteration 16700 	loss 2.3979599475860596
2022-12-12 19:58:01.101232	Iteration 16800 	loss 2.311600923538208
2022-12-12 19:58:03.392095	Iteration 16900 	loss 2.6732709407806396
2022-12-12 19:58:05.648270	Iteration 17000 	loss 2.524435520172119
2022-12-12 19:58:08.047529	Iteration 17100 	loss 1.918129563331604
2022-12-12 19:58:10.273276	Iteration 17200 	loss 2.536374807357788
2022-12-12 19:58:12.613979	Iteration 17300 	loss 2.370964765548706
2022-12-12 19:58:15.000863	Iteration 17400 	loss 2.384348154067993
2022-12-12 19:58:17.228229	Iteration 17500 	loss 2.2446727752685547
2022-12-12 19:58:19.591533	Iteration 17600 	loss 2.4424943923950195
2022-12-12 19:58:21.811665	Iteration 17700 	loss 2.3201961517333984
2022-12-12 19:58:24.068688	Iteration 17800 	loss 2.1183691024780273
2022-12-12 19:58:26.339634	Iteration 17900 	loss 1.9928841590881348
2022-12-12 19:58:28.575488	Iteration 18000 	loss 2.2423834800720215
2022-12-12 19:58:31.141280	Iteration 18100 	loss 1.868436336517334
2022-12-12 19:58:33.367192	Iteration 18200 	loss 2.2946152687072754
2022-12-12 19:58:35.620486	Iteration 18300 	loss 2.3633649349212646
2022-12-12 19:58:37.834415	Iteration 18400 	loss 2.336982250213623
2022-12-12 19:58:40.061311	Iteration 18500 	loss 2.237361431121826
2022-12-12 19:58:42.425728	Iteration 18600 	loss 2.3829619884490967
2022-12-12 19:58:44.642258	Iteration 18700 	loss 2.1645724773406982
2022-12-12 19:58:46.855291	Iteration 18800 	loss 2.310359001159668
2022-12-12 19:58:49.072644	Iteration 18900 	loss 2.309278964996338
2022-12-12 19:58:51.283649	Iteration 19000 	loss 2.2094016075134277
2022-12-12 19:58:53.631284	Iteration 19100 	loss 2.2086682319641113
2022-12-12 19:58:55.880495	Iteration 19200 	loss 2.4017629623413086
2022-12-12 19:58:58.111593	Iteration 19300 	loss 2.2463791370391846
2022-12-12 19:59:00.350228	Iteration 19400 	loss 2.1410775184631348
2022-12-12 19:59:02.575202	Iteration 19500 	loss 2.4976375102996826
2022-12-12 19:59:04.928669	Iteration 19600 	loss 2.270707130432129
2022-12-12 19:59:07.148157	Iteration 19700 	loss 2.322812795639038
2022-12-12 19:59:09.359150	Iteration 19800 	loss 2.026411294937134
2022-12-12 19:59:11.595875	Iteration 19900 	loss 2.2691617012023926
2022-12-12 19:59:13.807087	Iteration 20000 	loss 2.063441514968872
2022-12-12 19:59:16.151849	Iteration 20100 	loss 2.5003058910369873
2022-12-12 19:59:18.389325	Iteration 20200 	loss 1.7152152061462402
2022-12-12 19:59:20.621495	Iteration 20300 	loss 1.6062335968017578
2022-12-12 19:59:22.846100	Iteration 20400 	loss 2.11793851852417
2022-12-12 19:59:25.042517	Iteration 20500 	loss 1.9014769792556763
2022-12-12 19:59:27.388389	Iteration 20600 	loss 2.285496473312378
2022-12-12 19:59:29.599191	Iteration 20700 	loss 2.101891040802002
2022-12-12 19:59:31.831859	Iteration 20800 	loss 2.181145429611206
2022-12-12 19:59:34.047053	Iteration 20900 	loss 2.3283417224884033
2022-12-12 19:59:36.267161	Iteration 21000 	loss 2.500515937805176
2022-12-12 19:59:38.603077	Iteration 21100 	loss 2.5290722846984863
2022-12-12 19:59:40.810746	Iteration 21200 	loss 2.3724513053894043
2022-12-12 19:59:43.022002	Iteration 21300 	loss 2.051727056503296
2022-12-12 19:59:45.209183	Iteration 21400 	loss 2.382575035095215
2022-12-12 19:59:47.393733	Iteration 21500 	loss 2.416658878326416
2022-12-12 19:59:49.714674	Iteration 21600 	loss 2.3484444618225098
2022-12-12 19:59:51.904744	Iteration 21700 	loss 1.9799940586090088
2022-12-12 19:59:54.105700	Iteration 21800 	loss 1.9556273221969604
2022-12-12 19:59:56.298215	Iteration 21900 	loss 2.271536350250244
2022-12-12 19:59:58.489207	Iteration 22000 	loss 2.205188751220703
2022-12-12 20:00:00.828411	Iteration 22100 	loss 2.2182559967041016
2022-12-12 20:00:03.026441	Iteration 22200 	loss 2.6087751388549805
2022-12-12 20:00:05.222499	Iteration 22300 	loss 2.1414380073547363
2022-12-12 20:00:07.424383	Iteration 22400 	loss 2.023099422454834
2022-12-12 20:00:09.634268	Iteration 22500 	loss 2.1098570823669434
2022-12-12 20:00:11.962893	Iteration 22600 	loss 2.3719308376312256
2022-12-12 20:00:14.161136	Iteration 22700 	loss 2.3372130393981934
2022-12-12 20:00:16.359623	Iteration 22800 	loss 2.303819179534912
2022-12-12 20:00:18.567155	Iteration 22900 	loss 2.18322491645813
2022-12-12 20:00:20.770574	Iteration 23000 	loss 2.248129367828369
2022-12-12 20:00:23.104871	Iteration 23100 	loss 2.0532193183898926
2022-12-12 20:00:25.299479	Iteration 23200 	loss 2.511725425720215
2022-12-12 20:00:27.490176	Iteration 23300 	loss 2.2565958499908447
2022-12-12 20:00:29.688302	Iteration 23400 	loss 2.75173020362854
2022-12-12 20:00:31.881752	Iteration 23500 	loss 2.3610661029815674
2022-12-12 20:00:34.216627	Iteration 23600 	loss 2.138519287109375
2022-12-12 20:00:36.417865	Iteration 23700 	loss 2.316972017288208
2022-12-12 20:00:38.606073	Iteration 23800 	loss 2.234236001968384
2022-12-12 20:00:40.797611	Iteration 23900 	loss 2.244593381881714
2022-12-12 20:00:43.014100	Iteration 24000 	loss 2.323294162750244
2022-12-12 20:00:45.618415	Iteration 24100 	loss 2.3644447326660156
2022-12-12 20:00:47.824288	Iteration 24200 	loss 2.3939523696899414
2022-12-12 20:00:50.005344	Iteration 24300 	loss 2.308396339416504
2022-12-12 20:00:52.200348	Iteration 24400 	loss 2.2180488109588623
2022-12-12 20:00:54.397231	Iteration 24500 	loss 2.2900383472442627
2022-12-12 20:00:56.733744	Iteration 24600 	loss 2.258793830871582
2022-12-12 20:00:58.929751	Iteration 24700 	loss 2.161189079284668
2022-12-12 20:01:01.126447	Iteration 24800 	loss 2.4564669132232666
2022-12-12 20:01:03.311035	Iteration 24900 	loss 2.3326773643493652
2022-12-12 20:01:05.510562	Iteration 25000 	loss 2.3230957984924316
2022-12-12 20:01:07.839598	Iteration 25100 	loss 2.045478343963623
2022-12-12 20:01:10.034665	Iteration 25200 	loss 2.021064519882202
2022-12-12 20:01:12.238364	Iteration 25300 	loss 2.389796733856201
2022-12-12 20:01:14.424818	Iteration 25400 	loss 2.1770179271698
2022-12-12 20:01:16.619881	Iteration 25500 	loss 1.9452788829803467
2022-12-12 20:01:18.942069	Iteration 25600 	loss 2.3734474182128906
2022-12-12 20:01:21.146708	Iteration 25700 	loss 2.015902280807495
2022-12-12 20:01:23.353467	Iteration 25800 	loss 2.2696704864501953
2022-12-12 20:01:25.545347	Iteration 25900 	loss 2.035492420196533
2022-12-12 20:01:27.738876	Iteration 26000 	loss 2.1835856437683105
2022-12-12 20:01:30.076594	Iteration 26100 	loss 1.9966704845428467
2022-12-12 20:01:32.270605	Iteration 26200 	loss 2.376784324645996
2022-12-12 20:01:34.477102	Iteration 26300 	loss 2.246765375137329
2022-12-12 20:01:36.670947	Iteration 26400 	loss 2.5681936740875244
2022-12-12 20:01:38.869083	Iteration 26500 	loss 2.368227005004883
2022-12-12 20:01:41.201953	Iteration 26600 	loss 2.0179145336151123
2022-12-12 20:01:43.399114	Iteration 26700 	loss 2.310333013534546
2022-12-12 20:01:45.600608	Iteration 26800 	loss 2.5090527534484863
2022-12-12 20:01:47.798836	Iteration 26900 	loss 2.094059944152832
2022-12-12 20:01:49.992317	Iteration 27000 	loss 2.2666115760803223
2022-12-12 20:01:52.320152	Iteration 27100 	loss 2.3704710006713867
2022-12-12 20:01:54.525382	Iteration 27200 	loss 2.459507703781128
2022-12-12 20:01:56.729463	Iteration 27300 	loss 2.5732381343841553
2022-12-12 20:01:58.942399	Iteration 27400 	loss 2.10674786567688
2022-12-12 20:02:01.141492	Iteration 27500 	loss 2.3008060455322266
2022-12-12 20:02:03.499234	Iteration 27600 	loss 2.1724417209625244
2022-12-12 20:02:05.721115	Iteration 27700 	loss 2.088801622390747
2022-12-12 20:02:07.949160	Iteration 27800 	loss 2.4314818382263184
2022-12-12 20:02:10.163358	Iteration 27900 	loss 2.414008617401123
2022-12-12 20:02:12.374989	Iteration 28000 	loss 2.5856215953826904
2022-12-12 20:02:14.717884	Iteration 28100 	loss 2.329174518585205
2022-12-12 20:02:16.922007	Iteration 28200 	loss 2.0808775424957275
2022-12-12 20:02:19.135450	Iteration 28300 	loss 2.4294755458831787
2022-12-12 20:02:21.340824	Iteration 28400 	loss 2.351785182952881
2022-12-12 20:02:23.555339	Iteration 28500 	loss 2.5292727947235107
2022-12-12 20:02:25.894311	Iteration 28600 	loss 2.7022392749786377
2022-12-12 20:02:28.102594	Iteration 28700 	loss 2.5131497383117676
2022-12-12 20:02:30.305522	Iteration 28800 	loss 2.259157180786133
2022-12-12 20:02:32.519356	Iteration 28900 	loss 2.059706687927246
2022-12-12 20:02:34.723852	Iteration 29000 	loss 2.1293766498565674
2022-12-12 20:02:37.068491	Iteration 29100 	loss 2.5855472087860107
2022-12-12 20:02:39.261471	Iteration 29200 	loss 1.8193504810333252
2022-12-12 20:02:41.473276	Iteration 29300 	loss 2.1053919792175293
2022-12-12 20:02:43.673797	Iteration 29400 	loss 2.673807144165039
2022-12-12 20:02:45.899603	Iteration 29500 	loss 2.628361225128174
2022-12-12 20:02:48.240422	Iteration 29600 	loss 1.6382439136505127
2022-12-12 20:02:50.449386	Iteration 29700 	loss 1.8497793674468994
2022-12-12 20:02:52.710765	Iteration 29800 	loss 2.2311980724334717
2022-12-12 20:02:54.960083	Iteration 29900 	loss 2.5617332458496094
2022-12-12 20:02:57.178705	Iteration 30000 	loss 1.8303176164627075
2022-12-12 20:02:59.520734	Iteration 30100 	loss 2.2839267253875732
2022-12-12 20:03:01.720787	Iteration 30200 	loss 2.1137921810150146
2022-12-12 20:03:03.924720	Iteration 30300 	loss 2.0634477138519287
2022-12-12 20:03:06.137494	Iteration 30400 	loss 2.5096020698547363
2022-12-12 20:03:08.337457	Iteration 30500 	loss 1.9162529706954956
2022-12-12 20:03:10.662865	Iteration 30600 	loss 2.3422908782958984
2022-12-12 20:03:12.854217	Iteration 30700 	loss 1.977850317955017
2022-12-12 20:03:15.049435	Iteration 30800 	loss 1.8335760831832886
2022-12-12 20:03:17.238803	Iteration 30900 	loss 2.143810749053955
2022-12-12 20:03:19.440590	Iteration 31000 	loss 2.4780569076538086
2022-12-12 20:03:21.766787	Iteration 31100 	loss 2.5144095420837402
2022-12-12 20:03:23.975013	Iteration 31200 	loss 2.0782291889190674
2022-12-12 20:03:26.169793	Iteration 31300 	loss 2.1630642414093018
2022-12-12 20:03:28.366633	Iteration 31400 	loss 2.310833215713501
2022-12-12 20:03:30.564187	Iteration 31500 	loss 2.0497994422912598
2022-12-12 20:03:32.921126	Iteration 31600 	loss 2.0300397872924805
2022-12-12 20:03:35.227046	Iteration 31700 	loss 2.235081911087036
2022-12-12 20:03:37.501516	Iteration 31800 	loss 2.608232021331787
2022-12-12 20:03:39.685297	Iteration 31900 	loss 2.1876566410064697
2022-12-12 20:03:41.872450	Iteration 32000 	loss 1.951744556427002
2022-12-12 20:03:44.563697	Iteration 32100 	loss 2.407566785812378
2022-12-12 20:03:46.749355	Iteration 32200 	loss 2.0527679920196533
2022-12-12 20:03:48.956576	Iteration 32300 	loss 2.380373239517212
2022-12-12 20:03:51.141676	Iteration 32400 	loss 2.8585071563720703
2022-12-12 20:03:53.339454	Iteration 32500 	loss 1.8676660060882568
2022-12-12 20:03:55.659346	Iteration 32600 	loss 2.4313790798187256
2022-12-12 20:03:57.860680	Iteration 32700 	loss 2.5861308574676514
2022-12-12 20:04:00.049439	Iteration 32800 	loss 2.533729314804077
2022-12-12 20:04:02.240841	Iteration 32900 	loss 2.3609752655029297
2022-12-12 20:04:04.429945	Iteration 33000 	loss 1.957839012145996
2022-12-12 20:04:06.750345	Iteration 33100 	loss 2.1560354232788086
2022-12-12 20:04:08.967869	Iteration 33200 	loss 2.6475706100463867
2022-12-12 20:04:11.158434	Iteration 33300 	loss 1.7271413803100586
2022-12-12 20:04:13.358899	Iteration 33400 	loss 1.9464502334594727
2022-12-12 20:04:15.549269	Iteration 33500 	loss 2.5262935161590576
2022-12-12 20:04:17.900109	Iteration 33600 	loss 2.0909340381622314
2022-12-12 20:04:20.105729	Iteration 33700 	loss 2.34824275970459
2022-12-12 20:04:22.314268	Iteration 33800 	loss 2.6494808197021484
2022-12-12 20:04:24.551036	Iteration 33900 	loss 1.87470281124115
2022-12-12 20:04:26.776427	Iteration 34000 	loss 2.396984100341797
2022-12-12 20:04:29.096946	Iteration 34100 	loss 2.122297525405884
2022-12-12 20:04:31.298806	Iteration 34200 	loss 2.74348521232605
2022-12-12 20:04:33.483534	Iteration 34300 	loss 2.3034210205078125
2022-12-12 20:04:35.676625	Iteration 34400 	loss 2.029083728790283
2022-12-12 20:04:37.876289	Iteration 34500 	loss 2.447279930114746
2022-12-12 20:04:40.197782	Iteration 34600 	loss 1.9891457557678223
2022-12-12 20:04:42.390017	Iteration 34700 	loss 2.1405234336853027
2022-12-12 20:04:44.613700	Iteration 34800 	loss 2.011655569076538
2022-12-12 20:04:47.161420	Iteration 34900 	loss 2.6573691368103027
2022-12-12 20:04:49.369086	Iteration 35000 	loss 2.106478214263916
2022-12-12 20:04:51.730820	Iteration 35100 	loss 2.0094733238220215
2022-12-12 20:04:53.930633	Iteration 35200 	loss 2.3553950786590576
2022-12-12 20:04:56.123383	Iteration 35300 	loss 2.3052279949188232
2022-12-12 20:04:58.332307	Iteration 35400 	loss 2.198319435119629
2022-12-12 20:05:00.553727	Iteration 35500 	loss 2.071302652359009
2022-12-12 20:05:02.861142	Iteration 35600 	loss 2.230581521987915
2022-12-12 20:05:05.044032	Iteration 35700 	loss 2.118387460708618
2022-12-12 20:05:07.240803	Iteration 35800 	loss 2.066516876220703
2022-12-12 20:05:09.444778	Iteration 35900 	loss 2.409879207611084
2022-12-12 20:05:11.742496	Iteration 36000 	loss 2.450194835662842
2022-12-12 20:05:14.134835	Iteration 36100 	loss 2.253084182739258
2022-12-12 20:05:16.325825	Iteration 36200 	loss 2.5196619033813477
2022-12-12 20:05:18.523372	Iteration 36300 	loss 1.9882667064666748
2022-12-12 20:05:20.719545	Iteration 36400 	loss 2.722330331802368
2022-12-12 20:05:22.943516	Iteration 36500 	loss 2.683037519454956
2022-12-12 20:05:25.270388	Iteration 36600 	loss 2.3607289791107178
2022-12-12 20:05:27.458084	Iteration 36700 	loss 1.943022608757019
2022-12-12 20:05:29.667610	Iteration 36800 	loss 2.575766086578369
2022-12-12 20:05:31.881793	Iteration 36900 	loss 1.8552966117858887
2022-12-12 20:05:34.087647	Iteration 37000 	loss 2.4142303466796875
2022-12-12 20:05:36.423974	Iteration 37100 	loss 2.340827703475952
2022-12-12 20:05:38.637056	Iteration 37200 	loss 2.4532718658447266
2022-12-12 20:05:40.916810	Iteration 37300 	loss 2.052685022354126
2022-12-12 20:05:43.126575	Iteration 37400 	loss 2.350909471511841
2022-12-12 20:05:45.303544	Iteration 37500 	loss 2.4663310050964355
2022-12-12 20:05:47.716536	Iteration 37600 	loss 2.839775562286377
2022-12-12 20:05:49.937572	Iteration 37700 	loss 2.032984733581543
2022-12-12 20:05:52.245373	Iteration 37800 	loss 2.5210089683532715
2022-12-12 20:05:54.468999	Iteration 37900 	loss 2.1160693168640137
2022-12-12 20:05:56.664769	Iteration 38000 	loss 1.9898028373718262
2022-12-12 20:05:59.039545	Iteration 38100 	loss 2.607555389404297
2022-12-12 20:06:01.241006	Iteration 38200 	loss 2.5761256217956543
2022-12-12 20:06:03.417100	Iteration 38300 	loss 2.3437092304229736
2022-12-12 20:06:05.607020	Iteration 38400 	loss 2.494908332824707
2022-12-12 20:06:07.807170	Iteration 38500 	loss 2.3910021781921387
2022-12-12 20:06:10.123039	Iteration 38600 	loss 2.359802722930908
2022-12-12 20:06:12.316458	Iteration 38700 	loss 2.440511465072632
2022-12-12 20:06:14.507343	Iteration 38800 	loss 1.7689406871795654
2022-12-12 20:06:16.703265	Iteration 38900 	loss 2.1191325187683105
2022-12-12 20:06:18.897399	Iteration 39000 	loss 2.1529173851013184
2022-12-12 20:06:21.223290	Iteration 39100 	loss 1.7602633237838745
2022-12-12 20:06:23.379189	Iteration 39200 	loss 1.6977756023406982
2022-12-12 20:06:25.576791	Iteration 39300 	loss 2.2090463638305664
2022-12-12 20:06:27.779647	Iteration 39400 	loss 2.308837652206421
2022-12-12 20:06:29.975776	Iteration 39500 	loss 2.657944679260254
2022-12-12 20:06:32.303078	Iteration 39600 	loss 2.226201057434082
2022-12-12 20:06:34.463405	Iteration 39700 	loss 2.498218536376953
2022-12-12 20:06:36.664475	Iteration 39800 	loss 2.562100887298584
2022-12-12 20:06:38.855533	Iteration 39900 	loss 2.5311713218688965
2022-12-12 20:06:41.045346	Iteration 40000 	loss 2.337238073348999
2022-12-12 20:06:43.369949	Iteration 40100 	loss 2.0347275733947754
2022-12-12 20:06:45.536066	Iteration 40200 	loss 2.2876760959625244
2022-12-12 20:06:47.723476	Iteration 40300 	loss 2.312767744064331
2022-12-12 20:06:49.921762	Iteration 40400 	loss 2.390064001083374
2022-12-12 20:06:52.115003	Iteration 40500 	loss 2.320138692855835
2022-12-12 20:06:54.453404	Iteration 40600 	loss 1.9769991636276245
2022-12-12 20:06:56.633277	Iteration 40700 	loss 2.067486524581909
2022-12-12 20:06:58.814677	Iteration 40800 	loss 2.5296213626861572
2022-12-12 20:07:01.039161	Iteration 40900 	loss 2.683609962463379
2022-12-12 20:07:03.235235	Iteration 41000 	loss 2.121891498565674
2022-12-12 20:07:05.550208	Iteration 41100 	loss 1.8581746816635132
2022-12-12 20:07:07.737273	Iteration 41200 	loss 1.6162625551223755
2022-12-12 20:07:09.937208	Iteration 41300 	loss 2.224843978881836
2022-12-12 20:07:12.130568	Iteration 41400 	loss 2.1604907512664795
2022-12-12 20:07:14.336058	Iteration 41500 	loss 2.587829828262329
2022-12-12 20:07:16.635697	Iteration 41600 	loss 2.22847843170166
2022-12-12 20:07:18.824579	Iteration 41700 	loss 2.0025928020477295
2022-12-12 20:07:21.023933	Iteration 41800 	loss 2.0542004108428955
2022-12-12 20:07:23.234695	Iteration 41900 	loss 2.0702340602874756
2022-12-12 20:07:25.436128	Iteration 42000 	loss 2.373633861541748
2022-12-12 20:07:28.274080	Iteration 42100 	loss 2.5723087787628174
2022-12-12 20:07:30.425855	Iteration 42200 	loss 2.0326011180877686
2022-12-12 20:07:32.619816	Iteration 42300 	loss 2.6734724044799805
2022-12-12 20:07:34.817446	Iteration 42400 	loss 2.0106046199798584
2022-12-12 20:07:37.062187	Iteration 42500 	loss 2.300546169281006
2022-12-12 20:07:39.367022	Iteration 42600 	loss 2.34075927734375
2022-12-12 20:07:41.559190	Iteration 42700 	loss 1.7676893472671509
2022-12-12 20:07:43.755406	Iteration 42800 	loss 2.4379327297210693
2022-12-12 20:07:45.933268	Iteration 42900 	loss 2.2892568111419678
2022-12-12 20:07:48.121074	Iteration 43000 	loss 2.1943604946136475
2022-12-12 20:07:50.412572	Iteration 43100 	loss 2.1751925945281982
2022-12-12 20:07:52.592947	Iteration 43200 	loss 1.862269401550293
2022-12-12 20:07:54.790561	Iteration 43300 	loss 2.536329507827759
2022-12-12 20:07:56.989185	Iteration 43400 	loss 2.5213310718536377
2022-12-12 20:07:59.209816	Iteration 43500 	loss 2.465590000152588
2022-12-12 20:08:01.519051	Iteration 43600 	loss 2.6299073696136475
2022-12-12 20:08:03.723446	Iteration 43700 	loss 1.7779850959777832
2022-12-12 20:08:05.935457	Iteration 43800 	loss 2.2914352416992188
2022-12-12 20:08:08.119483	Iteration 43900 	loss 2.6015625
2022-12-12 20:08:10.306980	Iteration 44000 	loss 2.3638153076171875
2022-12-12 20:08:12.588848	Iteration 44100 	loss 2.520550012588501
2022-12-12 20:08:14.779231	Iteration 44200 	loss 1.7447261810302734
2022-12-12 20:08:16.974362	Iteration 44300 	loss 2.483754873275757
2022-12-12 20:08:19.203037	Iteration 44400 	loss 1.997265338897705
2022-12-12 20:08:21.362962	Iteration 44500 	loss 1.5012879371643066
2022-12-12 20:08:23.687943	Iteration 44600 	loss 2.2299563884735107
2022-12-12 20:08:25.885955	Iteration 44700 	loss 2.310047149658203
2022-12-12 20:08:28.065835	Iteration 44800 	loss 2.1625967025756836
2022-12-12 20:08:30.231093	Iteration 44900 	loss 2.102780342102051
2022-12-12 20:08:32.413742	Iteration 45000 	loss 2.295207977294922
2022-12-12 20:08:34.740728	Iteration 45100 	loss 2.5066769123077393
2022-12-12 20:08:36.926257	Iteration 45200 	loss 2.0684633255004883
2022-12-12 20:08:39.120885	Iteration 45300 	loss 2.445939064025879
2022-12-12 20:08:41.299100	Iteration 45400 	loss 2.2474961280822754
2022-12-12 20:08:43.456579	Iteration 45500 	loss 2.0324862003326416
2022-12-12 20:08:45.771797	Iteration 45600 	loss 2.280616521835327
2022-12-12 20:08:47.958111	Iteration 45700 	loss 2.1548514366149902
2022-12-12 20:08:50.147661	Iteration 45800 	loss 2.426926612854004
2022-12-12 20:08:52.315874	Iteration 45900 	loss 2.1728720664978027
2022-12-12 20:08:54.473780	Iteration 46000 	loss 2.3369357585906982
2022-12-12 20:08:56.790655	Iteration 46100 	loss 2.611816167831421
2022-12-12 20:08:58.978187	Iteration 46200 	loss 2.3972790241241455
2022-12-12 20:09:01.185678	Iteration 46300 	loss 2.0840704441070557
2022-12-12 20:09:03.354751	Iteration 46400 	loss 2.590463876724243
2022-12-12 20:09:05.532031	Iteration 46500 	loss 2.6671769618988037
2022-12-12 20:09:07.839106	Iteration 46600 	loss 2.0206689834594727
2022-12-12 20:09:10.014402	Iteration 46700 	loss 2.469351291656494
2022-12-12 20:09:12.167037	Iteration 46800 	loss 2.006406307220459
2022-12-12 20:09:14.326935	Iteration 46900 	loss 2.089890480041504
2022-12-12 20:09:16.504805	Iteration 47000 	loss 2.496454954147339
2022-12-12 20:09:18.814171	Iteration 47100 	loss 2.2964508533477783
2022-12-12 20:09:20.995365	Iteration 47200 	loss 2.49652361869812
2022-12-12 20:09:23.147662	Iteration 47300 	loss 2.2927825450897217
2022-12-12 20:09:25.335585	Iteration 47400 	loss 2.479952096939087
2022-12-12 20:09:27.545268	Iteration 47500 	loss 2.496290445327759
2022-12-12 20:09:29.872348	Iteration 47600 	loss 2.1311004161834717
2022-12-12 20:09:32.060864	Iteration 47700 	loss 1.8965952396392822
2022-12-12 20:09:34.200713	Iteration 47800 	loss 2.491297721862793
2022-12-12 20:09:36.389062	Iteration 47900 	loss 2.2396514415740967
2022-12-12 20:09:38.578830	Iteration 48000 	loss 2.284907817840576
2022-12-12 20:09:40.894706	Iteration 48100 	loss 2.1034204959869385
2022-12-12 20:09:43.074101	Iteration 48200 	loss 2.049630641937256
2022-12-12 20:09:45.259364	Iteration 48300 	loss 2.2892589569091797
2022-12-12 20:09:47.443680	Iteration 48400 	loss 1.8929673433303833
2022-12-12 20:09:49.621819	Iteration 48500 	loss 2.4303531646728516
2022-12-12 20:09:51.901836	Iteration 48600 	loss 2.078521251678467
2022-12-12 20:09:54.033992	Iteration 48700 	loss 1.5324739217758179
2022-12-12 20:09:56.207385	Iteration 48800 	loss 2.7536678314208984
2022-12-12 20:09:58.373351	Iteration 48900 	loss 2.3250069618225098
2022-12-12 20:10:00.546586	Iteration 49000 	loss 2.432889699935913
2022-12-12 20:10:02.823674	Iteration 49100 	loss 2.298691987991333
2022-12-12 20:10:04.985534	Iteration 49200 	loss 2.4924099445343018
2022-12-12 20:10:07.156776	Iteration 49300 	loss 2.1839489936828613
2022-12-12 20:10:09.338346	Iteration 49400 	loss 2.150599956512451
2022-12-12 20:10:11.509568	Iteration 49500 	loss 2.1663506031036377
2022-12-12 20:10:13.795263	Iteration 49600 	loss 2.0303773880004883
2022-12-12 20:10:15.959246	Iteration 49700 	loss 2.0485925674438477
2022-12-12 20:10:18.142183	Iteration 49800 	loss 2.199613332748413
2022-12-12 20:10:20.319420	Iteration 49900 	loss 2.1204261779785156
2022-12-12 20:10:22.505166	Iteration 50000 	loss 2.4579944610595703
2022-12-12 20:10:24.778276	Iteration 50100 	loss 2.308316946029663
2022-12-12 20:10:26.985950	Iteration 50200 	loss 1.9348647594451904
2022-12-12 20:10:29.172037	Iteration 50300 	loss 2.1651670932769775
2022-12-12 20:10:31.353752	Iteration 50400 	loss 2.115945339202881
2022-12-12 20:10:33.497074	Iteration 50500 	loss 2.211604595184326
2022-12-12 20:10:35.793143	Iteration 50600 	loss 2.272508382797241
2022-12-12 20:10:37.970257	Iteration 50700 	loss 2.8213307857513428
2022-12-12 20:10:40.143209	Iteration 50800 	loss 2.5849921703338623
2022-12-12 20:10:42.314039	Iteration 50900 	loss 2.309502601623535
2022-12-12 20:10:44.454470	Iteration 51000 	loss 2.504426956176758
2022-12-12 20:10:46.754167	Iteration 51100 	loss 2.063121795654297
2022-12-12 20:10:48.916200	Iteration 51200 	loss 2.0519638061523438
2022-12-12 20:10:51.089647	Iteration 51300 	loss 2.173222303390503
2022-12-12 20:10:53.241220	Iteration 51400 	loss 2.108001708984375
2022-12-12 20:10:55.397532	Iteration 51500 	loss 2.291921615600586
2022-12-12 20:10:57.689325	Iteration 51600 	loss 2.649397611618042
2022-12-12 20:10:59.852914	Iteration 51700 	loss 2.646242618560791
2022-12-12 20:11:02.028491	Iteration 51800 	loss 2.160745143890381
2022-12-12 20:11:04.170225	Iteration 51900 	loss 2.2538750171661377
2022-12-12 20:11:06.333687	Iteration 52000 	loss 2.909926176071167
2022-12-12 20:11:08.665802	Iteration 52100 	loss 2.341137647628784
2022-12-12 20:11:10.843070	Iteration 52200 	loss 2.5403523445129395
2022-12-12 20:11:12.970161	Iteration 52300 	loss 2.162709951400757
2022-12-12 20:11:15.135343	Iteration 52400 	loss 2.1944851875305176
2022-12-12 20:11:17.301032	Iteration 52500 	loss 1.7854522466659546
2022-12-12 20:11:19.600055	Iteration 52600 	loss 2.4621541500091553
2022-12-12 20:11:21.764190	Iteration 52700 	loss 2.196916103363037
2022-12-12 20:11:23.893106	Iteration 52800 	loss 2.43974232673645
2022-12-12 20:11:26.058761	Iteration 52900 	loss 2.1549160480499268
2022-12-12 20:11:28.232946	Iteration 53000 	loss 2.3347017765045166
2022-12-12 20:11:30.522623	Iteration 53100 	loss 2.2516846656799316
2022-12-12 20:11:32.690906	Iteration 53200 	loss 2.4602866172790527
2022-12-12 20:11:34.820077	Iteration 53300 	loss 2.574260711669922
2022-12-12 20:11:36.990169	Iteration 53400 	loss 2.974813222885132
2022-12-12 20:11:39.161247	Iteration 53500 	loss 2.0716052055358887
2022-12-12 20:11:41.453095	Iteration 53600 	loss 2.4573066234588623
2022-12-12 20:11:43.619527	Iteration 53700 	loss 2.5711348056793213
2022-12-12 20:11:45.752872	Iteration 53800 	loss 2.3236117362976074
2022-12-12 20:11:47.925261	Iteration 53900 	loss 2.123818874359131
2022-12-12 20:11:50.143224	Iteration 54000 	loss 2.1679580211639404
2022-12-12 20:11:52.429871	Iteration 54100 	loss 2.2168593406677246
2022-12-12 20:11:54.577269	Iteration 54200 	loss 1.573982834815979
2022-12-12 20:11:56.753197	Iteration 54300 	loss 2.411649465560913
2022-12-12 20:11:58.920623	Iteration 54400 	loss 1.895024061203003
2022-12-12 20:12:01.097423	Iteration 54500 	loss 2.5670409202575684
2022-12-12 20:12:04.078677	Iteration 54600 	loss 2.168207883834839
2022-12-12 20:12:06.209923	Iteration 54700 	loss 2.209296464920044
2022-12-12 20:12:08.370238	Iteration 54800 	loss 2.5629990100860596
2022-12-12 20:12:10.542421	Iteration 54900 	loss 2.149096965789795
2022-12-12 20:12:12.715034	Iteration 55000 	loss 2.1992759704589844
2022-12-12 20:12:15.008426	Iteration 55100 	loss 2.227586030960083
2022-12-12 20:12:17.150925	Iteration 55200 	loss 2.2418341636657715
2022-12-12 20:12:19.310581	Iteration 55300 	loss 2.468517303466797
2022-12-12 20:12:21.484986	Iteration 55400 	loss 2.0081160068511963
2022-12-12 20:12:23.652332	Iteration 55500 	loss 2.2761266231536865
2022-12-12 20:12:25.956891	Iteration 55600 	loss 2.117497444152832
2022-12-12 20:12:28.086329	Iteration 55700 	loss 2.1395764350891113
2022-12-12 20:12:30.259684	Iteration 55800 	loss 2.129276752471924
2022-12-12 20:12:32.472690	Iteration 55900 	loss 1.971268653869629
2022-12-12 20:12:34.638933	Iteration 56000 	loss 2.5313193798065186
2022-12-12 20:12:36.905589	Iteration 56100 	loss 1.6478137969970703
2022-12-12 20:12:39.079518	Iteration 56200 	loss 2.052058458328247
2022-12-12 20:12:41.244420	Iteration 56300 	loss 2.102038621902466
2022-12-12 20:12:43.409352	Iteration 56400 	loss 2.402456283569336
2022-12-12 20:12:45.556650	Iteration 56500 	loss 2.098952054977417
2022-12-12 20:12:47.832755	Iteration 56600 	loss 2.4749984741210938
2022-12-12 20:12:50.005920	Iteration 56700 	loss 2.035590887069702
2022-12-12 20:12:52.168586	Iteration 56800 	loss 2.064838171005249
2022-12-12 20:12:54.342139	Iteration 56900 	loss 2.539024591445923
2022-12-12 20:12:56.477141	Iteration 57000 	loss 2.2461776733398438
2022-12-12 20:12:58.777217	Iteration 57100 	loss 2.3437962532043457
2022-12-12 20:13:00.950999	Iteration 57200 	loss 1.6717931032180786
2022-12-12 20:13:03.134013	Iteration 57300 	loss 2.225411891937256
2022-12-12 20:13:05.305362	Iteration 57400 	loss 2.0244925022125244
2022-12-12 20:13:07.442590	Iteration 57500 	loss 1.9317797422409058
2022-12-12 20:13:09.738226	Iteration 57600 	loss 2.1711642742156982
2022-12-12 20:13:11.904837	Iteration 57700 	loss 2.285275936126709
2022-12-12 20:13:14.110588	Iteration 57800 	loss 2.3205313682556152
2022-12-12 20:13:16.244078	Iteration 57900 	loss 2.3681817054748535
2022-12-12 20:13:18.409583	Iteration 58000 	loss 1.6502171754837036
2022-12-12 20:13:20.705887	Iteration 58100 	loss 1.974636197090149
2022-12-12 20:13:22.879272	Iteration 58200 	loss 1.8152822256088257
2022-12-12 20:13:25.029300	Iteration 58300 	loss 1.952995777130127
2022-12-12 20:13:27.177397	Iteration 58400 	loss 2.1813056468963623
2022-12-12 20:13:29.340403	Iteration 58500 	loss 1.934807538986206
2022-12-12 20:13:31.645093	Iteration 58600 	loss 2.134542226791382
2022-12-12 20:13:33.814853	Iteration 58700 	loss 1.676947832107544
2022-12-12 20:13:35.961605	Iteration 58800 	loss 2.440077066421509
2022-12-12 20:13:38.108712	Iteration 58900 	loss 2.2257611751556396
2022-12-12 20:13:40.279082	Iteration 59000 	loss 2.15604305267334
2022-12-12 20:13:42.575981	Iteration 59100 	loss 2.6716551780700684
2022-12-12 20:13:44.754085	Iteration 59200 	loss 2.1687657833099365
2022-12-12 20:13:46.887650	Iteration 59300 	loss 2.559769868850708
2022-12-12 20:13:49.053289	Iteration 59400 	loss 2.245426654815674
2022-12-12 20:13:51.222746	Iteration 59500 	loss 1.9763983488082886
2022-12-12 20:13:53.523991	Iteration 59600 	loss 2.386101722717285
2022-12-12 20:13:55.704879	Iteration 59700 	loss 2.10916805267334
2022-12-12 20:13:57.869297	Iteration 59800 	loss 2.1228857040405273
2022-12-12 20:14:00.035733	Iteration 59900 	loss 2.4342408180236816
2022-12-12 20:14:02.204910	Iteration 60000 	loss 1.966820240020752
2022-12-12 20:14:04.508636	Iteration 60100 	loss 2.1554932594299316
2022-12-12 20:14:06.648865	Iteration 60200 	loss 1.8135299682617188
2022-12-12 20:14:08.830250	Iteration 60300 	loss 1.9749785661697388
2022-12-12 20:14:10.999164	Iteration 60400 	loss 2.7933597564697266
2022-12-12 20:14:13.182342	Iteration 60500 	loss 2.3436427116394043
2022-12-12 20:14:15.476235	Iteration 60600 	loss 2.226036548614502
2022-12-12 20:14:17.630805	Iteration 60700 	loss 2.540161371231079
2022-12-12 20:14:19.807016	Iteration 60800 	loss 2.145083427429199
2022-12-12 20:14:21.993148	Iteration 60900 	loss 2.4644620418548584
2022-12-12 20:14:24.174455	Iteration 61000 	loss 2.0650014877319336
2022-12-12 20:14:26.469283	Iteration 61100 	loss 2.152592182159424
2022-12-12 20:14:28.631171	Iteration 61200 	loss 2.0014452934265137
2022-12-12 20:14:30.822861	Iteration 61300 	loss 2.1159167289733887
2022-12-12 20:14:33.033889	Iteration 61400 	loss 2.5965189933776855
2022-12-12 20:14:35.211856	Iteration 61500 	loss 1.7715446949005127
2022-12-12 20:14:37.521732	Iteration 61600 	loss 2.1768712997436523
2022-12-12 20:14:39.733677	Iteration 61700 	loss 2.18057918548584
2022-12-12 20:14:41.941537	Iteration 61800 	loss 1.942792534828186
2022-12-12 20:14:44.139709	Iteration 61900 	loss 1.9682666063308716
2022-12-12 20:14:46.291899	Iteration 62000 	loss 2.370640516281128
2022-12-12 20:14:48.604643	Iteration 62100 	loss 2.6114211082458496
2022-12-12 20:14:50.807908	Iteration 62200 	loss 2.0899672508239746
2022-12-12 20:14:52.996963	Iteration 62300 	loss 2.092226982116699
2022-12-12 20:14:55.198619	Iteration 62400 	loss 2.2660698890686035
2022-12-12 20:14:57.328791	Iteration 62500 	loss 2.121056079864502
2022-12-12 20:14:59.664164	Iteration 62600 	loss 2.4324629306793213
2022-12-12 20:15:01.849751	Iteration 62700 	loss 2.4796369075775146
2022-12-12 20:15:04.043557	Iteration 62800 	loss 2.51230788230896
2022-12-12 20:15:06.235821	Iteration 62900 	loss 2.4903717041015625
2022-12-12 20:15:08.368601	Iteration 63000 	loss 2.2201004028320312
2022-12-12 20:15:10.708980	Iteration 63100 	loss 2.132612705230713
2022-12-12 20:15:12.898321	Iteration 63200 	loss 2.290098190307617
2022-12-12 20:15:15.092639	Iteration 63300 	loss 1.961491346359253
2022-12-12 20:15:17.280057	Iteration 63400 	loss 2.354440689086914
2022-12-12 20:15:19.433106	Iteration 63500 	loss 2.422349691390991
2022-12-12 20:15:21.811212	Iteration 63600 	loss 2.1022074222564697
2022-12-12 20:15:24.008078	Iteration 63700 	loss 2.0829362869262695
2022-12-12 20:15:26.194944	Iteration 63800 	loss 2.112504482269287
2022-12-12 20:15:28.342339	Iteration 63900 	loss 2.1472907066345215
2022-12-12 20:15:30.535829	Iteration 64000 	loss 1.9942251443862915
2022-12-12 20:15:32.874154	Iteration 64100 	loss 1.9287126064300537
2022-12-12 20:15:35.074233	Iteration 64200 	loss 2.9667208194732666
2022-12-12 20:15:37.268812	Iteration 64300 	loss 2.3669803142547607
2022-12-12 20:15:39.436325	Iteration 64400 	loss 2.5004546642303467
2022-12-12 20:15:41.623821	Iteration 64500 	loss 2.4362998008728027
2022-12-12 20:15:43.962671	Iteration 64600 	loss 2.498293876647949
2022-12-12 20:15:46.164148	Iteration 64700 	loss 2.2965097427368164
2022-12-12 20:15:48.380876	Iteration 64800 	loss 1.9406968355178833
2022-12-12 20:15:50.537827	Iteration 64900 	loss 2.380110263824463
2022-12-12 20:15:52.742445	Iteration 65000 	loss 2.2966816425323486
2022-12-12 20:15:55.070939	Iteration 65100 	loss 2.0592756271362305
2022-12-12 20:15:57.276625	Iteration 65200 	loss 2.299455165863037
2022-12-12 20:15:59.477928	Iteration 65300 	loss 1.8882180452346802
2022-12-12 20:16:01.646016	Iteration 65400 	loss 2.3264894485473633
2022-12-12 20:16:03.878439	Iteration 65500 	loss 2.230531692504883
2022-12-12 20:16:06.205274	Iteration 65600 	loss 2.263744831085205
2022-12-12 20:16:08.396229	Iteration 65700 	loss 1.940953254699707
2022-12-12 20:16:10.548869	Iteration 65800 	loss 2.078577756881714
2022-12-12 20:16:12.734937	Iteration 65900 	loss 1.946487307548523
2022-12-12 20:16:14.929542	Iteration 66000 	loss 2.0156009197235107
2022-12-12 20:16:17.246657	Iteration 66100 	loss 2.2622601985931396
2022-12-12 20:16:19.431691	Iteration 66200 	loss 2.4799447059631348
2022-12-12 20:16:21.596790	Iteration 66300 	loss 2.6333234310150146
2022-12-12 20:16:23.770080	Iteration 66400 	loss 1.9598439931869507
2022-12-12 20:16:25.958553	Iteration 66500 	loss 2.225517988204956
2022-12-12 20:16:28.276954	Iteration 66600 	loss 2.514307975769043
2022-12-12 20:16:30.469716	Iteration 66700 	loss 2.752984046936035
2022-12-12 20:16:32.633305	Iteration 66800 	loss 2.2858948707580566
2022-12-12 20:16:34.810354	Iteration 66900 	loss 1.74526846408844
2022-12-12 20:16:37.009208	Iteration 67000 	loss 2.128187417984009
2022-12-12 20:16:39.342589	Iteration 67100 	loss 2.0145957469940186
2022-12-12 20:16:41.544101	Iteration 67200 	loss 2.145667314529419
2022-12-12 20:16:43.717128	Iteration 67300 	loss 2.0384998321533203
2022-12-12 20:16:45.927701	Iteration 67400 	loss 2.0829081535339355
2022-12-12 20:16:48.118062	Iteration 67500 	loss 2.09147310256958
2022-12-12 20:16:50.435592	Iteration 67600 	loss 1.9311591386795044
2022-12-12 20:16:52.600788	Iteration 67700 	loss 2.4214093685150146
2022-12-12 20:16:54.783641	Iteration 67800 	loss 1.7973687648773193
2022-12-12 20:16:56.974524	Iteration 67900 	loss 2.1955113410949707
2022-12-12 20:16:59.167117	Iteration 68000 	loss 2.610586404800415
2022-12-12 20:17:01.486146	Iteration 68100 	loss 2.5577311515808105
2022-12-12 20:17:03.658278	Iteration 68200 	loss 2.2469897270202637
2022-12-12 20:17:05.822398	Iteration 68300 	loss 2.1558470726013184
2022-12-12 20:17:08.017668	Iteration 68400 	loss 2.6128149032592773
2022-12-12 20:17:10.219530	Iteration 68500 	loss 1.942535400390625
2022-12-12 20:17:12.593028	Iteration 68600 	loss 2.3811235427856445
2022-12-12 20:17:14.766756	Iteration 68700 	loss 1.8959271907806396
2022-12-12 20:17:16.943335	Iteration 68800 	loss 1.905566930770874
2022-12-12 20:17:19.139163	Iteration 68900 	loss 1.791440725326538
2022-12-12 20:17:21.328924	Iteration 69000 	loss 2.1128387451171875
2022-12-12 20:17:23.671739	Iteration 69100 	loss 2.467865228652954
2022-12-12 20:17:25.844312	Iteration 69200 	loss 2.064908504486084
2022-12-12 20:17:28.064452	Iteration 69300 	loss 2.1848549842834473
2022-12-12 20:17:30.252346	Iteration 69400 	loss 2.494654417037964
2022-12-12 20:17:32.448096	Iteration 69500 	loss 1.8382835388183594
2022-12-12 20:17:35.635870	Iteration 69600 	loss 2.037396192550659
2022-12-12 20:17:37.849074	Iteration 69700 	loss 2.0868985652923584
2022-12-12 20:17:40.010320	Iteration 69800 	loss 2.3451192378997803
2022-12-12 20:17:42.220526	Iteration 69900 	loss 2.133552312850952
2022-12-12 20:17:44.424578	Iteration 70000 	loss 2.047607183456421
2022-12-12 20:17:46.756905	Iteration 70100 	loss 2.2754387855529785
2022-12-12 20:17:49.083373	Iteration 70200 	loss 1.9848384857177734
2022-12-12 20:17:51.338725	Iteration 70300 	loss 2.0263524055480957
2022-12-12 20:17:53.506800	Iteration 70400 	loss 2.411430835723877
2022-12-12 20:17:55.709471	Iteration 70500 	loss 2.1758079528808594
2022-12-12 20:17:58.040398	Iteration 70600 	loss 2.214276075363159
2022-12-12 20:18:00.436645	Iteration 70700 	loss 1.8951358795166016
2022-12-12 20:18:02.631392	Iteration 70800 	loss 1.7692478895187378
2022-12-12 20:18:04.812719	Iteration 70900 	loss 2.6011481285095215
2022-12-12 20:18:07.029829	Iteration 71000 	loss 2.6245079040527344
2022-12-12 20:18:09.405129	Iteration 71100 	loss 1.9095702171325684
2022-12-12 20:18:11.592312	Iteration 71200 	loss 2.6716744899749756
2022-12-12 20:18:13.751829	Iteration 71300 	loss 2.0873305797576904
2022-12-12 20:18:15.943776	Iteration 71400 	loss 2.3648691177368164
2022-12-12 20:18:18.136382	Iteration 71500 	loss 2.0123867988586426
2022-12-12 20:18:20.468543	Iteration 71600 	loss 2.3490660190582275
2022-12-12 20:18:22.655623	Iteration 71700 	loss 2.372931957244873
2022-12-12 20:18:24.830912	Iteration 71800 	loss 2.244981050491333
2022-12-12 20:18:27.012166	Iteration 71900 	loss 1.6108710765838623
2022-12-12 20:18:29.227488	Iteration 72000 	loss 1.9365077018737793
2022-12-12 20:18:31.549370	Iteration 72100 	loss 2.398347854614258
2022-12-12 20:18:33.726664	Iteration 72200 	loss 2.3291664123535156
2022-12-12 20:18:35.942392	Iteration 72300 	loss 2.095562696456909
2022-12-12 20:18:38.135507	Iteration 72400 	loss 2.2748734951019287
2022-12-12 20:18:40.323982	Iteration 72500 	loss 2.1414527893066406
2022-12-12 20:18:42.631813	Iteration 72600 	loss 1.9611380100250244
2022-12-12 20:18:44.802445	Iteration 72700 	loss 2.025716543197632
2022-12-12 20:18:46.942526	Iteration 72800 	loss 2.074350118637085
2022-12-12 20:18:49.125413	Iteration 72900 	loss 2.76265811920166
2022-12-12 20:18:51.334674	Iteration 73000 	loss 1.5378968715667725
2022-12-12 20:18:53.639125	Iteration 73100 	loss 2.220473527908325
2022-12-12 20:18:55.785212	Iteration 73200 	loss 2.1244940757751465
2022-12-12 20:18:57.967367	Iteration 73300 	loss 2.4842965602874756
2022-12-12 20:19:00.145067	Iteration 73400 	loss 2.012981653213501
2022-12-12 20:19:02.326291	Iteration 73500 	loss 2.6295456886291504
2022-12-12 20:19:04.619163	Iteration 73600 	loss 2.211688280105591
2022-12-12 20:19:06.766260	Iteration 73700 	loss 2.189908027648926
2022-12-12 20:19:08.948779	Iteration 73800 	loss 2.6556687355041504
2022-12-12 20:19:11.131212	Iteration 73900 	loss 2.432703971862793
2022-12-12 20:19:13.309082	Iteration 74000 	loss 2.30844783782959
2022-12-12 20:19:15.604236	Iteration 74100 	loss 1.628478765487671
2022-12-12 20:19:17.748570	Iteration 74200 	loss 2.0313003063201904
2022-12-12 20:19:19.928199	Iteration 74300 	loss 2.5525286197662354
2022-12-12 20:19:22.102931	Iteration 74400 	loss 2.1319732666015625
2022-12-12 20:19:24.277492	Iteration 74500 	loss 2.0298120975494385
2022-12-12 20:19:26.582954	Iteration 74600 	loss 2.2838213443756104
2022-12-12 20:19:28.732976	Iteration 74700 	loss 2.0310750007629395
2022-12-12 20:19:30.922208	Iteration 74800 	loss 2.2375574111938477
2022-12-12 20:19:33.139404	Iteration 74900 	loss 2.351734161376953
2022-12-12 20:19:35.300547	Iteration 75000 	loss 1.931607723236084
2022-12-12 20:19:37.593951	Iteration 75100 	loss 2.3999571800231934
2022-12-12 20:19:39.768490	Iteration 75200 	loss 2.477182149887085
2022-12-12 20:19:41.949505	Iteration 75300 	loss 2.2155866622924805
2022-12-12 20:19:44.110157	Iteration 75400 	loss 2.2974531650543213
2022-12-12 20:19:46.248203	Iteration 75500 	loss 2.2307724952697754
2022-12-12 20:19:48.528907	Iteration 75600 	loss 1.707833170890808
2022-12-12 20:19:50.701307	Iteration 75700 	loss 1.792628526687622
2022-12-12 20:19:52.875870	Iteration 75800 	loss 2.42250394821167
2022-12-12 20:19:55.063518	Iteration 75900 	loss 2.056519031524658
2022-12-12 20:19:57.194675	Iteration 76000 	loss 2.2183406352996826
2022-12-12 20:19:59.490459	Iteration 76100 	loss 2.598567008972168
2022-12-12 20:20:01.638676	Iteration 76200 	loss 2.241968870162964
2022-12-12 20:20:03.797221	Iteration 76300 	loss 1.8638056516647339
2022-12-12 20:20:05.935911	Iteration 76400 	loss 2.3895153999328613
2022-12-12 20:20:08.054887	Iteration 76500 	loss 2.5403499603271484
2022-12-12 20:20:10.328042	Iteration 76600 	loss 1.833817958831787
2022-12-12 20:20:12.477184	Iteration 76700 	loss 2.136025905609131
2022-12-12 20:20:14.652725	Iteration 76800 	loss 1.861982822418213
2022-12-12 20:20:16.777843	Iteration 76900 	loss 2.5018973350524902
2022-12-12 20:20:18.929476	Iteration 77000 	loss 1.8638865947723389
2022-12-12 20:20:21.194520	Iteration 77100 	loss 2.364468574523926
2022-12-12 20:20:23.346132	Iteration 77200 	loss 2.1312825679779053
2022-12-12 20:20:25.449395	Iteration 77300 	loss 2.3433632850646973
2022-12-12 20:20:27.592500	Iteration 77400 	loss 2.401793956756592
2022-12-12 20:20:29.725518	Iteration 77500 	loss 2.225921154022217
2022-12-12 20:20:32.000798	Iteration 77600 	loss 2.530228614807129
2022-12-12 20:20:34.128634	Iteration 77700 	loss 2.23740553855896
2022-12-12 20:20:36.246316	Iteration 77800 	loss 2.2630910873413086
2022-12-12 20:20:38.391255	Iteration 77900 	loss 2.3230230808258057
2022-12-12 20:20:40.539601	Iteration 78000 	loss 2.3526570796966553
2022-12-12 20:20:42.808557	Iteration 78100 	loss 2.5269763469696045
2022-12-12 20:20:44.923974	Iteration 78200 	loss 2.4628682136535645
2022-12-12 20:20:47.045080	Iteration 78300 	loss 2.060910224914551
2022-12-12 20:20:49.218959	Iteration 78400 	loss 2.56289005279541
2022-12-12 20:20:51.493629	Iteration 78500 	loss 2.6320478916168213
2022-12-12 20:20:53.759590	Iteration 78600 	loss 1.4068830013275146
2022-12-12 20:20:55.862866	Iteration 78700 	loss 2.630002498626709
2022-12-12 20:20:58.032110	Iteration 78800 	loss 1.8172624111175537
2022-12-12 20:21:00.276315	Iteration 78900 	loss 2.617072820663452
2022-12-12 20:21:02.442795	Iteration 79000 	loss 2.3264849185943604
2022-12-12 20:21:04.771540	Iteration 79100 	loss 2.0884270668029785
2022-12-12 20:21:06.912756	Iteration 79200 	loss 2.233025312423706
2022-12-12 20:21:09.068718	Iteration 79300 	loss 2.2950668334960938
2022-12-12 20:21:11.232909	Iteration 79400 	loss 2.341702699661255
2022-12-12 20:21:13.381389	Iteration 79500 	loss 2.4916043281555176
2022-12-12 20:21:15.671512	Iteration 79600 	loss 2.1579935550689697
2022-12-12 20:21:17.804358	Iteration 79700 	loss 2.431471109390259
2022-12-12 20:21:19.976989	Iteration 79800 	loss 2.434839963912964
2022-12-12 20:21:22.171611	Iteration 79900 	loss 1.9793801307678223
2022-12-12 20:21:24.333615	Iteration 80000 	loss 1.681250810623169
2022-12-12 20:21:26.635480	Iteration 80100 	loss 2.266195774078369
2022-12-12 20:21:28.771359	Iteration 80200 	loss 2.407545804977417
2022-12-12 20:21:30.943151	Iteration 80300 	loss 1.827476978302002
2022-12-12 20:21:33.116428	Iteration 80400 	loss 2.189229726791382
2022-12-12 20:21:35.238059	Iteration 80500 	loss 2.486543655395508
2022-12-12 20:21:37.505024	Iteration 80600 	loss 2.166808605194092
2022-12-12 20:21:39.664590	Iteration 80700 	loss 2.419247627258301
2022-12-12 20:21:41.831694	Iteration 80800 	loss 1.8685178756713867
2022-12-12 20:21:43.955728	Iteration 80900 	loss 2.3973004817962646
2022-12-12 20:21:46.087527	Iteration 81000 	loss 2.4260401725769043
2022-12-12 20:21:48.351468	Iteration 81100 	loss 1.7722477912902832
2022-12-12 20:21:50.494510	Iteration 81200 	loss 2.289156913757324
2022-12-12 20:21:52.632400	Iteration 81300 	loss 2.465542793273926
2022-12-12 20:21:54.760276	Iteration 81400 	loss 2.3727269172668457
2022-12-12 20:21:56.894389	Iteration 81500 	loss 2.2351481914520264
2022-12-12 20:21:59.157341	Iteration 81600 	loss 2.442697048187256
2022-12-12 20:22:01.296479	Iteration 81700 	loss 1.6837751865386963
2022-12-12 20:22:03.439783	Iteration 81800 	loss 2.0167007446289062
2022-12-12 20:22:05.569505	Iteration 81900 	loss 1.8702340126037598
2022-12-12 20:22:07.711201	Iteration 82000 	loss 2.2116098403930664
2022-12-12 20:22:09.982445	Iteration 82100 	loss 2.453432083129883
2022-12-12 20:22:12.113366	Iteration 82200 	loss 2.1088500022888184
2022-12-12 20:22:14.268161	Iteration 82300 	loss 1.747429609298706
2022-12-12 20:22:16.390572	Iteration 82400 	loss 2.31380033493042
2022-12-12 20:22:18.542765	Iteration 82500 	loss 2.4168546199798584
2022-12-12 20:22:20.824564	Iteration 82600 	loss 2.3103418350219727
2022-12-12 20:22:22.970267	Iteration 82700 	loss 2.1922988891601562
2022-12-12 20:22:25.099843	Iteration 82800 	loss 2.6791486740112305
2022-12-12 20:22:27.243993	Iteration 82900 	loss 2.3114380836486816
2022-12-12 20:22:29.388769	Iteration 83000 	loss 2.630859613418579
2022-12-12 20:22:31.659124	Iteration 83100 	loss 1.8654274940490723
2022-12-12 20:22:33.803463	Iteration 83200 	loss 2.4663405418395996
2022-12-12 20:22:35.930732	Iteration 83300 	loss 1.9401260614395142
2022-12-12 20:22:38.071888	Iteration 83400 	loss 2.5089550018310547
2022-12-12 20:22:40.207579	Iteration 83500 	loss 2.245234489440918
2022-12-12 20:22:42.479223	Iteration 83600 	loss 2.067081928253174
2022-12-12 20:22:44.611506	Iteration 83700 	loss 1.7638158798217773
2022-12-12 20:22:46.743399	Iteration 83800 	loss 2.208209991455078
2022-12-12 20:22:48.881398	Iteration 83900 	loss 2.148651123046875
2022-12-12 20:22:51.022573	Iteration 84000 	loss 2.536449432373047
2022-12-12 20:22:53.286979	Iteration 84100 	loss 2.368884325027466
2022-12-12 20:22:55.427819	Iteration 84200 	loss 1.9589934349060059
2022-12-12 20:22:57.551664	Iteration 84300 	loss 2.1702065467834473
2022-12-12 20:22:59.693880	Iteration 84400 	loss 2.4831130504608154
2022-12-12 20:23:01.857554	Iteration 84500 	loss 2.3775839805603027
2022-12-12 20:23:04.119897	Iteration 84600 	loss 2.524981737136841
2022-12-12 20:23:06.259215	Iteration 84700 	loss 2.2649903297424316
2022-12-12 20:23:08.394830	Iteration 84800 	loss 2.1145548820495605
2022-12-12 20:23:10.534335	Iteration 84900 	loss 2.2825160026550293
2022-12-12 20:23:12.675680	Iteration 85000 	loss 2.2798500061035156
2022-12-12 20:23:14.947042	Iteration 85100 	loss 2.6671335697174072
2022-12-12 20:23:17.084488	Iteration 85200 	loss 2.191481828689575
2022-12-12 20:23:19.236204	Iteration 85300 	loss 2.3549985885620117
2022-12-12 20:23:21.379813	Iteration 85400 	loss 2.378617763519287
2022-12-12 20:23:23.538271	Iteration 85500 	loss 2.285414695739746
2022-12-12 20:23:25.800083	Iteration 85600 	loss 1.7197813987731934
2022-12-12 20:23:27.932206	Iteration 85700 	loss 1.6159154176712036
2022-12-12 20:23:30.069359	Iteration 85800 	loss 2.403960704803467
2022-12-12 20:23:32.207765	Iteration 85900 	loss 2.3501315116882324
2022-12-12 20:23:34.342186	Iteration 86000 	loss 2.347902774810791
2022-12-12 20:23:36.600814	Iteration 86100 	loss 1.9765417575836182
2022-12-12 20:23:38.730984	Iteration 86200 	loss 2.4462809562683105
2022-12-12 20:23:40.858961	Iteration 86300 	loss 2.1435039043426514
2022-12-12 20:23:43.012241	Iteration 86400 	loss 2.4240529537200928
2022-12-12 20:23:45.125507	Iteration 86500 	loss 2.705451250076294
2022-12-12 20:23:47.393457	Iteration 86600 	loss 1.9895511865615845
2022-12-12 20:23:49.526503	Iteration 86700 	loss 2.2405550479888916
2022-12-12 20:23:51.664886	Iteration 86800 	loss 1.5912432670593262
2022-12-12 20:23:53.793541	Iteration 86900 	loss 2.2571160793304443
2022-12-12 20:23:55.926663	Iteration 87000 	loss 2.3317229747772217
2022-12-12 20:23:58.192060	Iteration 87100 	loss 2.1746060848236084
2022-12-12 20:24:00.330396	Iteration 87200 	loss 2.324939727783203
2022-12-12 20:24:02.459861	Iteration 87300 	loss 2.233346939086914
2022-12-12 20:24:04.601803	Iteration 87400 	loss 2.254861831665039
2022-12-12 20:24:06.729391	Iteration 87500 	loss 2.619105339050293
2022-12-12 20:24:08.995706	Iteration 87600 	loss 2.1018266677856445
2022-12-12 20:24:11.123093	Iteration 87700 	loss 2.209841012954712
2022-12-12 20:24:13.254464	Iteration 87800 	loss 2.5133986473083496
2022-12-12 20:24:15.382985	Iteration 87900 	loss 2.4170243740081787
2022-12-12 20:24:17.509170	Iteration 88000 	loss 1.7627761363983154
2022-12-12 20:24:19.832382	Iteration 88100 	loss 2.3700814247131348
2022-12-12 20:24:21.981783	Iteration 88200 	loss 2.744655132293701
2022-12-12 20:24:24.126584	Iteration 88300 	loss 2.071707248687744
2022-12-12 20:24:26.256331	Iteration 88400 	loss 1.7556337118148804
2022-12-12 20:24:28.405487	Iteration 88500 	loss 2.48606014251709
2022-12-12 20:24:30.657048	Iteration 88600 	loss 2.6172220706939697
2022-12-12 20:24:32.837847	Iteration 88700 	loss 2.9639358520507812
2022-12-12 20:24:34.990145	Iteration 88800 	loss 1.8480852842330933
2022-12-12 20:24:37.210585	Iteration 88900 	loss 2.2244985103607178
2022-12-12 20:24:39.400532	Iteration 89000 	loss 1.9969600439071655
2022-12-12 20:24:43.079283	Iteration 89100 	loss 2.0372519493103027
2022-12-12 20:25:41.984791	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-12-12 20:25:41.984963	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-12-12 20:27:47.976387	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-12-12 20:27:47.976871	cuda unavailable
2022-12-12 20:27:49.984502	Iteration 1 	loss 2.4800446033477783
2022-12-12 20:27:52.237518	Iteration 100 	loss 2.319432258605957
2022-12-12 20:27:54.491986	Iteration 200 	loss 2.296196937561035
2022-12-12 20:27:56.852984	Iteration 300 	loss 2.3362579345703125
2022-12-12 20:27:59.118632	Iteration 400 	loss 2.2824692726135254
2022-12-12 20:28:01.406959	Iteration 500 	loss 2.2227389812469482
2022-12-12 20:28:03.835234	Iteration 600 	loss 2.29850697517395
2022-12-12 20:28:06.187427	Iteration 700 	loss 2.220231056213379
2022-12-12 20:28:08.479607	Iteration 800 	loss 2.2478129863739014
2022-12-12 20:28:10.890293	Iteration 900 	loss 2.3442461490631104
2022-12-12 20:28:13.182294	Iteration 1000 	loss 2.1705496311187744
2022-12-12 20:28:15.587248	Iteration 1100 	loss 2.1334989070892334
2022-12-12 20:28:17.860243	Iteration 1200 	loss 2.340773820877075
2022-12-12 20:28:20.130214	Iteration 1300 	loss 2.3872599601745605
2022-12-12 20:28:22.406286	Iteration 1400 	loss 2.148660898208618
2022-12-12 20:28:24.724838	Iteration 1500 	loss 2.3434245586395264
2022-12-12 20:28:27.162169	Iteration 1600 	loss 2.1505672931671143
2022-12-12 20:28:29.454011	Iteration 1700 	loss 2.3689992427825928
2022-12-12 20:28:31.741386	Iteration 1800 	loss 2.289189100265503
2022-12-12 20:28:34.042567	Iteration 1900 	loss 2.3203020095825195
2022-12-12 20:28:36.320272	Iteration 2000 	loss 2.3197450637817383
2022-12-12 20:28:38.772115	Iteration 2100 	loss 2.022676944732666
2022-12-12 20:28:41.053673	Iteration 2200 	loss 2.2603602409362793
2022-12-12 20:28:43.319096	Iteration 2300 	loss 2.3291211128234863
2022-12-12 20:28:45.631351	Iteration 2400 	loss 2.3875584602355957
2022-12-12 20:28:47.954794	Iteration 2500 	loss 2.4567887783050537
2022-12-12 20:28:50.487894	Iteration 2600 	loss 2.5590670108795166
2022-12-12 20:28:52.798891	Iteration 2700 	loss 2.32680344581604
2022-12-12 20:28:55.092162	Iteration 2800 	loss 2.164280891418457
2022-12-12 20:28:57.363033	Iteration 2900 	loss 2.2271828651428223
2022-12-12 20:28:59.607420	Iteration 3000 	loss 2.0485594272613525
2022-12-12 20:29:02.006328	Iteration 3100 	loss 2.1019251346588135
2022-12-12 20:29:04.242814	Iteration 3200 	loss 2.323376178741455
2022-12-12 20:29:06.490101	Iteration 3300 	loss 2.048975944519043
2022-12-12 20:29:08.731200	Iteration 3400 	loss 2.348632335662842
2022-12-12 20:29:10.980562	Iteration 3500 	loss 2.614192485809326
2022-12-12 20:29:13.354341	Iteration 3600 	loss 2.46923828125
2022-12-12 20:29:15.599944	Iteration 3700 	loss 2.3904075622558594
2022-12-12 20:29:17.831659	Iteration 3800 	loss 2.449347496032715
2022-12-12 20:29:20.075554	Iteration 3900 	loss 2.458779811859131
2022-12-12 20:29:22.314763	Iteration 4000 	loss 2.247079849243164
2022-12-12 20:29:24.679581	Iteration 4100 	loss 1.8383156061172485
2022-12-12 20:29:26.912619	Iteration 4200 	loss 1.9126088619232178
2022-12-12 20:29:29.157056	Iteration 4300 	loss 2.088117837905884
2022-12-12 20:29:31.392861	Iteration 4400 	loss 2.1225409507751465
2022-12-12 20:29:33.639565	Iteration 4500 	loss 1.9926679134368896
2022-12-12 20:29:35.995130	Iteration 4600 	loss 2.041195869445801
2022-12-12 20:29:38.200708	Iteration 4700 	loss 2.4012088775634766
2022-12-12 20:29:40.422747	Iteration 4800 	loss 2.2179393768310547
2022-12-12 20:29:42.642285	Iteration 4900 	loss 2.626845121383667
2022-12-12 20:29:44.853438	Iteration 5000 	loss 2.175438404083252
2022-12-12 20:29:47.274991	Iteration 5100 	loss 2.322824001312256
2022-12-12 20:29:49.465518	Iteration 5200 	loss 2.4604647159576416
2022-12-12 20:29:51.650751	Iteration 5300 	loss 1.9649125337600708
2022-12-12 20:29:53.893128	Iteration 5400 	loss 2.2345681190490723
2022-12-12 20:29:56.099466	Iteration 5500 	loss 2.0120849609375
2022-12-12 20:29:58.450678	Iteration 5600 	loss 2.5874547958374023
2022-12-12 20:30:00.644210	Iteration 5700 	loss 2.099691152572632
2022-12-12 20:30:02.861270	Iteration 5800 	loss 2.006899356842041
2022-12-12 20:30:05.074033	Iteration 5900 	loss 2.0907044410705566
2022-12-12 20:30:07.298856	Iteration 6000 	loss 2.20927357673645
2022-12-12 20:30:09.668250	Iteration 6100 	loss 2.369231939315796
2022-12-12 20:30:11.897602	Iteration 6200 	loss 2.129879951477051
2022-12-16 20:37:48.512412	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-12-16 20:37:48.512930	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-12-16 20:38:11.405907	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-12-16 20:38:11.406356	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-12-16 20:38:13.395500	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-12-16 20:38:13.395828	cuda unavailable
2022-12-16 20:44:10.754164	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-12-16 20:44:10.755342	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-12-16 20:44:11.713790	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-12-16 20:44:11.714046	cuda unavailable
2022-12-17 09:40:40.128201	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-12-17 09:40:40.128581	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-12-17 09:42:41.281659	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-12-17 09:42:41.282015	cuda unavailable
2022-12-17 09:42:43.309428	Iteration 1 	loss 2.453317642211914
2022-12-17 09:42:45.687129	Iteration 100 	loss 2.2825775146484375
2022-12-17 09:42:48.098818	Iteration 200 	loss 2.3599612712860107
2022-12-17 09:42:50.520227	Iteration 300 	loss 2.217090606689453
2022-12-17 09:42:52.880946	Iteration 400 	loss 2.3337972164154053
2022-12-17 09:42:55.198359	Iteration 500 	loss 2.264185905456543
2022-12-17 09:42:57.721167	Iteration 600 	loss 2.338815212249756
2022-12-17 09:42:59.964802	Iteration 700 	loss 2.3812079429626465
2022-12-17 09:43:02.192795	Iteration 800 	loss 2.2271180152893066
2022-12-17 09:43:04.496040	Iteration 900 	loss 2.246727228164673
2022-12-17 09:43:06.743303	Iteration 1000 	loss 2.3603291511535645
2022-12-17 09:43:09.118746	Iteration 1100 	loss 2.517632484436035
2022-12-17 09:43:11.399297	Iteration 1200 	loss 2.2348077297210693
2022-12-17 09:43:13.671971	Iteration 1300 	loss 2.4160008430480957
2022-12-17 09:43:15.976676	Iteration 1400 	loss 2.4135994911193848
2022-12-17 09:43:18.371680	Iteration 1500 	loss 2.1326727867126465
2022-12-17 09:43:20.813306	Iteration 1600 	loss 2.380288600921631
2022-12-17 09:43:23.066118	Iteration 1700 	loss 1.911970615386963
2022-12-17 09:43:25.321860	Iteration 1800 	loss 2.0212631225585938
2022-12-17 09:43:27.562868	Iteration 1900 	loss 1.9701731204986572
2022-12-17 09:43:29.824790	Iteration 2000 	loss 1.9196768999099731
2022-12-17 09:43:32.222501	Iteration 2100 	loss 2.268789052963257
2022-12-17 09:43:34.500298	Iteration 2200 	loss 2.155808925628662
2022-12-17 09:43:36.760829	Iteration 2300 	loss 2.096320629119873
2022-12-17 09:43:39.057269	Iteration 2400 	loss 2.1679129600524902
2022-12-17 09:43:41.294718	Iteration 2500 	loss 2.1208693981170654
2022-12-17 09:43:43.756924	Iteration 2600 	loss 2.154041290283203
2022-12-17 09:43:46.044906	Iteration 2700 	loss 2.231377601623535
2022-12-17 09:43:48.301144	Iteration 2800 	loss 2.203029155731201
2022-12-17 09:43:50.535534	Iteration 2900 	loss 2.41570782661438
2022-12-17 09:43:52.778900	Iteration 3000 	loss 2.2930307388305664
2022-12-17 09:43:55.138975	Iteration 3100 	loss 2.3279387950897217
2022-12-17 09:43:57.365426	Iteration 3200 	loss 2.3754448890686035
2022-12-17 09:43:59.609985	Iteration 3300 	loss 2.0538887977600098
2022-12-17 09:44:01.837407	Iteration 3400 	loss 2.07138991355896
2022-12-17 09:44:04.094908	Iteration 3500 	loss 1.9305238723754883
2022-12-17 09:44:06.474474	Iteration 3600 	loss 1.921309471130371
2022-12-17 09:44:08.753236	Iteration 3700 	loss 2.2081775665283203
2022-12-17 09:44:11.018454	Iteration 3800 	loss 2.295527935028076
2022-12-17 09:44:13.333731	Iteration 3900 	loss 2.3013503551483154
2022-12-17 09:44:15.565647	Iteration 4000 	loss 2.2152585983276367
2022-12-17 09:44:17.988970	Iteration 4100 	loss 2.17673659324646
2022-12-17 09:44:20.225387	Iteration 4200 	loss 2.3976659774780273
2022-12-17 09:44:22.499186	Iteration 4300 	loss 2.2434234619140625
2022-12-17 09:44:24.726480	Iteration 4400 	loss 2.215045928955078
2022-12-17 09:44:26.968088	Iteration 4500 	loss 1.9786754846572876
2022-12-17 11:07:39.220010	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-12-17 11:07:39.220292	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-12-17 11:08:04.534454	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-12-17 11:08:04.534751	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-12-17 11:08:53.221830	folder path: saved/riemannian-diffusion/sphere/UniformSlice-64-0.001-10000-0-0-2.0-256-2-0-12345-0.995-100
2022-12-17 11:08:53.222309	Namespace(dataset='UniformSlice', dataroot='data', saveroot='saved', expname='sphere', print_every=100, sample_every=500, checkpoint_every=500, evaluate_every=5000, evaluation_K=10, num_steps=100, inference_num_steps=100, evaluation_num_steps=100, seed=12345, T0=2.0, emb_size=256, hidden_layers=2, imp=0, div=None, batch_size=64, val_batch_size=256, test_batch_size=256, lr=0.001, num_iterations=10000, sch=0, warmup_iters=0, ema_decay=0.995, mode='train')
2022-12-17 11:08:55.447128	ConcatSinMLP(
  (net): Sequential(
    (0): Linear(in_features=4, out_features=256, bias=True)
    (1): Sin()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): Sin()
    (4): Linear(in_features=256, out_features=3, bias=True)
  )
)
2022-12-17 11:08:55.447332	cuda unavailable
2022-12-17 11:08:57.764058	Iteration 1 	loss 2.567962884902954
2022-12-17 11:09:00.186816	Iteration 100 	loss 2.4803733825683594
2022-12-17 11:09:02.627046	Iteration 200 	loss 2.4052834510803223
2022-12-17 11:09:05.100162	Iteration 300 	loss 2.4778172969818115
2022-12-17 11:09:07.543842	Iteration 400 	loss 2.501185178756714
2022-12-17 11:09:09.994586	Iteration 500 	loss 2.3224339485168457
2022-12-17 11:09:12.585245	Iteration 600 	loss 2.340200901031494
2022-12-17 11:09:15.034954	Iteration 700 	loss 2.395251989364624
2022-12-17 11:09:17.472109	Iteration 800 	loss 2.4648642539978027
2022-12-17 11:09:19.919296	Iteration 900 	loss 2.5602362155914307
2022-12-17 11:09:22.366171	Iteration 1000 	loss 2.4250307083129883
2022-12-17 11:09:24.957447	Iteration 1100 	loss 2.3001067638397217
2022-12-17 11:09:27.424871	Iteration 1200 	loss 2.357445478439331
2022-12-17 11:09:29.877845	Iteration 1300 	loss 2.5044095516204834
2022-12-17 11:09:32.321823	Iteration 1400 	loss 2.311295747756958
2022-12-17 11:09:34.797429	Iteration 1500 	loss 2.406876564025879
2022-12-17 11:09:37.438264	Iteration 1600 	loss 2.540776252746582
2022-12-17 11:09:39.860091	Iteration 1700 	loss 2.572272777557373
2022-12-17 11:09:42.296124	Iteration 1800 	loss 2.626483201980591
2022-12-17 11:09:44.742546	Iteration 1900 	loss 2.057983160018921
2022-12-17 11:09:47.192752	Iteration 2000 	loss 2.1814427375793457
2022-12-17 11:09:49.782303	Iteration 2100 	loss 2.7180089950561523
2022-12-17 11:09:52.235730	Iteration 2200 	loss 2.2893452644348145
2022-12-17 11:09:54.674139	Iteration 2300 	loss 2.448410749435425
2022-12-17 11:09:57.126344	Iteration 2400 	loss 2.3597052097320557
2022-12-17 11:09:59.572719	Iteration 2500 	loss 2.5185599327087402
2022-12-17 11:10:02.157712	Iteration 2600 	loss 2.2006585597991943
2022-12-17 11:10:04.593084	Iteration 2700 	loss 2.5206661224365234
2022-12-17 11:10:07.024237	Iteration 2800 	loss 2.1962168216705322
2022-12-17 11:10:09.454639	Iteration 2900 	loss 2.6380720138549805
2022-12-17 11:10:11.917500	Iteration 3000 	loss 2.213380813598633
2022-12-17 11:10:14.498416	Iteration 3100 	loss 2.382883310317993
2022-12-17 11:10:16.965643	Iteration 3200 	loss 2.471787929534912
2022-12-17 11:10:19.410096	Iteration 3300 	loss 2.4064571857452393
2022-12-17 11:10:21.846648	Iteration 3400 	loss 2.606250524520874
2022-12-17 11:10:24.297718	Iteration 3500 	loss 2.396326780319214
2022-12-17 11:10:26.877692	Iteration 3600 	loss 2.3129513263702393
2022-12-17 11:10:29.307502	Iteration 3700 	loss 2.223670482635498
2022-12-17 11:10:31.744149	Iteration 3800 	loss 2.346906900405884
2022-12-17 11:10:34.182140	Iteration 3900 	loss 2.5318546295166016
2022-12-17 11:10:36.618165	Iteration 4000 	loss 2.491208791732788
2022-12-17 11:10:39.275030	Iteration 4100 	loss 2.332047462463379
2022-12-17 11:10:41.695837	Iteration 4200 	loss 2.2302021980285645
2022-12-17 11:10:44.139163	Iteration 4300 	loss 2.1221139430999756
2022-12-17 11:10:46.567110	Iteration 4400 	loss 2.175847291946411
2022-12-17 11:10:49.004906	Iteration 4500 	loss 2.2396247386932373
2022-12-17 11:10:51.571118	Iteration 4600 	loss 2.469651460647583
2022-12-17 11:10:53.983374	Iteration 4700 	loss 2.8303449153900146
2022-12-17 11:10:56.411043	Iteration 4800 	loss 2.6190950870513916
2022-12-17 11:10:58.878234	Iteration 4900 	loss 2.320078134536743
2022-12-17 11:11:01.316729	Iteration 5000 	loss 2.210092544555664
2022-12-17 11:11:03.899596	Iteration 5100 	loss 2.322174072265625
2022-12-17 11:11:06.339934	Iteration 5200 	loss 2.474480152130127
2022-12-17 11:11:08.772213	Iteration 5300 	loss 2.454055070877075
2022-12-17 11:11:11.206993	Iteration 5400 	loss 2.3622395992279053
2022-12-17 11:11:13.638207	Iteration 5500 	loss 2.561724901199341
2022-12-17 11:11:16.214250	Iteration 5600 	loss 2.35032057762146
2022-12-17 11:11:18.645764	Iteration 5700 	loss 2.568140983581543
2022-12-17 11:11:21.090386	Iteration 5800 	loss 2.168631076812744
2022-12-17 11:11:23.539939	Iteration 5900 	loss 2.4063503742218018
2022-12-17 11:11:26.219554	Iteration 6000 	loss 2.46510910987854
2022-12-17 11:11:28.852930	Iteration 6100 	loss 2.4207918643951416
2022-12-17 11:11:31.326950	Iteration 6200 	loss 2.528386116027832
2022-12-17 11:11:33.913245	Iteration 6300 	loss 2.240342378616333
2022-12-17 11:11:36.400546	Iteration 6400 	loss 2.1961097717285156
2022-12-17 11:11:38.845777	Iteration 6500 	loss 2.3957135677337646
2022-12-17 11:11:41.508938	Iteration 6600 	loss 2.438535690307617
2022-12-17 11:11:43.893748	Iteration 6700 	loss 2.029562473297119
2022-12-17 11:11:46.282629	Iteration 6800 	loss 2.2809603214263916
2022-12-17 11:11:48.685514	Iteration 6900 	loss 2.3248562812805176
2022-12-17 11:11:51.165036	Iteration 7000 	loss 2.0171170234680176
2022-12-17 11:11:53.760887	Iteration 7100 	loss 2.0410029888153076
2022-12-17 11:11:56.172199	Iteration 7200 	loss 2.2681944370269775
2022-12-17 11:11:58.626983	Iteration 7300 	loss 2.294517993927002
2022-12-17 11:12:01.066108	Iteration 7400 	loss 1.93663489818573
2022-12-17 11:12:03.500334	Iteration 7500 	loss 2.429300546646118
2022-12-17 11:12:06.050883	Iteration 7600 	loss 2.2439143657684326
2022-12-17 11:12:08.485865	Iteration 7700 	loss 2.234614610671997
2022-12-17 11:12:10.936490	Iteration 7800 	loss 2.1826202869415283
2022-12-17 11:12:13.408192	Iteration 7900 	loss 2.655390739440918
2022-12-17 11:12:15.867982	Iteration 8000 	loss 2.4647483825683594
2022-12-17 11:12:18.454036	Iteration 8100 	loss 2.623460292816162
2022-12-17 11:12:20.913736	Iteration 8200 	loss 2.224721908569336
2022-12-17 11:12:23.330809	Iteration 8300 	loss 2.4131805896759033
2022-12-17 11:12:25.758664	Iteration 8400 	loss 1.9273078441619873
2022-12-17 11:12:28.185200	Iteration 8500 	loss 2.3036584854125977
2022-12-17 11:12:30.750930	Iteration 8600 	loss 2.372187614440918
2022-12-17 11:12:33.191360	Iteration 8700 	loss 2.2976772785186768
2022-12-17 11:12:35.660784	Iteration 8800 	loss 2.2042407989501953
2022-12-17 11:12:38.118048	Iteration 8900 	loss 2.2050042152404785
2022-12-17 11:12:40.588770	Iteration 9000 	loss 2.313812255859375
2022-12-17 11:12:43.218691	Iteration 9100 	loss 2.338207483291626
2022-12-17 11:12:45.673005	Iteration 9200 	loss 1.8733731508255005
2022-12-17 11:12:48.143997	Iteration 9300 	loss 2.46541690826416
2022-12-17 11:12:50.583920	Iteration 9400 	loss 2.443890333175659
2022-12-17 11:12:52.990277	Iteration 9500 	loss 1.9508695602416992
2022-12-17 11:12:55.516325	Iteration 9600 	loss 2.0384573936462402
2022-12-17 11:12:58.001730	Iteration 9700 	loss 1.7181991338729858
2022-12-17 11:13:00.533700	Iteration 9800 	loss 2.156651496887207
2022-12-17 11:13:03.020420	Iteration 9900 	loss 2.2708871364593506
2022-12-17 11:13:05.454136	Iteration 10000 	loss 1.852538824081421
2022-12-17 11:13:08.176312	Iteration 10100 	loss 2.2898144721984863
2022-12-17 11:13:10.699408	Iteration 10200 	loss 2.1973330974578857
2022-12-17 11:13:13.253516	Iteration 10300 	loss 1.725917100906372
2022-12-17 11:13:15.683695	Iteration 10400 	loss 2.350999355316162
2022-12-17 11:13:18.140967	Iteration 10500 	loss 1.8748536109924316
2022-12-17 11:13:20.858542	Iteration 10600 	loss 2.2169952392578125
2022-12-17 11:13:23.286165	Iteration 10700 	loss 2.0570108890533447
2022-12-17 11:13:25.710115	Iteration 10800 	loss 2.379514217376709
2022-12-17 11:13:28.136446	Iteration 10900 	loss 2.4428467750549316
2022-12-17 11:13:30.559720	Iteration 11000 	loss 1.8254152536392212
2022-12-17 11:13:33.157562	Iteration 11100 	loss 2.413116216659546
2022-12-17 11:13:35.619114	Iteration 11200 	loss 2.297384262084961
2022-12-17 11:13:38.108111	Iteration 11300 	loss 2.1931393146514893
2022-12-17 11:13:40.530713	Iteration 11400 	loss 1.9325957298278809
2022-12-17 11:13:42.958141	Iteration 11500 	loss 2.1019299030303955
2022-12-17 11:13:45.635527	Iteration 11600 	loss 2.292733907699585
2022-12-17 11:13:48.082795	Iteration 11700 	loss 2.125675678253174
2022-12-17 11:13:50.554567	Iteration 11800 	loss 2.1989312171936035
2022-12-17 11:13:53.005179	Iteration 11900 	loss 2.5223047733306885
2022-12-17 11:13:55.459203	Iteration 12000 	loss 1.8217291831970215
2022-12-17 11:13:58.093865	Iteration 12100 	loss 2.803953170776367
2022-12-17 11:14:00.612378	Iteration 12200 	loss 2.4044392108917236
2022-12-17 11:14:03.079049	Iteration 12300 	loss 2.415088653564453
2022-12-17 11:14:05.577685	Iteration 12400 	loss 2.382905960083008
2022-12-17 11:14:08.029007	Iteration 12500 	loss 2.3843069076538086
2022-12-17 11:14:10.613618	Iteration 12600 	loss 2.1165690422058105
2022-12-17 11:14:13.078919	Iteration 12700 	loss 2.299055576324463
2022-12-17 11:14:15.581107	Iteration 12800 	loss 2.1443703174591064
2022-12-17 11:14:18.049663	Iteration 12900 	loss 2.311479330062866
2022-12-17 11:14:20.523436	Iteration 13000 	loss 2.5543198585510254
2022-12-17 11:14:23.134746	Iteration 13100 	loss 2.4275660514831543
2022-12-17 11:14:25.573803	Iteration 13200 	loss 2.165876626968384
2022-12-17 11:14:28.047317	Iteration 13300 	loss 2.5560719966888428
2022-12-17 11:14:30.493562	Iteration 13400 	loss 1.8272171020507812
2022-12-17 11:14:32.979084	Iteration 13500 	loss 2.3223717212677
2022-12-17 11:14:35.584754	Iteration 13600 	loss 2.117689847946167
2022-12-17 11:14:38.053395	Iteration 13700 	loss 1.9500079154968262
2022-12-17 11:14:40.503921	Iteration 13800 	loss 2.1372554302215576
2022-12-17 11:14:42.966146	Iteration 13900 	loss 2.0399582386016846
2022-12-17 11:14:45.403061	Iteration 14000 	loss 2.521477222442627
2022-12-17 11:14:48.010077	Iteration 14100 	loss 2.1456525325775146
2022-12-17 11:14:50.464774	Iteration 14200 	loss 2.409808397293091
2022-12-17 11:14:52.923211	Iteration 14300 	loss 1.8969404697418213
2022-12-17 11:14:55.360375	Iteration 14400 	loss 2.3861613273620605
2022-12-17 11:14:57.798100	Iteration 14500 	loss 2.0130605697631836
2022-12-17 11:15:00.392218	Iteration 14600 	loss 2.2817206382751465
2022-12-17 11:15:02.824755	Iteration 14700 	loss 2.3896644115448
2022-12-17 11:15:05.300165	Iteration 14800 	loss 2.2903363704681396
2022-12-17 11:15:07.834240	Iteration 14900 	loss 2.1973116397857666
2022-12-17 11:15:10.301394	Iteration 15000 	loss 2.7660341262817383
2022-12-17 11:15:13.101053	Iteration 15100 	loss 2.379896879196167
2022-12-17 11:15:15.545879	Iteration 15200 	loss 2.577453851699829
2022-12-17 11:15:17.980584	Iteration 15300 	loss 2.4070353507995605
2022-12-17 11:15:20.418683	Iteration 15400 	loss 2.3398075103759766
2022-12-17 11:15:22.851541	Iteration 15500 	loss 2.1358046531677246
2022-12-17 11:15:25.429718	Iteration 15600 	loss 2.3682312965393066
2022-12-17 11:15:27.857992	Iteration 15700 	loss 2.1826000213623047
2022-12-17 11:15:30.324762	Iteration 15800 	loss 2.331507444381714
2022-12-17 11:15:32.800823	Iteration 15900 	loss 2.0323452949523926
2022-12-17 11:15:35.297175	Iteration 16000 	loss 2.211061477661133
2022-12-17 11:15:37.902962	Iteration 16100 	loss 2.1481733322143555
2022-12-17 11:15:40.384274	Iteration 16200 	loss 2.0733399391174316
2022-12-17 11:15:42.883584	Iteration 16300 	loss 2.2183291912078857
2022-12-17 11:15:45.384850	Iteration 16400 	loss 2.5980916023254395
2022-12-17 11:15:47.842682	Iteration 16500 	loss 2.06553053855896
2022-12-17 11:15:50.471875	Iteration 16600 	loss 2.2805230617523193
2022-12-17 11:15:52.944345	Iteration 16700 	loss 2.240003824234009
2022-12-17 11:15:55.390312	Iteration 16800 	loss 2.732544422149658
2022-12-17 11:15:57.850355	Iteration 16900 	loss 2.0606911182403564
2022-12-17 11:16:00.318907	Iteration 17000 	loss 2.73049259185791
2022-12-17 11:16:02.929486	Iteration 17100 	loss 2.2147257328033447
2022-12-17 11:16:05.367580	Iteration 17200 	loss 2.254133939743042
2022-12-17 11:16:07.797362	Iteration 17300 	loss 2.427263021469116
2022-12-17 11:16:10.242147	Iteration 17400 	loss 1.6954237222671509
2022-12-17 11:16:12.672789	Iteration 17500 	loss 2.0943174362182617
2022-12-17 11:16:15.241000	Iteration 17600 	loss 2.333082437515259
2022-12-17 11:16:17.668922	Iteration 17700 	loss 2.2132649421691895
2022-12-17 11:16:20.098308	Iteration 17800 	loss 2.56254243850708
2022-12-17 11:16:22.545810	Iteration 17900 	loss 2.04752516746521
2022-12-17 11:16:24.995215	Iteration 18000 	loss 2.331305980682373
2022-12-17 11:16:27.596381	Iteration 18100 	loss 2.3782267570495605
2022-12-17 11:16:30.056041	Iteration 18200 	loss 2.1720194816589355
2022-12-17 11:16:32.544569	Iteration 18300 	loss 2.389359474182129
2022-12-17 11:16:34.996941	Iteration 18400 	loss 2.300760269165039
2022-12-17 11:16:37.463323	Iteration 18500 	loss 2.4907660484313965
2022-12-17 11:16:40.063639	Iteration 18600 	loss 2.3564178943634033
2022-12-17 11:16:42.494040	Iteration 18700 	loss 1.849530816078186
2022-12-17 11:16:44.920651	Iteration 18800 	loss 2.356302261352539
2022-12-17 11:16:47.351435	Iteration 18900 	loss 2.060145854949951
2022-12-17 11:16:49.809604	Iteration 19000 	loss 2.185473918914795
2022-12-17 11:16:52.417466	Iteration 19100 	loss 1.8576922416687012
2022-12-17 11:16:54.883263	Iteration 19200 	loss 2.2299015522003174
2022-12-17 11:16:57.362441	Iteration 19300 	loss 2.3636512756347656
2022-12-17 11:16:59.837256	Iteration 19400 	loss 2.026834011077881
2022-12-17 11:17:02.314716	Iteration 19500 	loss 2.717064142227173
2022-12-17 11:17:05.025849	Iteration 19600 	loss 2.202009916305542
2022-12-17 11:17:07.496087	Iteration 19700 	loss 1.8661271333694458
2022-12-17 11:17:09.942434	Iteration 19800 	loss 2.091686725616455
2022-12-17 11:17:12.371899	Iteration 19900 	loss 2.3474669456481934
2022-12-17 11:17:14.815931	Iteration 20000 	loss 2.1663260459899902
2022-12-17 11:17:17.392418	Iteration 20100 	loss 2.5134148597717285
2022-12-17 11:17:19.835055	Iteration 20200 	loss 2.089975595474243
2022-12-17 11:17:22.369394	Iteration 20300 	loss 1.8176134824752808
2022-12-17 11:17:24.851390	Iteration 20400 	loss 2.4674932956695557
2022-12-17 11:17:27.286332	Iteration 20500 	loss 2.505842447280884
2022-12-17 11:17:30.098113	Iteration 20600 	loss 2.62843656539917
2022-12-17 11:17:32.617520	Iteration 20700 	loss 1.7879003286361694
2022-12-17 11:17:35.095437	Iteration 20800 	loss 2.3719468116760254
2022-12-17 11:17:37.842050	Iteration 20900 	loss 2.1291298866271973
2022-12-17 11:17:40.308534	Iteration 21000 	loss 2.215355396270752
2022-12-17 11:17:43.041068	Iteration 21100 	loss 1.8276326656341553
2022-12-17 11:17:45.554796	Iteration 21200 	loss 2.107184410095215
2022-12-17 11:17:48.110803	Iteration 21300 	loss 2.259099245071411
2022-12-17 11:17:50.777637	Iteration 21400 	loss 2.3028695583343506
2022-12-17 11:17:53.296063	Iteration 21500 	loss 2.3720078468322754
2022-12-17 11:17:55.903384	Iteration 21600 	loss 2.0702013969421387
2022-12-17 11:17:58.464245	Iteration 21700 	loss 2.8096351623535156
2022-12-17 11:18:00.964133	Iteration 21800 	loss 2.1959476470947266
2022-12-17 11:18:03.486763	Iteration 21900 	loss 2.484562635421753
2022-12-17 11:18:05.945837	Iteration 22000 	loss 2.5074987411499023
2022-12-17 11:18:08.573474	Iteration 22100 	loss 2.2608039379119873
2022-12-17 11:18:11.057895	Iteration 22200 	loss 2.377487897872925
2022-12-17 11:18:13.650769	Iteration 22300 	loss 2.3297159671783447
2022-12-17 11:18:16.178991	Iteration 22400 	loss 2.507547616958618
2022-12-17 11:18:18.923098	Iteration 22500 	loss 2.497628688812256
2022-12-17 11:18:21.608520	Iteration 22600 	loss 2.0029852390289307
2022-12-17 11:18:24.248733	Iteration 22700 	loss 2.235694408416748
2022-12-17 11:18:26.862733	Iteration 22800 	loss 2.225586175918579
2022-12-17 11:18:29.368629	Iteration 22900 	loss 2.397866725921631
2022-12-17 11:18:31.932735	Iteration 23000 	loss 1.8754099607467651
2022-12-17 11:18:34.619763	Iteration 23100 	loss 2.1540021896362305
2022-12-17 11:18:37.176068	Iteration 23200 	loss 1.9122238159179688
2022-12-17 11:18:39.683295	Iteration 23300 	loss 2.564551591873169
2022-12-17 11:18:42.198518	Iteration 23400 	loss 1.734331488609314
2022-12-17 11:18:44.869622	Iteration 23500 	loss 2.549246311187744
2022-12-17 11:18:47.551261	Iteration 23600 	loss 2.2544751167297363
2022-12-17 11:18:50.003636	Iteration 23700 	loss 2.4374845027923584
2022-12-17 11:18:52.581670	Iteration 23800 	loss 2.099607467651367
2022-12-17 11:18:55.066721	Iteration 23900 	loss 2.314804792404175
2022-12-17 11:18:57.562302	Iteration 24000 	loss 2.613269805908203
2022-12-17 11:19:00.241545	Iteration 24100 	loss 2.214874267578125
2022-12-17 11:19:02.712971	Iteration 24200 	loss 2.0678629875183105
2022-12-17 11:19:05.232183	Iteration 24300 	loss 2.3623838424682617
2022-12-17 11:19:07.741668	Iteration 24400 	loss 1.766297698020935
2022-12-17 11:19:10.291797	Iteration 24500 	loss 2.6577491760253906
2022-12-17 11:19:13.052615	Iteration 24600 	loss 2.6774673461914062
2022-12-17 11:19:15.509837	Iteration 24700 	loss 2.1057379245758057
2022-12-17 11:19:17.970744	Iteration 24800 	loss 2.076772689819336
2022-12-17 11:19:20.544681	Iteration 24900 	loss 2.2822048664093018
2022-12-17 11:19:23.128348	Iteration 25000 	loss 2.779395580291748
2022-12-17 11:19:25.803473	Iteration 25100 	loss 2.893782377243042
2022-12-17 11:19:28.327603	Iteration 25200 	loss 2.202554702758789
2022-12-17 11:19:30.810445	Iteration 25300 	loss 2.1165530681610107
2022-12-17 11:19:33.342190	Iteration 25400 	loss 1.8179572820663452
2022-12-17 11:19:35.967829	Iteration 25500 	loss 2.0571534633636475
2022-12-17 11:19:38.564109	Iteration 25600 	loss 2.075998067855835
2022-12-17 11:19:40.990824	Iteration 25700 	loss 1.9707660675048828
2022-12-17 11:19:43.423335	Iteration 25800 	loss 2.761260986328125
2022-12-17 11:19:45.862408	Iteration 25900 	loss 2.072467088699341
2022-12-17 11:19:48.291961	Iteration 26000 	loss 2.2768001556396484
2022-12-17 11:19:50.982649	Iteration 26100 	loss 2.312565803527832
2022-12-17 11:19:53.419481	Iteration 26200 	loss 2.1976425647735596
2022-12-17 11:19:55.871251	Iteration 26300 	loss 2.5959150791168213
2022-12-17 11:19:58.390737	Iteration 26400 	loss 2.1482014656066895
2022-12-17 11:20:00.998998	Iteration 26500 	loss 1.8370696306228638
2022-12-17 11:20:03.568243	Iteration 26600 	loss 2.3496532440185547
2022-12-17 11:20:06.012775	Iteration 26700 	loss 2.1363725662231445
2022-12-17 11:20:08.552009	Iteration 26800 	loss 1.9602346420288086
2022-12-17 11:20:11.097097	Iteration 26900 	loss 2.3683764934539795
2022-12-17 11:20:13.611770	Iteration 27000 	loss 2.7246432304382324
2022-12-17 11:20:16.297840	Iteration 27100 	loss 2.2623307704925537
2022-12-17 11:20:18.748940	Iteration 27200 	loss 2.432568073272705
2022-12-17 11:20:21.307859	Iteration 27300 	loss 2.3546793460845947
2022-12-17 11:20:23.810392	Iteration 27400 	loss 1.9326882362365723
2022-12-17 11:20:26.245826	Iteration 27500 	loss 2.1752617359161377
2022-12-17 11:20:29.166080	Iteration 27600 	loss 2.2281980514526367
2022-12-17 11:20:31.592815	Iteration 27700 	loss 2.044497489929199
2022-12-17 11:20:34.037727	Iteration 27800 	loss 2.0083086490631104
2022-12-17 11:20:36.466741	Iteration 27900 	loss 2.673971176147461
2022-12-17 11:20:38.890686	Iteration 28000 	loss 2.163689613342285
2022-12-17 11:20:41.448525	Iteration 28100 	loss 2.0899124145507812
2022-12-17 11:20:43.891993	Iteration 28200 	loss 1.7759568691253662
2022-12-17 11:20:46.325090	Iteration 28300 	loss 2.3422791957855225
2022-12-17 11:20:48.745356	Iteration 28400 	loss 2.429555892944336
2022-12-17 11:20:51.173222	Iteration 28500 	loss 2.443786382675171
2022-12-17 11:20:53.739430	Iteration 28600 	loss 2.197848081588745
2022-12-17 11:20:56.155052	Iteration 28700 	loss 1.890852928161621
2022-12-17 11:20:58.561749	Iteration 28800 	loss 2.018481969833374
2022-12-17 11:21:00.978123	Iteration 28900 	loss 2.46211576461792
2022-12-17 11:21:03.403684	Iteration 29000 	loss 2.112541913986206
2022-12-17 11:21:05.968482	Iteration 29100 	loss 1.9507806301116943
2022-12-17 11:21:08.394638	Iteration 29200 	loss 2.0356223583221436
2022-12-17 11:21:10.823745	Iteration 29300 	loss 2.3262932300567627
2022-12-17 11:21:13.244904	Iteration 29400 	loss 2.6175334453582764
2022-12-17 11:21:15.673583	Iteration 29500 	loss 2.517782688140869
2022-12-17 11:21:18.234581	Iteration 29600 	loss 2.7781291007995605
2022-12-17 11:21:20.660847	Iteration 29700 	loss 2.3359804153442383
2022-12-17 11:21:23.067650	Iteration 29800 	loss 2.011154890060425
2022-12-17 11:21:25.517646	Iteration 29900 	loss 2.376879930496216
2022-12-17 11:21:27.952515	Iteration 30000 	loss 2.079637050628662
2022-12-17 11:21:30.513827	Iteration 30100 	loss 2.3892037868499756
2022-12-17 11:21:32.950680	Iteration 30200 	loss 2.1081364154815674
2022-12-17 11:21:35.378649	Iteration 30300 	loss 2.4290881156921387
2022-12-17 11:21:37.811109	Iteration 30400 	loss 2.8620455265045166
2022-12-17 11:21:40.232089	Iteration 30500 	loss 2.197317361831665
2022-12-17 11:21:42.802828	Iteration 30600 	loss 2.77482008934021
2022-12-17 11:21:45.211664	Iteration 30700 	loss 2.3302571773529053
2022-12-17 11:21:47.695259	Iteration 30800 	loss 1.981182336807251
2022-12-17 11:21:50.314738	Iteration 30900 	loss 2.468369722366333
2022-12-17 11:21:52.917447	Iteration 31000 	loss 1.9610555171966553
2022-12-17 11:21:55.520331	Iteration 31100 	loss 2.131220579147339
2022-12-17 11:21:57.988166	Iteration 31200 	loss 2.0476255416870117
2022-12-17 11:22:00.549178	Iteration 31300 	loss 2.2026619911193848
2022-12-17 11:22:03.037878	Iteration 31400 	loss 2.023653030395508
2022-12-17 11:22:05.582751	Iteration 31500 	loss 2.2782540321350098
2022-12-17 11:22:08.180148	Iteration 31600 	loss 1.7656790018081665
2022-12-17 11:22:10.712295	Iteration 31700 	loss 2.5395052433013916
2022-12-17 11:22:13.345154	Iteration 31800 	loss 2.603346347808838
2022-12-17 11:22:15.791797	Iteration 31900 	loss 2.1248111724853516
2022-12-17 11:22:18.228464	Iteration 32000 	loss 2.4603850841522217
2022-12-17 11:22:20.812546	Iteration 32100 	loss 1.4772913455963135
2022-12-17 11:22:23.251188	Iteration 32200 	loss 2.3445165157318115
2022-12-17 11:22:25.685431	Iteration 32300 	loss 2.1988818645477295
2022-12-17 11:22:28.103673	Iteration 32400 	loss 1.3470854759216309
2022-12-17 11:22:30.538497	Iteration 32500 	loss 2.201496124267578
2022-12-17 11:22:33.103299	Iteration 32600 	loss 2.1073875427246094
2022-12-17 11:22:35.535214	Iteration 32700 	loss 2.4173786640167236
2022-12-17 11:22:38.008077	Iteration 32800 	loss 2.1946840286254883
2022-12-17 11:22:40.529147	Iteration 32900 	loss 2.209218978881836
2022-12-17 11:22:43.190318	Iteration 33000 	loss 2.296333074569702
2022-12-17 11:22:45.914796	Iteration 33100 	loss 2.149563789367676
2022-12-17 11:22:48.531096	Iteration 33200 	loss 2.3942785263061523
2022-12-17 11:22:51.133648	Iteration 33300 	loss 1.9494521617889404
2022-12-17 11:22:53.657512	Iteration 33400 	loss 2.3159878253936768
2022-12-17 11:22:56.152068	Iteration 33500 	loss 2.0071017742156982
2022-12-17 11:22:58.781580	Iteration 33600 	loss 2.686131715774536
2022-12-17 11:23:01.256706	Iteration 33700 	loss 2.4049782752990723
2022-12-17 11:23:03.810164	Iteration 33800 	loss 2.400599479675293
2022-12-17 11:23:06.237541	Iteration 33900 	loss 2.659820318222046
2022-12-17 11:23:08.813685	Iteration 34000 	loss 2.1251912117004395
2022-12-17 11:23:11.509520	Iteration 34100 	loss 1.9452804327011108
2022-12-17 11:23:14.035295	Iteration 34200 	loss 2.286895275115967
2022-12-17 11:23:16.485301	Iteration 34300 	loss 2.5937836170196533
2022-12-17 11:23:18.999187	Iteration 34400 	loss 2.4404759407043457
2022-12-17 11:23:21.495276	Iteration 34500 	loss 2.344451665878296
2022-12-17 11:23:24.087593	Iteration 34600 	loss 2.1981146335601807
2022-12-17 11:23:26.566366	Iteration 34700 	loss 2.128126621246338
2022-12-17 11:23:29.037512	Iteration 34800 	loss 1.9920755624771118
2022-12-17 11:23:31.475973	Iteration 34900 	loss 2.0413331985473633
2022-12-17 11:23:33.970445	Iteration 35000 	loss 2.23633074760437
2022-12-17 11:23:36.631672	Iteration 35100 	loss 2.270908832550049
2022-12-17 11:23:39.134592	Iteration 35200 	loss 2.2257728576660156
2022-12-17 11:23:41.675302	Iteration 35300 	loss 1.9856250286102295
2022-12-17 11:23:44.155268	Iteration 35400 	loss 2.2113797664642334
2022-12-17 11:23:46.646015	Iteration 35500 	loss 2.3146443367004395
2022-12-17 11:23:49.248264	Iteration 35600 	loss 2.2887215614318848
2022-12-17 11:23:51.698183	Iteration 35700 	loss 1.9783239364624023
2022-12-17 11:23:54.152371	Iteration 35800 	loss 2.3770928382873535
2022-12-17 11:23:56.577715	Iteration 35900 	loss 2.187318801879883
2022-12-17 11:23:58.997149	Iteration 36000 	loss 2.219882011413574
2022-12-17 11:24:01.570575	Iteration 36100 	loss 2.3409061431884766
2022-12-17 11:24:04.085327	Iteration 36200 	loss 2.184736728668213
2022-12-17 11:24:06.569176	Iteration 36300 	loss 2.036677598953247
2022-12-17 11:24:09.041921	Iteration 36400 	loss 2.138007402420044
2022-12-17 11:24:11.518288	Iteration 36500 	loss 2.151226043701172
2022-12-17 11:24:14.570373	Iteration 36600 	loss 2.328512668609619
2022-12-17 11:24:16.986117	Iteration 36700 	loss 2.687943458557129
2022-12-17 11:24:19.399186	Iteration 36800 	loss 2.807215690612793
2022-12-17 11:24:21.812465	Iteration 36900 	loss 2.3219075202941895
2022-12-17 11:24:24.228627	Iteration 37000 	loss 2.1267266273498535
2022-12-17 11:24:26.872408	Iteration 37100 	loss 2.2734532356262207
2022-12-17 11:24:29.352149	Iteration 37200 	loss 2.4867329597473145
2022-12-17 11:24:31.891884	Iteration 37300 	loss 2.494898557662964
2022-12-17 11:24:34.440926	Iteration 37400 	loss 2.4021425247192383
2022-12-17 11:24:36.906239	Iteration 37500 	loss 2.477471351623535
2022-12-17 11:24:39.516631	Iteration 37600 	loss 2.791454792022705
2022-12-17 11:24:41.954741	Iteration 37700 	loss 2.455070734024048
2022-12-17 11:24:44.348167	Iteration 37800 	loss 2.203474521636963
2022-12-17 11:24:46.856864	Iteration 37900 	loss 2.3188512325286865
2022-12-17 11:24:49.437293	Iteration 38000 	loss 2.8024673461914062
2022-12-17 11:24:52.006150	Iteration 38100 	loss 2.145974636077881
2022-12-17 11:24:54.461128	Iteration 38200 	loss 1.8926117420196533
2022-12-17 11:24:57.039416	Iteration 38300 	loss 2.584617853164673
2022-12-17 11:24:59.613069	Iteration 38400 	loss 2.572150707244873
2022-12-17 11:25:02.145827	Iteration 38500 	loss 2.150796890258789
2022-12-17 11:25:04.690485	Iteration 38600 	loss 2.6317129135131836
2022-12-17 11:25:07.095741	Iteration 38700 	loss 1.7197320461273193
2022-12-17 11:25:09.546889	Iteration 38800 	loss 1.638831377029419
2022-12-17 11:25:12.016608	Iteration 38900 	loss 1.851806879043579
2022-12-17 11:25:14.424987	Iteration 39000 	loss 2.529648780822754
2022-12-17 11:25:16.971218	Iteration 39100 	loss 1.9768779277801514
2022-12-17 11:25:19.381921	Iteration 39200 	loss 2.101378917694092
2022-12-17 11:25:21.790077	Iteration 39300 	loss 1.5260862112045288
2022-12-17 11:25:24.366604	Iteration 39400 	loss 2.067258358001709
2022-12-17 11:25:26.929873	Iteration 39500 	loss 2.300632953643799
2022-12-17 11:25:29.513155	Iteration 39600 	loss 1.8888214826583862
2022-12-17 11:25:31.959460	Iteration 39700 	loss 2.4016921520233154
2022-12-17 11:25:34.361417	Iteration 39800 	loss 1.939514398574829
2022-12-17 11:25:36.898218	Iteration 39900 	loss 2.162363290786743
2022-12-17 11:25:39.403388	Iteration 40000 	loss 2.050471305847168
2022-12-17 11:25:41.954495	Iteration 40100 	loss 2.306833267211914
2022-12-17 11:25:44.369040	Iteration 40200 	loss 2.1964542865753174
2022-12-17 11:25:46.813626	Iteration 40300 	loss 1.7216598987579346
2022-12-17 11:25:49.278600	Iteration 40400 	loss 2.153346061706543
2022-12-17 11:25:51.758742	Iteration 40500 	loss 2.429243564605713
2022-12-17 11:25:54.396148	Iteration 40600 	loss 2.4546053409576416
2022-12-17 11:25:56.820665	Iteration 40700 	loss 2.217589855194092
2022-12-17 11:25:59.250756	Iteration 40800 	loss 2.1935064792633057
2022-12-17 11:26:01.699534	Iteration 40900 	loss 2.012054681777954
2022-12-17 11:26:04.090462	Iteration 41000 	loss 2.105461835861206
2022-12-17 11:26:06.622192	Iteration 41100 	loss 2.4783051013946533
2022-12-17 11:26:09.011421	Iteration 41200 	loss 2.208852767944336
2022-12-17 11:26:11.408383	Iteration 41300 	loss 2.1408989429473877
2022-12-17 11:26:13.795935	Iteration 41400 	loss 2.4547572135925293
2022-12-17 11:26:16.185754	Iteration 41500 	loss 2.1579430103302
2022-12-17 11:26:18.722195	Iteration 41600 	loss 2.4003400802612305
2022-12-17 11:26:21.116255	Iteration 41700 	loss 2.0007622241973877
2022-12-17 11:26:23.502121	Iteration 41800 	loss 1.9630029201507568
2022-12-17 11:26:25.907861	Iteration 41900 	loss 2.344481945037842
2022-12-17 11:26:28.293713	Iteration 42000 	loss 2.0669891834259033
2022-12-17 11:26:30.881651	Iteration 42100 	loss 1.7931146621704102
2022-12-17 11:26:33.341344	Iteration 42200 	loss 2.2271687984466553
2022-12-17 11:26:35.795511	Iteration 42300 	loss 2.7111637592315674
2022-12-17 11:26:38.211682	Iteration 42400 	loss 2.121098756790161
2022-12-17 11:26:40.693090	Iteration 42500 	loss 1.8203456401824951
2022-12-17 11:26:43.279603	Iteration 42600 	loss 1.522139310836792
2022-12-17 11:26:45.669682	Iteration 42700 	loss 2.6984102725982666
2022-12-17 11:26:48.073238	Iteration 42800 	loss 2.1690611839294434
2022-12-17 11:26:50.466195	Iteration 42900 	loss 2.1933953762054443
2022-12-17 11:26:52.850738	Iteration 43000 	loss 2.0377190113067627
2022-12-17 11:26:55.410986	Iteration 43100 	loss 2.235050916671753
2022-12-17 11:26:57.941298	Iteration 43200 	loss 2.293731689453125
2022-12-17 11:27:00.400721	Iteration 43300 	loss 1.8068859577178955
2022-12-17 11:27:02.863753	Iteration 43400 	loss 2.017111301422119
2022-12-17 11:27:05.328152	Iteration 43500 	loss 2.438234329223633
2022-12-17 11:27:07.897241	Iteration 43600 	loss 2.0322022438049316
2022-12-17 11:27:10.293128	Iteration 43700 	loss 1.99712336063385
2022-12-17 11:27:12.734293	Iteration 43800 	loss 2.570543050765991
2022-12-17 11:27:15.138267	Iteration 43900 	loss 2.5979056358337402
2022-12-17 11:27:17.550245	Iteration 44000 	loss 1.9979239702224731
2022-12-17 11:27:20.117579	Iteration 44100 	loss 2.4412155151367188
2022-12-17 11:27:22.533123	Iteration 44200 	loss 2.391118288040161
2022-12-17 11:27:24.960316	Iteration 44300 	loss 2.0897738933563232
2022-12-17 11:27:27.357928	Iteration 44400 	loss 2.3480653762817383
2022-12-17 11:27:29.768335	Iteration 44500 	loss 2.1624631881713867
2022-12-17 11:27:32.314206	Iteration 44600 	loss 2.2002623081207275
2022-12-17 11:27:34.738198	Iteration 44700 	loss 2.181103467941284
2022-12-17 11:27:37.157329	Iteration 44800 	loss 2.0555195808410645
2022-12-17 11:27:39.590911	Iteration 44900 	loss 2.867816209793091
2022-12-17 11:27:42.119334	Iteration 45000 	loss 2.046069622039795
2022-12-17 11:27:44.681509	Iteration 45100 	loss 2.634755849838257
2022-12-17 11:27:47.087537	Iteration 45200 	loss 2.072039842605591
2022-12-17 11:27:49.482231	Iteration 45300 	loss 2.0763051509857178
2022-12-17 11:27:51.873462	Iteration 45400 	loss 2.3097569942474365
2022-12-17 11:27:54.267615	Iteration 45500 	loss 2.337801933288574
2022-12-17 11:27:56.814117	Iteration 45600 	loss 2.918060302734375
2022-12-17 11:27:59.216162	Iteration 45700 	loss 1.9674656391143799
2022-12-17 11:28:01.618262	Iteration 45800 	loss 2.3597002029418945
2022-12-17 11:28:04.290931	Iteration 45900 	loss 1.895325779914856
2022-12-17 11:28:06.750872	Iteration 46000 	loss 2.241044521331787
2022-12-17 11:28:09.481834	Iteration 46100 	loss 2.0878028869628906
2022-12-17 11:28:11.930041	Iteration 46200 	loss 2.1647799015045166
2022-12-17 11:28:14.403957	Iteration 46300 	loss 1.8240717649459839
2022-12-17 11:28:16.819883	Iteration 46400 	loss 1.7790151834487915
2022-12-17 11:28:19.264672	Iteration 46500 	loss 1.918320655822754
2022-12-17 11:28:21.858600	Iteration 46600 	loss 2.0945467948913574
2022-12-17 11:28:24.394417	Iteration 46700 	loss 2.2793240547180176
2022-12-17 11:28:26.829914	Iteration 46800 	loss 2.6383697986602783
2022-12-17 11:28:29.234471	Iteration 46900 	loss 2.0426111221313477
2022-12-17 11:28:31.673262	Iteration 47000 	loss 2.2348549365997314
2022-12-17 11:28:34.982541	Iteration 47100 	loss 2.245755672454834
2022-12-17 11:28:37.371241	Iteration 47200 	loss 2.6157774925231934
2022-12-17 11:28:39.764809	Iteration 47300 	loss 1.3007874488830566
2022-12-17 11:28:42.143037	Iteration 47400 	loss 2.6062512397766113
2022-12-17 11:28:44.526764	Iteration 47500 	loss 2.1712729930877686
2022-12-17 11:28:47.041284	Iteration 47600 	loss 2.763044834136963
2022-12-17 11:28:49.427908	Iteration 47700 	loss 2.5216784477233887
2022-12-17 11:28:51.804562	Iteration 47800 	loss 2.7135186195373535
2022-12-17 11:28:54.199701	Iteration 47900 	loss 1.778334140777588
2022-12-17 11:28:56.581665	Iteration 48000 	loss 2.253493547439575
2022-12-17 11:28:59.114208	Iteration 48100 	loss 2.9215400218963623
2022-12-17 11:29:01.486087	Iteration 48200 	loss 1.9319558143615723
2022-12-17 11:29:03.865308	Iteration 48300 	loss 1.6315661668777466
2022-12-17 11:29:06.276025	Iteration 48400 	loss 2.2306931018829346
2022-12-17 11:29:08.687886	Iteration 48500 	loss 1.6311986446380615
2022-12-17 11:29:11.241963	Iteration 48600 	loss 1.7494841814041138
2022-12-17 11:29:13.622591	Iteration 48700 	loss 2.07806134223938
2022-12-17 11:29:16.009558	Iteration 48800 	loss 2.4049854278564453
2022-12-17 11:29:18.371688	Iteration 48900 	loss 2.2452197074890137
2022-12-17 11:29:20.744148	Iteration 49000 	loss 2.0652074813842773
2022-12-17 11:29:23.247524	Iteration 49100 	loss 1.8620033264160156
2022-12-17 11:29:25.670564	Iteration 49200 	loss 1.9866608381271362
2022-12-17 11:29:28.051320	Iteration 49300 	loss 2.2418668270111084
2022-12-17 11:29:30.441248	Iteration 49400 	loss 2.4177427291870117
2022-12-17 11:29:32.826472	Iteration 49500 	loss 2.1616616249084473
2022-12-17 11:29:35.364448	Iteration 49600 	loss 2.223734140396118
2022-12-17 11:29:37.757927	Iteration 49700 	loss 2.2385828495025635
2022-12-17 11:29:40.135576	Iteration 49800 	loss 2.0652573108673096
2022-12-17 11:29:42.526338	Iteration 49900 	loss 2.475956439971924
2022-12-17 11:29:44.925923	Iteration 50000 	loss 2.2771549224853516
2022-12-17 11:29:47.483408	Iteration 50100 	loss 2.198995590209961
2022-12-17 11:29:49.884081	Iteration 50200 	loss 2.2464723587036133
2022-12-17 11:29:52.283153	Iteration 50300 	loss 2.1515021324157715
2022-12-17 11:29:54.670876	Iteration 50400 	loss 1.859384536743164
2022-12-17 11:29:57.054898	Iteration 50500 	loss 2.6056933403015137
2022-12-17 11:29:59.585053	Iteration 50600 	loss 1.7800750732421875
2022-12-17 11:30:01.986361	Iteration 50700 	loss 2.781435966491699
2022-12-17 11:30:04.364508	Iteration 50800 	loss 1.7799441814422607
2022-12-17 11:30:06.766847	Iteration 50900 	loss 1.9704102277755737
2022-12-17 11:30:09.160791	Iteration 51000 	loss 2.355156898498535
2022-12-17 11:30:11.698523	Iteration 51100 	loss 2.148348569869995
2022-12-17 11:30:14.084642	Iteration 51200 	loss 2.0274760723114014
2022-12-17 11:30:16.514038	Iteration 51300 	loss 3.0202627182006836
2022-12-17 11:30:18.888732	Iteration 51400 	loss 2.2934255599975586
2022-12-17 11:30:21.276544	Iteration 51500 	loss 1.9315024614334106
2022-12-17 11:30:23.802041	Iteration 51600 	loss 2.388514280319214
2022-12-17 11:30:26.192518	Iteration 51700 	loss 1.825226902961731
2022-12-17 11:30:28.609980	Iteration 51800 	loss 2.045377254486084
2022-12-17 11:30:31.003405	Iteration 51900 	loss 2.1874499320983887
2022-12-17 11:30:33.407973	Iteration 52000 	loss 2.7002458572387695
2022-12-17 11:30:35.927709	Iteration 52100 	loss 2.165044069290161
2022-12-17 11:30:38.329193	Iteration 52200 	loss 1.980771541595459
2022-12-17 11:30:40.797326	Iteration 52300 	loss 2.1485300064086914
2022-12-17 11:30:43.322292	Iteration 52400 	loss 2.015141010284424
2022-12-17 11:30:46.035448	Iteration 52500 	loss 1.9153759479522705
2022-12-17 11:30:48.906203	Iteration 52600 	loss 1.711087942123413
2022-12-17 11:30:51.444921	Iteration 52700 	loss 2.527339458465576
2022-12-17 11:30:54.028089	Iteration 52800 	loss 1.6887894868850708
2022-12-17 11:30:56.563048	Iteration 52900 	loss 1.8522677421569824
2022-12-17 11:30:59.014636	Iteration 53000 	loss 2.2651309967041016
2022-12-17 11:31:01.598525	Iteration 53100 	loss 1.624894380569458
2022-12-17 11:31:04.052851	Iteration 53200 	loss 2.30873966217041
2022-12-17 11:31:06.480696	Iteration 53300 	loss 2.2535455226898193
2022-12-17 11:31:08.976256	Iteration 53400 	loss 2.053096294403076
2022-12-17 11:31:11.455456	Iteration 53500 	loss 2.3798940181732178
2022-12-17 11:31:14.014697	Iteration 53600 	loss 2.5781309604644775
2022-12-17 11:31:16.468374	Iteration 53700 	loss 2.503021001815796
2022-12-17 11:31:18.887240	Iteration 53800 	loss 1.9877272844314575
2022-12-17 11:31:21.331112	Iteration 53900 	loss 2.097219944000244
2022-12-17 11:31:23.774631	Iteration 54000 	loss 2.4512808322906494
2022-12-17 11:31:26.410057	Iteration 54100 	loss 2.014949321746826
2022-12-17 11:31:28.852724	Iteration 54200 	loss 2.108381509780884
2022-12-17 11:31:31.289441	Iteration 54300 	loss 2.6284656524658203
2022-12-17 11:31:33.711809	Iteration 54400 	loss 2.1010866165161133
2022-12-17 11:31:36.136722	Iteration 54500 	loss 2.0192437171936035
2022-12-17 11:31:38.830190	Iteration 54600 	loss 2.7758736610412598
2022-12-17 11:31:41.358384	Iteration 54700 	loss 2.256891965866089
2022-12-17 11:31:43.955536	Iteration 54800 	loss 2.4669408798217773
2022-12-17 11:31:46.620339	Iteration 54900 	loss 2.402491331100464
2022-12-17 11:31:49.128465	Iteration 55000 	loss 2.3732595443725586
2022-12-17 11:31:51.855243	Iteration 55100 	loss 2.5977892875671387
2022-12-17 11:31:54.617742	Iteration 55200 	loss 2.6289315223693848
2022-12-17 11:31:57.105847	Iteration 55300 	loss 2.7084693908691406
2022-12-17 11:31:59.525379	Iteration 55400 	loss 1.9306628704071045
2022-12-17 11:32:01.935975	Iteration 55500 	loss 2.139052629470825
2022-12-17 11:32:04.515814	Iteration 55600 	loss 1.732405662536621
2022-12-17 11:32:07.009165	Iteration 55700 	loss 2.0332999229431152
2022-12-17 11:32:09.417194	Iteration 55800 	loss 1.9041948318481445
2022-12-17 11:32:11.824482	Iteration 55900 	loss 2.4491546154022217
2022-12-17 11:32:14.270113	Iteration 56000 	loss 1.8973137140274048
2022-12-17 11:32:16.795538	Iteration 56100 	loss 2.1517081260681152
2022-12-17 11:32:19.186945	Iteration 56200 	loss 2.1577982902526855
2022-12-17 11:32:21.664093	Iteration 56300 	loss 1.6584534645080566
2022-12-17 11:32:24.091855	Iteration 56400 	loss 2.289177179336548
2022-12-17 11:32:26.501197	Iteration 56500 	loss 2.3325588703155518
2022-12-17 11:32:29.039868	Iteration 56600 	loss 2.5653727054595947
2022-12-17 11:32:31.456803	Iteration 56700 	loss 2.6104085445404053
2022-12-17 11:32:33.882993	Iteration 56800 	loss 2.8288204669952393
2022-12-17 11:32:36.273222	Iteration 56900 	loss 2.5673699378967285
2022-12-17 11:32:38.659446	Iteration 57000 	loss 2.5856685638427734
2022-12-17 11:32:41.192042	Iteration 57100 	loss 2.163639783859253
2022-12-17 11:32:43.596838	Iteration 57200 	loss 2.0384669303894043
2022-12-17 11:32:46.006932	Iteration 57300 	loss 2.0022342205047607
2022-12-17 11:32:48.400308	Iteration 57400 	loss 2.2642524242401123
2022-12-17 11:32:50.793489	Iteration 57500 	loss 1.9857420921325684
2022-12-17 11:32:53.322279	Iteration 57600 	loss 2.1595962047576904
2022-12-17 11:32:55.725056	Iteration 57700 	loss 1.8112339973449707
2022-12-17 11:32:58.112052	Iteration 57800 	loss 1.6858986616134644
2022-12-17 11:33:00.561597	Iteration 57900 	loss 2.009398937225342
2022-12-17 11:33:03.075457	Iteration 58000 	loss 2.5250978469848633
2022-12-17 11:33:05.612313	Iteration 58100 	loss 1.952223777770996
2022-12-17 11:33:08.009130	Iteration 58200 	loss 2.4499058723449707
2022-12-17 11:33:10.407301	Iteration 58300 	loss 2.0678513050079346
2022-12-17 11:33:12.808885	Iteration 58400 	loss 1.4182496070861816
2022-12-17 11:33:15.238113	Iteration 58500 	loss 1.680396318435669
2022-12-17 11:33:17.813882	Iteration 58600 	loss 2.1107823848724365
2022-12-17 11:33:20.197751	Iteration 58700 	loss 2.7088687419891357
2022-12-17 11:33:22.606292	Iteration 58800 	loss 2.562934398651123
2022-12-17 11:33:25.002735	Iteration 58900 	loss 1.7761428356170654
2022-12-17 11:33:27.414835	Iteration 59000 	loss 2.1565189361572266
2022-12-17 11:33:29.953286	Iteration 59100 	loss 2.381502389907837
2022-12-17 11:33:32.357259	Iteration 59200 	loss 2.121427536010742
2022-12-17 11:33:34.756870	Iteration 59300 	loss 1.8964769840240479
2022-12-17 11:33:37.159361	Iteration 59400 	loss 2.2137603759765625
2022-12-17 11:33:39.548579	Iteration 59500 	loss 2.7532660961151123
2022-12-17 11:33:42.079003	Iteration 59600 	loss 2.2146553993225098
2022-12-17 11:33:44.476087	Iteration 59700 	loss 1.5472705364227295
2022-12-17 11:33:46.874261	Iteration 59800 	loss 2.656040668487549
2022-12-17 11:33:49.272544	Iteration 59900 	loss 2.2307634353637695
2022-12-17 11:33:51.672290	Iteration 60000 	loss 3.049729347229004
2022-12-17 11:33:54.214099	Iteration 60100 	loss 2.295572519302368
2022-12-17 11:33:56.632053	Iteration 60200 	loss 2.4440500736236572
2022-12-17 11:33:59.024722	Iteration 60300 	loss 2.1297459602355957
2022-12-17 11:34:01.414793	Iteration 60400 	loss 2.1424202919006348
2022-12-17 11:34:03.831284	Iteration 60500 	loss 2.401393413543701
2022-12-17 11:34:06.359695	Iteration 60600 	loss 1.665172815322876
2022-12-17 11:34:08.760079	Iteration 60700 	loss 2.260397434234619
2022-12-17 11:34:11.285935	Iteration 60800 	loss 2.7018609046936035
2022-12-17 11:34:13.949386	Iteration 60900 	loss 2.250868082046509
2022-12-17 11:34:16.431607	Iteration 61000 	loss 1.9864583015441895
2022-12-17 11:34:20.133749	Iteration 61100 	loss 1.7675107717514038
2022-12-17 11:34:22.654952	Iteration 61200 	loss 2.084632158279419
2022-12-17 11:34:25.130870	Iteration 61300 	loss 1.0597554445266724
2022-12-17 11:34:27.577758	Iteration 61400 	loss 2.1375732421875
2022-12-17 11:34:29.997331	Iteration 61500 	loss 2.1540122032165527
2022-12-17 11:34:32.666532	Iteration 61600 	loss 2.115206003189087
2022-12-17 11:34:35.111856	Iteration 61700 	loss 2.2013471126556396
2022-12-17 11:34:37.516596	Iteration 61800 	loss 2.0712902545928955
2022-12-17 11:34:39.958579	Iteration 61900 	loss 1.8480610847473145
2022-12-17 11:34:42.370101	Iteration 62000 	loss 2.546431303024292
2022-12-17 11:34:44.930540	Iteration 62100 	loss 2.4007763862609863
2022-12-17 11:34:47.346928	Iteration 62200 	loss 2.140004873275757
2022-12-17 11:34:49.724268	Iteration 62300 	loss 2.08487868309021
2022-12-17 11:34:52.145865	Iteration 62400 	loss 2.259805679321289
2022-12-17 11:34:54.565352	Iteration 62500 	loss 1.8500933647155762
2022-12-17 11:34:57.268136	Iteration 62600 	loss 2.065450668334961
2022-12-17 11:34:59.804559	Iteration 62700 	loss 2.3576676845550537
2022-12-17 11:35:02.287453	Iteration 62800 	loss 2.6232900619506836
2022-12-17 11:35:04.752606	Iteration 62900 	loss 2.6376094818115234
2022-12-17 11:35:07.247290	Iteration 63000 	loss 2.0756478309631348
2022-12-17 11:35:09.934377	Iteration 63100 	loss 1.6956535577774048
2022-12-17 11:35:12.338233	Iteration 63200 	loss 2.7587978839874268
2022-12-17 11:35:15.002897	Iteration 63300 	loss 2.118776559829712
2022-12-17 11:35:17.455344	Iteration 63400 	loss 1.8799160718917847
2022-12-17 11:35:19.995258	Iteration 63500 	loss 1.951432228088379
2022-12-17 11:35:22.629841	Iteration 63600 	loss 2.35501766204834
2022-12-17 11:35:25.159860	Iteration 63700 	loss 1.894356608390808
2022-12-17 11:35:27.589040	Iteration 63800 	loss 2.242097854614258
2022-12-17 11:35:30.006439	Iteration 63900 	loss 2.2255215644836426
2022-12-17 11:35:32.438052	Iteration 64000 	loss 2.368961811065674
2022-12-17 11:35:35.093147	Iteration 64100 	loss 2.341796636581421
2022-12-17 11:35:37.572259	Iteration 64200 	loss 2.266237258911133
2022-12-17 11:35:40.082088	Iteration 64300 	loss 2.644777774810791
2022-12-17 11:35:42.659190	Iteration 64400 	loss 1.8941216468811035
2022-12-17 11:35:45.128608	Iteration 64500 	loss 2.3805809020996094
2022-12-17 11:35:47.698745	Iteration 64600 	loss 2.0907623767852783
2022-12-17 11:35:50.215500	Iteration 64700 	loss 2.1091160774230957
2022-12-17 11:35:52.646794	Iteration 64800 	loss 1.7616336345672607
2022-12-17 11:35:55.073579	Iteration 64900 	loss 2.808248519897461
2022-12-17 11:35:57.582658	Iteration 65000 	loss 1.6737942695617676
2022-12-17 11:36:00.122074	Iteration 65100 	loss 2.125633716583252
2022-12-17 11:36:02.632023	Iteration 65200 	loss 2.2157397270202637
2022-12-17 11:36:05.124162	Iteration 65300 	loss 2.0781638622283936
2022-12-17 11:36:07.661810	Iteration 65400 	loss 1.8053333759307861
2022-12-17 11:36:10.140115	Iteration 65500 	loss 2.636054277420044
2022-12-17 11:36:12.746857	Iteration 65600 	loss 2.551725387573242
2022-12-17 11:36:15.140829	Iteration 65700 	loss 2.313049793243408
2022-12-17 11:36:17.600357	Iteration 65800 	loss 2.7340338230133057
2022-12-17 11:36:20.073802	Iteration 65900 	loss 1.7449599504470825
2022-12-17 11:36:22.546707	Iteration 66000 	loss 2.4327268600463867
2022-12-17 11:36:25.197241	Iteration 66100 	loss 2.5969245433807373
2022-12-17 11:36:27.611646	Iteration 66200 	loss 2.6478958129882812
2022-12-17 11:36:30.086916	Iteration 66300 	loss 2.1084682941436768
2022-12-17 11:36:32.546836	Iteration 66400 	loss 2.056946277618408
2022-12-17 11:36:34.943448	Iteration 66500 	loss 2.640143871307373
2022-12-17 11:36:37.517486	Iteration 66600 	loss 2.396721363067627
2022-12-17 11:36:40.034302	Iteration 66700 	loss 2.353592872619629
2022-12-17 11:36:42.442560	Iteration 66800 	loss 1.8196977376937866
2022-12-17 11:36:44.983978	Iteration 66900 	loss 2.9361190795898438
2022-12-17 11:36:47.427173	Iteration 67000 	loss 1.594643473625183
2022-12-17 11:36:49.962376	Iteration 67100 	loss 2.877985954284668
2022-12-17 11:36:52.446047	Iteration 67200 	loss 2.03060245513916
2022-12-17 11:36:54.949310	Iteration 67300 	loss 2.0459139347076416
2022-12-17 11:36:57.358367	Iteration 67400 	loss 2.0776681900024414
2022-12-17 11:36:59.736227	Iteration 67500 	loss 2.5657644271850586
2022-12-17 11:37:02.270419	Iteration 67600 	loss 2.168099880218506
2022-12-17 11:37:04.766185	Iteration 67700 	loss 2.830533981323242
2022-12-17 11:37:07.179300	Iteration 67800 	loss 2.1610984802246094
2022-12-17 11:37:09.628946	Iteration 67900 	loss 1.6268881559371948
2022-12-17 11:37:12.081619	Iteration 68000 	loss 2.401097297668457
2022-12-17 11:37:14.713397	Iteration 68100 	loss 2.299762725830078
2022-12-17 11:37:17.112176	Iteration 68200 	loss 2.261763572692871
2022-12-17 11:37:19.495329	Iteration 68300 	loss 2.1592519283294678
2022-12-17 11:37:21.874261	Iteration 68400 	loss 2.254955768585205
2022-12-17 11:37:24.256731	Iteration 68500 	loss 1.7670396566390991
2022-12-17 11:37:26.797339	Iteration 68600 	loss 1.8518184423446655
2022-12-17 11:37:29.174988	Iteration 68700 	loss 1.8782697916030884
2022-12-17 11:37:31.572114	Iteration 68800 	loss 2.150561809539795
2022-12-17 11:37:33.946773	Iteration 68900 	loss 2.158501625061035
2022-12-17 11:37:36.309903	Iteration 69000 	loss 1.8992981910705566
2022-12-17 11:37:38.815882	Iteration 69100 	loss 2.4386954307556152
2022-12-17 11:37:41.178742	Iteration 69200 	loss 2.241572141647339
2022-12-17 11:37:43.559810	Iteration 69300 	loss 2.3560333251953125
2022-12-17 11:37:45.927702	Iteration 69400 	loss 2.0812032222747803
2022-12-17 11:37:48.311446	Iteration 69500 	loss 2.037642478942871
2022-12-17 11:37:50.806441	Iteration 69600 	loss 2.1494410037994385
2022-12-17 11:37:53.200707	Iteration 69700 	loss 2.3334662914276123
2022-12-17 11:37:55.606972	Iteration 69800 	loss 1.7031381130218506
2022-12-17 11:37:58.030196	Iteration 69900 	loss 2.0148508548736572
2022-12-17 11:38:00.390156	Iteration 70000 	loss 1.9016417264938354
2022-12-17 11:38:02.898235	Iteration 70100 	loss 2.2649168968200684
2022-12-17 11:38:05.259353	Iteration 70200 	loss 2.3337645530700684
2022-12-17 11:38:07.662039	Iteration 70300 	loss 2.0681750774383545
2022-12-17 11:38:10.023392	Iteration 70400 	loss 2.331610918045044
2022-12-17 11:38:12.380449	Iteration 70500 	loss 1.3342312574386597
2022-12-17 11:38:14.888346	Iteration 70600 	loss 1.7909162044525146
2022-12-17 11:38:17.255908	Iteration 70700 	loss 2.0533287525177
2022-12-17 11:38:19.636474	Iteration 70800 	loss 2.2487990856170654
2022-12-17 11:38:22.010616	Iteration 70900 	loss 2.7068371772766113
2022-12-17 11:38:24.389799	Iteration 71000 	loss 1.7244092226028442
2022-12-17 11:38:26.894026	Iteration 71100 	loss 2.5994648933410645
2022-12-17 11:38:29.264138	Iteration 71200 	loss 2.7503650188446045
2022-12-17 11:38:31.619887	Iteration 71300 	loss 1.981816053390503
2022-12-17 11:38:33.979581	Iteration 71400 	loss 2.00056791305542
2022-12-17 11:38:36.345233	Iteration 71500 	loss 2.4345896244049072
2022-12-17 11:38:38.852840	Iteration 71600 	loss 2.688539981842041
2022-12-17 11:38:41.230322	Iteration 71700 	loss 2.1333611011505127
2022-12-17 11:38:43.751166	Iteration 71800 	loss 2.4402847290039062
2022-12-17 11:38:46.226535	Iteration 71900 	loss 2.3762640953063965
2022-12-17 11:38:48.737859	Iteration 72000 	loss 2.361041307449341
2022-12-17 11:38:51.264811	Iteration 72100 	loss 1.7363983392715454
2022-12-17 11:38:53.740194	Iteration 72200 	loss 1.7003315687179565
2022-12-17 11:38:56.196545	Iteration 72300 	loss 2.4437003135681152
2022-12-17 11:38:58.678228	Iteration 72400 	loss 2.088926076889038
2022-12-17 11:39:01.182268	Iteration 72500 	loss 2.144287586212158
2022-12-17 11:39:03.719314	Iteration 72600 	loss 1.9378163814544678
2022-12-17 11:39:06.099071	Iteration 72700 	loss 2.3914742469787598
2022-12-17 11:39:08.473609	Iteration 72800 	loss 2.8597054481506348
2022-12-17 11:39:10.850460	Iteration 72900 	loss 2.0829973220825195
2022-12-17 11:39:13.203065	Iteration 73000 	loss 2.5340874195098877
2022-12-17 11:39:15.715715	Iteration 73100 	loss 2.1858439445495605
2022-12-17 11:39:18.085490	Iteration 73200 	loss 2.219486951828003
2022-12-17 11:39:20.462982	Iteration 73300 	loss 1.9357792139053345
2022-12-17 11:39:22.837230	Iteration 73400 	loss 2.642533302307129
2022-12-17 11:39:25.212328	Iteration 73500 	loss 2.645068883895874
2022-12-17 11:39:27.740543	Iteration 73600 	loss 1.995002031326294
2022-12-17 11:39:30.134522	Iteration 73700 	loss 2.1772537231445312
2022-12-17 11:39:32.518044	Iteration 73800 	loss 2.0087194442749023
2022-12-17 11:39:34.885027	Iteration 73900 	loss 1.8548181056976318
2022-12-17 11:39:37.262154	Iteration 74000 	loss 2.3135766983032227
2022-12-17 11:39:39.771654	Iteration 74100 	loss 1.8096661567687988
2022-12-17 11:39:42.154681	Iteration 74200 	loss 2.065096855163574
2022-12-17 11:39:44.535221	Iteration 74300 	loss 1.589193344116211
2022-12-17 11:39:46.902009	Iteration 74400 	loss 2.1693930625915527
2022-12-17 11:39:49.262074	Iteration 74500 	loss 2.4467175006866455
2022-12-17 11:39:51.759812	Iteration 74600 	loss 2.338759422302246
2022-12-17 11:39:54.154114	Iteration 74700 	loss 2.2805960178375244
2022-12-17 11:39:56.561412	Iteration 74800 	loss 2.3374223709106445
2022-12-17 11:39:58.966321	Iteration 74900 	loss 2.5483036041259766
2022-12-17 11:40:01.453382	Iteration 75000 	loss 2.63029146194458
2022-12-17 11:40:04.050938	Iteration 75100 	loss 1.923905611038208
2022-12-17 11:40:06.464441	Iteration 75200 	loss 2.1119844913482666
2022-12-17 11:40:09.057301	Iteration 75300 	loss 2.521313190460205
2022-12-17 11:40:11.463892	Iteration 75400 	loss 2.235401153564453
2022-12-17 11:40:13.845785	Iteration 75500 	loss 1.9394465684890747
2022-12-17 11:40:16.462052	Iteration 75600 	loss 2.2481234073638916
2022-12-17 11:40:18.872510	Iteration 75700 	loss 2.424574136734009
2022-12-17 11:40:21.390386	Iteration 75800 	loss 2.628277063369751
2022-12-17 11:40:23.854571	Iteration 75900 	loss 2.0173637866973877
2022-12-17 11:40:26.222668	Iteration 76000 	loss 2.3313302993774414
2022-12-17 11:40:28.820949	Iteration 76100 	loss 2.561763286590576
2022-12-17 11:40:31.262609	Iteration 76200 	loss 2.725740432739258
2022-12-17 11:40:33.641357	Iteration 76300 	loss 1.8429192304611206
2022-12-17 11:40:36.112593	Iteration 76400 	loss 2.763918161392212
2022-12-17 11:40:38.465714	Iteration 76500 	loss 2.1506943702697754
2022-12-17 11:40:40.960284	Iteration 76600 	loss 2.314220428466797
2022-12-17 11:40:43.326569	Iteration 76700 	loss 1.519957184791565
2022-12-17 11:40:45.690124	Iteration 76800 	loss 2.317777633666992
2022-12-17 11:40:48.044496	Iteration 76900 	loss 2.476372241973877
2022-12-17 11:40:50.578733	Iteration 77000 	loss 2.5916991233825684
2022-12-17 11:40:53.083565	Iteration 77100 	loss 2.1476314067840576
2022-12-17 11:40:55.474309	Iteration 77200 	loss 2.6769134998321533
2022-12-17 11:40:57.881260	Iteration 77300 	loss 2.1027419567108154
2022-12-17 11:41:00.375361	Iteration 77400 	loss 2.515620231628418
2022-12-17 11:41:02.928229	Iteration 77500 	loss 1.750963568687439
2022-12-17 11:41:06.587524	Iteration 77600 	loss 2.0569007396698
2022-12-17 11:41:08.945723	Iteration 77700 	loss 1.8537095785140991
2022-12-17 11:41:11.312298	Iteration 77800 	loss 2.5828092098236084
2022-12-17 11:41:13.681823	Iteration 77900 	loss 2.661280870437622
2022-12-17 11:41:16.036151	Iteration 78000 	loss 1.8500357866287231
2022-12-17 11:41:18.525911	Iteration 78100 	loss 1.8837668895721436
2022-12-17 11:41:20.927755	Iteration 78200 	loss 2.1920042037963867
2022-12-17 11:41:23.294813	Iteration 78300 	loss 2.5319137573242188
2022-12-17 11:41:25.666011	Iteration 78400 	loss 2.0897579193115234
2022-12-17 11:41:28.040648	Iteration 78500 	loss 2.8121330738067627
2022-12-17 11:41:30.598887	Iteration 78600 	loss 1.966074824333191
2022-12-17 11:41:32.970652	Iteration 78700 	loss 2.6387763023376465
2022-12-17 11:41:35.356615	Iteration 78800 	loss 2.300464153289795
2022-12-17 11:41:37.730890	Iteration 78900 	loss 2.64788556098938
2022-12-17 11:41:40.085532	Iteration 79000 	loss 2.281827211380005
2022-12-17 11:41:42.574313	Iteration 79100 	loss 2.1764440536499023
2022-12-17 11:41:44.926213	Iteration 79200 	loss 2.2910256385803223
2022-12-17 11:41:47.283987	Iteration 79300 	loss 2.1488356590270996
2022-12-17 11:41:49.630563	Iteration 79400 	loss 2.1137428283691406
2022-12-17 11:41:51.980871	Iteration 79500 	loss 2.249602794647217
2022-12-17 11:41:54.458862	Iteration 79600 	loss 2.84977650642395
2022-12-17 11:41:56.852080	Iteration 79700 	loss 2.04008150100708
2022-12-17 11:41:59.245096	Iteration 79800 	loss 2.112034559249878
2022-12-17 11:42:01.638620	Iteration 79900 	loss 2.0352389812469482
2022-12-17 11:42:04.026906	Iteration 80000 	loss 2.0329766273498535
2022-12-17 11:42:06.514399	Iteration 80100 	loss 1.6510722637176514
2022-12-17 11:42:08.877654	Iteration 80200 	loss 1.7975802421569824
2022-12-17 11:42:11.216166	Iteration 80300 	loss 2.263144016265869
2022-12-17 11:42:13.567073	Iteration 80400 	loss 2.1147890090942383
2022-12-17 11:42:15.921381	Iteration 80500 	loss 2.209995985031128
2022-12-17 11:42:18.442913	Iteration 80600 	loss 1.7228572368621826
2022-12-17 11:42:20.797017	Iteration 80700 	loss 1.8199316263198853
2022-12-17 11:42:23.173688	Iteration 80800 	loss 2.4290812015533447
2022-12-17 11:42:25.517218	Iteration 80900 	loss 2.723884105682373
2022-12-17 11:42:27.872342	Iteration 81000 	loss 2.61364483833313
2022-12-17 11:42:30.354752	Iteration 81100 	loss 2.20658278465271
2022-12-17 11:42:32.717906	Iteration 81200 	loss 2.2778139114379883
2022-12-17 11:42:35.080216	Iteration 81300 	loss 2.5304970741271973
2022-12-17 11:42:37.442975	Iteration 81400 	loss 2.473184108734131
2022-12-17 11:42:39.798900	Iteration 81500 	loss 1.8726439476013184
2022-12-17 11:42:42.288220	Iteration 81600 	loss 2.8295607566833496
2022-12-17 11:42:44.637684	Iteration 81700 	loss 2.602947473526001
2022-12-17 11:42:46.979535	Iteration 81800 	loss 1.9919264316558838
2022-12-17 11:42:49.326860	Iteration 81900 	loss 2.108727216720581
2022-12-17 11:42:51.677094	Iteration 82000 	loss 2.1120615005493164
2022-12-17 11:42:54.201008	Iteration 82100 	loss 2.4997880458831787
2022-12-17 11:42:56.605012	Iteration 82200 	loss 2.0666706562042236
2022-12-17 11:42:58.992495	Iteration 82300 	loss 1.8324674367904663
2022-12-17 11:43:01.555839	Iteration 82400 	loss 2.3431055545806885
2022-12-17 11:43:03.936669	Iteration 82500 	loss 2.01674485206604
2022-12-17 11:43:06.556131	Iteration 82600 	loss 2.112241506576538
2022-12-17 11:43:09.188143	Iteration 82700 	loss 1.8250025510787964
2022-12-17 11:43:11.709007	Iteration 82800 	loss 2.03547739982605
2022-12-17 11:43:14.373888	Iteration 82900 	loss 2.245485305786133
2022-12-17 11:43:16.816111	Iteration 83000 	loss 1.8766653537750244
2022-12-17 11:43:19.347905	Iteration 83100 	loss 2.254434108734131
2022-12-17 11:43:21.865096	Iteration 83200 	loss 2.127288818359375
2022-12-17 11:43:24.278674	Iteration 83300 	loss 2.75653076171875
2022-12-17 11:43:26.806532	Iteration 83400 	loss 2.3245294094085693
2022-12-17 11:43:29.226908	Iteration 83500 	loss 1.9832409620285034
2022-12-17 11:43:31.806935	Iteration 83600 	loss 2.137101888656616
2022-12-17 11:43:34.264156	Iteration 83700 	loss 1.4960014820098877
2022-12-17 11:43:36.684698	Iteration 83800 	loss 2.7849180698394775
2022-12-17 11:43:39.153966	Iteration 83900 	loss 2.67695951461792
2022-12-17 11:43:41.687612	Iteration 84000 	loss 2.7669358253479004
2022-12-17 11:43:44.329168	Iteration 84100 	loss 2.2822580337524414
2022-12-17 11:43:46.767385	Iteration 84200 	loss 1.892654299736023
2022-12-17 11:43:49.156701	Iteration 84300 	loss 2.3892226219177246
2022-12-17 11:43:51.584277	Iteration 84400 	loss 2.0021402835845947
2022-12-17 11:43:54.062997	Iteration 84500 	loss 2.528533697128296
2022-12-17 11:43:56.638557	Iteration 84600 	loss 1.8632749319076538
2022-12-17 11:43:59.067288	Iteration 84700 	loss 1.96151864528656
2022-12-17 11:44:01.611804	Iteration 84800 	loss 2.267003059387207
2022-12-17 11:44:04.062682	Iteration 84900 	loss 2.6246376037597656
2022-12-17 11:44:06.462894	Iteration 85000 	loss 2.0941433906555176
2022-12-17 11:44:08.991004	Iteration 85100 	loss 1.9655563831329346
2022-12-17 11:44:11.397243	Iteration 85200 	loss 1.9837937355041504
2022-12-17 11:44:13.826625	Iteration 85300 	loss 2.237536907196045
2022-12-17 11:44:16.259435	Iteration 85400 	loss 2.5117075443267822
2022-12-17 11:44:18.759415	Iteration 85500 	loss 2.024345874786377
2022-12-17 11:44:21.342382	Iteration 85600 	loss 1.7627079486846924
2022-12-17 11:44:23.849657	Iteration 85700 	loss 2.916043996810913
2022-12-17 11:44:26.269397	Iteration 85800 	loss 2.416049003601074
2022-12-17 11:44:28.664750	Iteration 85900 	loss 2.6225473880767822
2022-12-17 11:44:31.038177	Iteration 86000 	loss 2.3361058235168457
2022-12-17 11:44:33.561640	Iteration 86100 	loss 2.2733700275421143
2022-12-17 11:44:35.940570	Iteration 86200 	loss 2.1715786457061768
2022-12-17 11:44:38.328517	Iteration 86300 	loss 2.025693416595459
2022-12-17 11:44:40.779351	Iteration 86400 	loss 2.37239408493042
2022-12-17 11:44:43.243995	Iteration 86500 	loss 1.7521824836730957
2022-12-17 11:44:45.791514	Iteration 86600 	loss 1.7422866821289062
2022-12-17 11:44:48.184184	Iteration 86700 	loss 2.6994071006774902
2022-12-17 11:44:50.672791	Iteration 86800 	loss 2.0485031604766846
2022-12-17 11:44:53.069612	Iteration 86900 	loss 2.4324464797973633
2022-12-17 11:44:55.480793	Iteration 87000 	loss 1.843769907951355
2022-12-17 11:44:58.024020	Iteration 87100 	loss 2.048673391342163
2022-12-17 11:45:00.455355	Iteration 87200 	loss 2.432734489440918
2022-12-17 11:45:02.869118	Iteration 87300 	loss 2.2695348262786865
2022-12-17 11:45:05.284880	Iteration 87400 	loss 2.653468608856201
2022-12-17 11:45:07.678098	Iteration 87500 	loss 1.8326107263565063
2022-12-17 11:45:10.207818	Iteration 87600 	loss 1.8563898801803589
2022-12-17 11:45:12.681001	Iteration 87700 	loss 1.8507920503616333
2022-12-17 11:45:15.082598	Iteration 87800 	loss 2.409636974334717
2022-12-17 11:45:17.474579	Iteration 87900 	loss 2.4568448066711426
2022-12-17 11:45:19.857046	Iteration 88000 	loss 2.242896318435669
2022-12-17 11:45:22.414949	Iteration 88100 	loss 1.8912289142608643
2022-12-17 11:45:24.801910	Iteration 88200 	loss 2.3182671070098877
2022-12-17 11:45:27.195355	Iteration 88300 	loss 1.7785732746124268
2022-12-17 11:45:29.590965	Iteration 88400 	loss 1.9666424989700317
2022-12-17 11:45:31.986867	Iteration 88500 	loss 2.2756834030151367
2022-12-17 11:45:34.530474	Iteration 88600 	loss 2.5430502891540527
2022-12-17 11:45:36.938874	Iteration 88700 	loss 1.7300245761871338
2022-12-17 11:45:39.330756	Iteration 88800 	loss 2.9148941040039062
2022-12-17 11:45:41.724754	Iteration 88900 	loss 2.247140645980835
2022-12-17 11:45:44.122588	Iteration 89000 	loss 1.5159282684326172
2022-12-17 11:45:46.691924	Iteration 89100 	loss 2.495039463043213
2022-12-17 11:45:49.083207	Iteration 89200 	loss 1.8191707134246826
2022-12-17 11:45:51.448973	Iteration 89300 	loss 3.1034107208251953
2022-12-17 11:45:53.821827	Iteration 89400 	loss 2.216383457183838
2022-12-17 11:45:56.213250	Iteration 89500 	loss 2.061174154281616
2022-12-17 11:45:58.741701	Iteration 89600 	loss 2.1761772632598877
2022-12-17 11:46:01.127588	Iteration 89700 	loss 1.6115875244140625
2022-12-17 11:46:03.532284	Iteration 89800 	loss 1.6444755792617798
2022-12-17 11:46:05.930018	Iteration 89900 	loss 2.3535423278808594
2022-12-17 11:46:08.315577	Iteration 90000 	loss 2.5213005542755127
2022-12-17 11:46:10.864018	Iteration 90100 	loss 1.646549105644226
2022-12-17 11:46:13.246232	Iteration 90200 	loss 2.402994155883789
2022-12-17 11:46:15.609653	Iteration 90300 	loss 2.5037195682525635
2022-12-17 11:46:17.989939	Iteration 90400 	loss 1.8648481369018555
2022-12-17 11:46:20.377867	Iteration 90500 	loss 2.6624083518981934
2022-12-17 11:46:22.903577	Iteration 90600 	loss 2.541337490081787
2022-12-17 11:46:25.287414	Iteration 90700 	loss 2.4165916442871094
2022-12-17 11:46:27.691220	Iteration 90800 	loss 2.5726799964904785
2022-12-17 11:46:30.047127	Iteration 90900 	loss 2.1713719367980957
2022-12-17 11:46:32.424288	Iteration 91000 	loss 2.363530397415161
2022-12-17 11:46:34.951757	Iteration 91100 	loss 2.177739143371582
2022-12-17 11:46:37.349436	Iteration 91200 	loss 2.1875922679901123
2022-12-17 11:46:39.755546	Iteration 91300 	loss 2.6662964820861816
2022-12-17 11:46:42.151326	Iteration 91400 	loss 2.004749298095703
2022-12-17 11:46:44.554870	Iteration 91500 	loss 2.551919937133789
2022-12-17 11:46:47.096256	Iteration 91600 	loss 1.5992662906646729
2022-12-17 11:46:49.473072	Iteration 91700 	loss 2.29264235496521
2022-12-17 11:46:51.853957	Iteration 91800 	loss 2.1454131603240967
2022-12-17 11:46:54.231672	Iteration 91900 	loss 2.0621678829193115
2022-12-17 11:46:56.607943	Iteration 92000 	loss 2.421961545944214
2022-12-17 11:46:59.141198	Iteration 92100 	loss 2.394667148590088
2022-12-17 11:47:01.536015	Iteration 92200 	loss 1.8638907670974731
2022-12-17 11:47:03.928302	Iteration 92300 	loss 1.6542620658874512
2022-12-17 11:47:06.307607	Iteration 92400 	loss 2.469693422317505
2022-12-17 11:47:08.686499	Iteration 92500 	loss 2.393537998199463
2022-12-17 11:47:11.206869	Iteration 92600 	loss 2.1220052242279053
2022-12-17 11:47:13.587609	Iteration 92700 	loss 2.1163930892944336
2022-12-17 11:47:15.977718	Iteration 92800 	loss 2.3790996074676514
2022-12-17 11:47:18.394297	Iteration 92900 	loss 1.471174716949463
2022-12-17 11:47:20.810245	Iteration 93000 	loss 2.040968418121338
2022-12-17 11:47:23.347549	Iteration 93100 	loss 2.1490368843078613
2022-12-17 11:47:25.755470	Iteration 93200 	loss 2.2533295154571533
2022-12-17 11:47:28.150293	Iteration 93300 	loss 2.0295279026031494
2022-12-17 11:47:30.577765	Iteration 93400 	loss 2.1914730072021484
2022-12-17 11:47:33.046031	Iteration 93500 	loss 1.7861096858978271
2022-12-17 11:47:35.710784	Iteration 93600 	loss 2.3424086570739746
2022-12-17 11:47:38.113308	Iteration 93700 	loss 2.528468370437622
2022-12-17 11:47:40.530630	Iteration 93800 	loss 2.5195400714874268
2022-12-17 11:47:42.938089	Iteration 93900 	loss 2.604336977005005
2022-12-17 11:47:45.317139	Iteration 94000 	loss 2.4828617572784424
2022-12-17 11:47:47.902571	Iteration 94100 	loss 2.5388386249542236
2022-12-17 11:47:50.287446	Iteration 94200 	loss 1.8636527061462402
2022-12-17 11:47:52.676842	Iteration 94300 	loss 1.6537163257598877
2022-12-17 11:47:55.072701	Iteration 94400 	loss 1.9498718976974487
2022-12-17 11:47:57.467414	Iteration 94500 	loss 1.9760680198669434
2022-12-17 11:47:59.981666	Iteration 94600 	loss 2.223778247833252
2022-12-17 11:48:02.367793	Iteration 94700 	loss 2.1162567138671875
2022-12-17 11:48:04.742381	Iteration 94800 	loss 1.8717856407165527
2022-12-17 11:48:07.130849	Iteration 94900 	loss 1.969579815864563
2022-12-17 11:48:09.494858	Iteration 95000 	loss 2.117598533630371
2022-12-17 11:48:12.010078	Iteration 95100 	loss 2.4067764282226562
2022-12-17 11:48:14.386226	Iteration 95200 	loss 1.9845380783081055
2022-12-17 11:48:16.767985	Iteration 95300 	loss 2.103496551513672
2022-12-17 11:48:19.166101	Iteration 95400 	loss 2.096580743789673
2022-12-17 11:48:21.557106	Iteration 95500 	loss 1.9646108150482178
2022-12-17 11:48:24.071727	Iteration 95600 	loss 1.9031174182891846
2022-12-17 11:48:26.450655	Iteration 95700 	loss 2.210972547531128
2022-12-17 11:48:28.844459	Iteration 95800 	loss 2.4207000732421875
2022-12-17 11:48:31.201631	Iteration 95900 	loss 1.658719778060913
2022-12-17 11:48:33.600324	Iteration 96000 	loss 2.010774850845337
2022-12-17 11:48:36.133244	Iteration 96100 	loss 1.7546970844268799
2022-12-17 11:48:38.518397	Iteration 96200 	loss 1.9948837757110596
2022-12-17 11:48:40.905287	Iteration 96300 	loss 2.4637742042541504
2022-12-17 11:48:43.283822	Iteration 96400 	loss 1.7246392965316772
2022-12-17 11:48:45.695539	Iteration 96500 	loss 2.85205340385437
2022-12-17 11:48:48.236546	Iteration 96600 	loss 2.2319278717041016
2022-12-17 11:48:50.634381	Iteration 96700 	loss 2.5654478073120117
2022-12-17 11:48:53.011397	Iteration 96800 	loss 1.7638647556304932
2022-12-17 11:48:55.397628	Iteration 96900 	loss 2.5298051834106445
2022-12-17 11:48:57.792859	Iteration 97000 	loss 1.5940463542938232
2022-12-17 11:49:00.298720	Iteration 97100 	loss 2.239427089691162
2022-12-17 11:49:02.674808	Iteration 97200 	loss 2.1067147254943848
2022-12-17 11:49:05.047615	Iteration 97300 	loss 2.4114229679107666
2022-12-17 11:49:07.395083	Iteration 97400 	loss 2.0416789054870605
2022-12-17 11:49:09.772285	Iteration 97500 	loss 2.520862102508545
2022-12-17 11:49:12.296869	Iteration 97600 	loss 2.264409303665161
2022-12-17 11:49:14.695876	Iteration 97700 	loss 1.6786118745803833
2022-12-17 11:49:17.112664	Iteration 97800 	loss 2.333721160888672
2022-12-17 11:49:19.484593	Iteration 97900 	loss 2.436572790145874
2022-12-17 11:49:21.828338	Iteration 98000 	loss 2.2726471424102783
2022-12-17 11:49:24.365048	Iteration 98100 	loss 2.749138355255127
2022-12-17 11:49:26.741000	Iteration 98200 	loss 1.9883687496185303
2022-12-17 11:49:29.111357	Iteration 98300 	loss 2.3160877227783203
2022-12-17 11:49:31.466799	Iteration 98400 	loss 1.964387059211731
2022-12-17 11:49:33.844285	Iteration 98500 	loss 2.264012098312378
2022-12-17 11:49:37.861834	Iteration 98600 	loss 2.3462677001953125
2022-12-17 11:49:40.218047	Iteration 98700 	loss 1.7438557147979736
2022-12-17 11:49:42.585443	Iteration 98800 	loss 2.0163793563842773
2022-12-17 11:49:44.962572	Iteration 98900 	loss 1.9686410427093506
2022-12-17 11:49:47.338591	Iteration 99000 	loss 2.6259543895721436
2022-12-17 11:49:49.848314	Iteration 99100 	loss 2.6581621170043945
2022-12-17 11:49:52.238086	Iteration 99200 	loss 2.014336109161377
2022-12-17 11:49:54.634559	Iteration 99300 	loss 2.342104911804199
2022-12-17 11:49:57.072523	Iteration 99400 	loss 1.518506407737732
2022-12-17 11:49:59.445823	Iteration 99500 	loss 1.6289228200912476
2022-12-17 11:50:01.954271	Iteration 99600 	loss 2.402038335800171
2022-12-17 11:50:04.309298	Iteration 99700 	loss 2.184877872467041
2022-12-17 11:50:06.667265	Iteration 99800 	loss 2.5987861156463623
2022-12-17 11:50:09.008606	Iteration 99900 	loss 2.600560188293457
2022-12-17 11:50:11.357884	Iteration 100000 	loss 3.252898931503296
2022-12-17 11:50:13.838612	Iteration 100100 	loss 1.7277954816818237
2022-12-17 11:50:16.190694	Iteration 100200 	loss 2.418841600418091
2022-12-17 11:50:18.554935	Iteration 100300 	loss 2.4013915061950684
2022-12-17 11:50:20.918187	Iteration 100400 	loss 1.9230746030807495
2022-12-17 11:50:23.287888	Iteration 100500 	loss 2.3808114528656006
2022-12-17 11:50:25.779603	Iteration 100600 	loss 2.2144174575805664
2022-12-17 11:50:28.140098	Iteration 100700 	loss 2.210237741470337
2022-12-17 11:50:30.504446	Iteration 100800 	loss 2.1696534156799316
2022-12-17 11:50:32.879398	Iteration 100900 	loss 2.4530842304229736
2022-12-17 11:50:35.257993	Iteration 101000 	loss 2.5403640270233154
2022-12-17 11:50:37.775140	Iteration 101100 	loss 2.25736403465271
2022-12-17 11:50:40.121965	Iteration 101200 	loss 1.9791876077651978
2022-12-17 11:50:42.466280	Iteration 101300 	loss 1.925626277923584
2022-12-17 11:50:44.815874	Iteration 101400 	loss 2.1095995903015137
2022-12-17 11:50:47.191976	Iteration 101500 	loss 2.0774688720703125
2022-12-17 11:50:49.680142	Iteration 101600 	loss 2.304792881011963
2022-12-17 11:50:52.047966	Iteration 101700 	loss 1.8141510486602783
2022-12-17 11:50:54.444804	Iteration 101800 	loss 1.8545658588409424
2022-12-17 11:50:56.855823	Iteration 101900 	loss 1.885266661643982
2022-12-17 11:50:59.206769	Iteration 102000 	loss 2.36645245552063
2022-12-17 11:51:01.711118	Iteration 102100 	loss 2.5798444747924805
2022-12-17 11:51:04.079920	Iteration 102200 	loss 2.6184825897216797
2022-12-17 11:51:06.424980	Iteration 102300 	loss 2.3631515502929688
2022-12-17 11:51:08.779181	Iteration 102400 	loss 1.5417271852493286
2022-12-17 11:51:11.123954	Iteration 102500 	loss 2.3802552223205566
2022-12-17 11:51:13.599440	Iteration 102600 	loss 1.8542850017547607
2022-12-17 11:51:15.936398	Iteration 102700 	loss 1.6447069644927979
2022-12-17 11:51:18.280253	Iteration 102800 	loss 2.474458694458008
2022-12-17 11:51:20.642478	Iteration 102900 	loss 2.435765504837036
2022-12-17 11:51:23.015097	Iteration 103000 	loss 1.7228492498397827
2022-12-17 11:51:25.502695	Iteration 103100 	loss 2.0076904296875
2022-12-17 11:51:27.908843	Iteration 103200 	loss 2.1345653533935547
2022-12-17 11:51:30.273004	Iteration 103300 	loss 2.4079761505126953
2022-12-17 11:51:32.751420	Iteration 103400 	loss 2.366215467453003
2022-12-17 11:51:35.320141	Iteration 103500 	loss 1.9022855758666992
2022-12-17 11:51:37.840608	Iteration 103600 	loss 2.305666446685791
2022-12-17 11:51:40.209730	Iteration 103700 	loss 1.669226050376892
2022-12-17 11:51:42.567563	Iteration 103800 	loss 1.8966197967529297
2022-12-17 11:51:45.124977	Iteration 103900 	loss 1.7603142261505127
2022-12-17 11:51:47.663469	Iteration 104000 	loss 1.7094707489013672
2022-12-17 11:51:50.171226	Iteration 104100 	loss 1.9695795774459839
2022-12-17 11:51:52.530506	Iteration 104200 	loss 1.9669674634933472
2022-12-17 11:51:54.874658	Iteration 104300 	loss 1.9547611474990845
2022-12-17 11:51:57.215857	Iteration 104400 	loss 1.841152310371399
2022-12-17 11:51:59.577062	Iteration 104500 	loss 2.1355607509613037
2022-12-17 11:52:02.094837	Iteration 104600 	loss 2.1659319400787354
2022-12-17 11:52:04.484634	Iteration 104700 	loss 1.8379161357879639
2022-12-17 11:52:06.907881	Iteration 104800 	loss 2.1699068546295166
2022-12-17 11:52:09.330086	Iteration 104900 	loss 1.991914987564087
2022-12-17 11:52:11.704337	Iteration 105000 	loss 2.444309711456299
2022-12-17 11:52:14.220391	Iteration 105100 	loss 1.9476895332336426
2022-12-17 11:52:16.663710	Iteration 105200 	loss 2.722407341003418
2022-12-17 11:52:19.024298	Iteration 105300 	loss 2.242137908935547
2022-12-17 11:52:21.400572	Iteration 105400 	loss 2.169858455657959
2022-12-17 11:52:23.823943	Iteration 105500 	loss 2.612372398376465
2022-12-17 11:52:26.430830	Iteration 105600 	loss 2.358901262283325
2022-12-17 11:52:28.800795	Iteration 105700 	loss 2.748044729232788
2022-12-17 11:52:31.202271	Iteration 105800 	loss 1.420660138130188
2022-12-17 11:52:33.545024	Iteration 105900 	loss 1.681995153427124
2022-12-17 11:52:35.898492	Iteration 106000 	loss 2.1738381385803223
2022-12-17 11:52:38.376056	Iteration 106100 	loss 2.3072848320007324
2022-12-17 11:52:40.732820	Iteration 106200 	loss 1.6602060794830322
2022-12-17 11:52:43.149071	Iteration 106300 	loss 2.3369624614715576
2022-12-17 11:52:45.548905	Iteration 106400 	loss 2.315232753753662
2022-12-17 11:52:47.905795	Iteration 106500 	loss 1.8714094161987305
2022-12-17 11:52:50.384099	Iteration 106600 	loss 2.5168416500091553
2022-12-17 11:52:52.711944	Iteration 106700 	loss 1.9234445095062256
2022-12-17 11:52:55.075139	Iteration 106800 	loss 1.7858186960220337
2022-12-17 11:52:57.425493	Iteration 106900 	loss 2.474813461303711
2022-12-17 11:52:59.780062	Iteration 107000 	loss 2.614262819290161
2022-12-17 11:53:02.316037	Iteration 107100 	loss 2.198883056640625
2022-12-17 11:53:04.711924	Iteration 107200 	loss 2.6611344814300537
2022-12-17 11:53:07.069124	Iteration 107300 	loss 2.407693386077881
2022-12-17 11:53:09.447702	Iteration 107400 	loss 2.0103442668914795
2022-12-17 11:53:11.819685	Iteration 107500 	loss 2.6107819080352783
2022-12-17 11:53:14.304884	Iteration 107600 	loss 1.8167047500610352
2022-12-17 11:53:16.667976	Iteration 107700 	loss 2.03128719329834
2022-12-17 11:53:19.016258	Iteration 107800 	loss 1.6546711921691895
2022-12-17 11:53:21.362586	Iteration 107900 	loss 2.771191358566284
2022-12-17 11:53:23.691128	Iteration 108000 	loss 1.8378924131393433
2022-12-17 11:53:26.200306	Iteration 108100 	loss 2.6117398738861084
2022-12-17 11:53:28.546824	Iteration 108200 	loss 1.5756123065948486
2022-12-17 11:53:30.881964	Iteration 108300 	loss 2.658296585083008
2022-12-17 11:53:33.232732	Iteration 108400 	loss 1.8950892686843872
2022-12-17 11:53:35.552758	Iteration 108500 	loss 1.8298720121383667
2022-12-17 11:53:38.026066	Iteration 108600 	loss 2.024060010910034
2022-12-17 11:53:40.371094	Iteration 108700 	loss 1.5380862951278687
2022-12-17 11:53:42.725435	Iteration 108800 	loss 1.705188512802124
2022-12-17 11:53:45.064197	Iteration 108900 	loss 2.587614059448242
2022-12-17 11:53:47.416197	Iteration 109000 	loss 1.715057134628296
2022-12-17 11:53:49.891844	Iteration 109100 	loss 2.050994396209717
2022-12-17 11:53:52.239183	Iteration 109200 	loss 2.253288507461548
2022-12-17 11:53:54.562733	Iteration 109300 	loss 1.9412521123886108
2022-12-17 11:53:56.925107	Iteration 109400 	loss 2.4535717964172363
2022-12-17 11:53:59.269957	Iteration 109500 	loss 2.102727174758911
2022-12-17 11:54:01.751743	Iteration 109600 	loss 2.136746883392334
2022-12-17 11:54:04.102249	Iteration 109700 	loss 2.7478365898132324
2022-12-17 11:54:06.468913	Iteration 109800 	loss 2.007650375366211
2022-12-17 11:54:08.809925	Iteration 109900 	loss 2.19291090965271
2022-12-17 11:54:11.151657	Iteration 110000 	loss 2.7275500297546387
2022-12-17 11:54:13.638180	Iteration 110100 	loss 2.768028736114502
2022-12-17 11:54:15.981951	Iteration 110200 	loss 1.8053258657455444
2022-12-17 11:54:18.337876	Iteration 110300 	loss 1.970348834991455
2022-12-17 11:54:20.683043	Iteration 110400 	loss 2.7198643684387207
2022-12-17 11:54:23.039563	Iteration 110500 	loss 1.765454888343811
2022-12-17 11:54:25.516938	Iteration 110600 	loss 2.262570858001709
2022-12-17 11:54:27.857333	Iteration 110700 	loss 1.7062468528747559
2022-12-17 11:54:30.201375	Iteration 110800 	loss 2.311868667602539
2022-12-17 11:54:32.571923	Iteration 110900 	loss 1.5591092109680176
2022-12-17 11:54:35.000077	Iteration 111000 	loss 2.2519304752349854
2022-12-17 11:54:37.519162	Iteration 111100 	loss 2.408418655395508
2022-12-17 11:54:39.869202	Iteration 111200 	loss 2.2442662715911865
2022-12-17 11:54:42.222354	Iteration 111300 	loss 2.550635576248169
2022-12-17 11:54:44.561387	Iteration 111400 	loss 2.2171857357025146
2022-12-17 11:54:46.886559	Iteration 111500 	loss 2.155890464782715
2022-12-17 11:54:49.377438	Iteration 111600 	loss 1.9055273532867432
2022-12-17 11:54:51.702539	Iteration 111700 	loss 1.9742412567138672
2022-12-17 11:54:54.032681	Iteration 111800 	loss 1.6776957511901855
2022-12-17 11:54:56.377859	Iteration 111900 	loss 2.205012321472168
2022-12-17 11:54:58.705746	Iteration 112000 	loss 1.9262808561325073
2022-12-17 11:55:01.196011	Iteration 112100 	loss 2.0098049640655518
2022-12-17 11:55:03.539528	Iteration 112200 	loss 1.8284213542938232
2022-12-17 11:55:05.965194	Iteration 112300 	loss 1.7533364295959473
2022-12-17 11:55:08.476619	Iteration 112400 	loss 2.3738558292388916
2022-12-17 11:55:10.857696	Iteration 112500 	loss 1.8834387063980103
2022-12-17 11:55:13.520348	Iteration 112600 	loss 2.210887908935547
2022-12-17 11:55:16.161293	Iteration 112700 	loss 2.509650230407715
2022-12-17 11:55:18.543473	Iteration 112800 	loss 1.6877598762512207
2022-12-17 11:55:20.928589	Iteration 112900 	loss 2.349254608154297
2022-12-17 11:55:23.276161	Iteration 113000 	loss 2.2623748779296875
2022-12-17 11:55:25.822027	Iteration 113100 	loss 2.0086865425109863
2022-12-17 11:55:28.267135	Iteration 113200 	loss 1.9866849184036255
2022-12-17 11:55:30.673833	Iteration 113300 	loss 2.0955440998077393
2022-12-17 11:55:33.049824	Iteration 113400 	loss 2.042239189147949
2022-12-17 11:55:35.431136	Iteration 113500 	loss 1.7584865093231201
2022-12-17 11:55:37.905923	Iteration 113600 	loss 2.157304286956787
2022-12-17 11:55:40.302433	Iteration 113700 	loss 2.2914483547210693
2022-12-17 11:55:42.771388	Iteration 113800 	loss 2.544203758239746
2022-12-17 11:55:45.140737	Iteration 113900 	loss 2.177544593811035
2022-12-17 11:55:47.543804	Iteration 114000 	loss 2.080197811126709
2022-12-17 11:55:50.145708	Iteration 114100 	loss 2.232194662094116
2022-12-17 11:55:52.509409	Iteration 114200 	loss 1.776253581047058
2022-12-17 11:55:54.846519	Iteration 114300 	loss 1.9577982425689697
2022-12-17 11:55:57.287991	Iteration 114400 	loss 2.4687700271606445
2022-12-17 11:55:59.713767	Iteration 114500 	loss 1.741614580154419
2022-12-17 11:56:02.505732	Iteration 114600 	loss 2.2276275157928467
2022-12-17 11:56:04.872976	Iteration 114700 	loss 2.5599887371063232
2022-12-17 11:56:07.235508	Iteration 114800 	loss 2.1928024291992188
2022-12-17 11:56:09.601832	Iteration 114900 	loss 2.538402557373047
2022-12-17 11:56:11.962994	Iteration 115000 	loss 2.317504405975342
2022-12-17 11:56:14.447131	Iteration 115100 	loss 2.0825791358947754
2022-12-17 11:56:16.819853	Iteration 115200 	loss 2.289797067642212
2022-12-17 11:56:19.153019	Iteration 115300 	loss 2.2532997131347656
2022-12-17 11:56:21.509422	Iteration 115400 	loss 2.123378276824951
2022-12-17 11:56:23.850034	Iteration 115500 	loss 1.6008316278457642
2022-12-17 11:56:26.324113	Iteration 115600 	loss 2.7601311206817627
2022-12-17 11:56:28.669199	Iteration 115700 	loss 2.6921703815460205
2022-12-17 11:56:31.000152	Iteration 115800 	loss 2.105851173400879
2022-12-17 11:56:33.352834	Iteration 115900 	loss 1.9317080974578857
2022-12-17 11:56:35.695880	Iteration 116000 	loss 1.9029302597045898
2022-12-17 11:56:38.161901	Iteration 116100 	loss 1.8319785594940186
2022-12-17 11:56:40.490439	Iteration 116200 	loss 1.8569551706314087
2022-12-17 11:56:42.826829	Iteration 116300 	loss 2.5248005390167236
2022-12-17 11:56:45.162639	Iteration 116400 	loss 1.917578935623169
2022-12-17 11:56:47.508265	Iteration 116500 	loss 1.5390148162841797
2022-12-17 11:56:49.992960	Iteration 116600 	loss 2.456517457962036
2022-12-17 11:56:52.330660	Iteration 116700 	loss 2.0441882610321045
2022-12-17 11:56:54.700634	Iteration 116800 	loss 1.6947083473205566
2022-12-17 11:56:57.091109	Iteration 116900 	loss 1.868866205215454
2022-12-17 11:56:59.432488	Iteration 117000 	loss 2.0362534523010254
2022-12-17 11:57:01.896838	Iteration 117100 	loss 2.6482980251312256
2022-12-17 11:57:04.229082	Iteration 117200 	loss 1.5127215385437012
2022-12-17 11:57:06.593112	Iteration 117300 	loss 2.411573886871338
2022-12-17 11:57:08.955755	Iteration 117400 	loss 2.1321468353271484
2022-12-17 11:57:11.306898	Iteration 117500 	loss 2.0081160068511963
2022-12-17 11:57:13.767563	Iteration 117600 	loss 2.235265016555786
2022-12-17 11:57:16.080132	Iteration 117700 	loss 2.075728178024292
2022-12-17 11:57:18.399990	Iteration 117800 	loss 1.8261749744415283
2022-12-17 11:57:20.711341	Iteration 117900 	loss 2.3083651065826416
2022-12-17 11:57:23.105978	Iteration 118000 	loss 2.2146382331848145
2022-12-17 11:57:25.637223	Iteration 118100 	loss 2.0594515800476074
2022-12-17 11:57:27.961713	Iteration 118200 	loss 2.0558557510375977
2022-12-17 11:57:30.279971	Iteration 118300 	loss 2.1226089000701904
2022-12-17 11:57:32.606635	Iteration 118400 	loss 1.7395877838134766
2022-12-17 11:57:34.913481	Iteration 118500 	loss 1.6974892616271973
2022-12-17 11:57:37.386339	Iteration 118600 	loss 1.866072416305542
2022-12-17 11:57:39.706670	Iteration 118700 	loss 2.5911738872528076
2022-12-17 11:57:42.037702	Iteration 118800 	loss 2.0785694122314453
2022-12-17 11:57:44.357197	Iteration 118900 	loss 2.012140989303589
2022-12-17 11:57:46.760295	Iteration 119000 	loss 1.8733477592468262
2022-12-17 11:57:49.280772	Iteration 119100 	loss 1.7429143190383911
2022-12-17 11:57:51.686922	Iteration 119200 	loss 2.068204641342163
2022-12-17 11:57:54.017225	Iteration 119300 	loss 2.117900848388672
2022-12-17 11:57:56.339405	Iteration 119400 	loss 2.1214656829833984
2022-12-17 11:57:58.701234	Iteration 119500 	loss 2.45458984375
2022-12-17 11:58:01.161187	Iteration 119600 	loss 2.2566580772399902
2022-12-17 11:58:03.486922	Iteration 119700 	loss 1.9548958539962769
2022-12-17 11:58:05.799710	Iteration 119800 	loss 2.6458420753479004
2022-12-17 11:58:08.193634	Iteration 119900 	loss 2.882854461669922
2022-12-17 11:58:10.517787	Iteration 120000 	loss 2.5008041858673096
2022-12-17 11:58:12.980639	Iteration 120100 	loss 2.39180588722229
2022-12-17 11:58:15.335976	Iteration 120200 	loss 2.3394219875335693
2022-12-17 11:58:17.711736	Iteration 120300 	loss 2.1379947662353516
2022-12-17 11:58:20.028495	Iteration 120400 	loss 1.8195921182632446
2022-12-17 11:58:22.355868	Iteration 120500 	loss 2.510660171508789
2022-12-17 11:58:24.811441	Iteration 120600 	loss 1.7874984741210938
2022-12-17 11:58:27.534203	Iteration 120700 	loss 1.7637823820114136
2022-12-17 11:58:30.096805	Iteration 120800 	loss 2.193629503250122
2022-12-17 11:58:32.500445	Iteration 120900 	loss 2.773597240447998
